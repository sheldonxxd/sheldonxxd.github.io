[{"title":"利用 pandoc 把 markdown 文稿转换为独立 html 文件","url":"/2021/05/06/pandoc_md2html/","content":"\n### 命令行尝试\n\npandoc 安装后一般会被加入环境变量，这样就可以在 python 中使用 `os.system` 命令调用。pandoc 的版本是 2.12，能够基本实现我想要的效果的命令如下：\n\n```bash\npandoc -s D:\\log\\main\\source\\_posts\\20210506.md --from markdown+emoji --self-contained -c github-pandoc.css -o test.html\n```\n\npandoc 可以带很多参数，对上面这条命令注解：\n\n- `-s`：source，代表需要转换的源文档\n- `--from markdown+emoji`：对 emoji 代码进行转换\n- `--self-contained`：生成独立文件，比如会把图片嵌入到 html 中\n- `-c`：用户自定义样式，我从网上下载了一个 github-pandoc 的样式文件\n- `-o`：output，输出文档路径\n\n需要注意，此条命令如果要嵌入图片，而源文档中使用的是相对路径引用图片，则需要保证 pandoc 命令运行的根目录与源文档相同，否则会出现 「File Not Found」错误。\n\n### 修改 embed 的槽函数\n\n为了保持版本稳定，修改在副本 `main4_2.py` 上完成。\n\nembed 对应的槽函数为 htmlEmbedding，代码修改如下：\n\n```python\ndef htmlEmbedding(ui):\n    ''' \n    2021-05-06 12:03:51\n    调用 pandoc 完成独立 html 文件的导出\n    ''' \n    global wks\n    global qdir\n    base_fname = ui.fname.text() \n    md_filepath = os.path.join(wks, base_fname+'.md')\n    html_filename = base_fname+'.html'\n    html_filepath = os.path.expanduser('~/Desktop/{}'.format(html_filename))\n    css_filepath = qdir + '/github-pandoc.css'  # 样式文件与此脚本放在同级目录\n    command = 'cd \"{}\" && pandoc -s {} --from markdown+emoji  --self-contained -c \"{}\" -o \"{}\"'.format(\n        wks, md_filepath, css_filepath, html_filepath)\n    os.system(command)\n    ui.searchResults.setText('图片嵌入成功！\\nHTML文件已保存至桌面！')\n```\n\n### 测试结果\n\n![image-20210506121556166](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210506121556166.png)\n\n完美，这样就大幅度减少了 LogTool 对 BeautifulSoup、base64、urllib 等模块的依赖。\n\n### 参考资料\n\n[Posiible to use pandoc with HTML containing base64 inline images? - Stack Overflow](https://stackoverflow.com/questions/20059542/posiible-to-use-pandoc-with-html-containing-base64-inline-images)\n\n[GitHub-like CSS for pandoc standalone HTML files (perfect for HTML5 output). Based on Marked.app's GitHub CSS. Added normalize.css (v2.1.3) in the prior to GitHub css.](https://gist.github.com/Dashed/6714393)","tags":["pandoc","html","LogTool"],"categories":["python"]},{"title":"文献精读工作流","url":"/2021/04/25/deepReadingPipeline/","content":"\n很多时候要针对一个主题进行文献阅读，会关注不同文章的不同局部细节内容，并且做笔记。做完笔记后，可以把相关的参考文献重新汇总一下，和阅读笔记一起放到 mendeley 的新建子目录内。这个过程涉及到一些信息的提取和文件格式的转化，因此整合了一些代码，方便自己。\n\n\n具体方法是这样的：\n\n1. 使用`newPost.bat`脚本在`_posts`目录下生成一个带有 front matter 模板的 markdown 文件\n2. 打开 mendeley 查找和阅读刚兴趣主题的文献，并在 markdown 中做笔记\n3. 修改和补充 front matter 中的信息，`title`、`excerpt`、`tags`等信息是必需的\n4. 从typora中直接复制 markdown 的文件路径，作为修改上面`collectCitations.py` 中 md 的值（字符串），然后运行脚本\n5. 生成的 refs.bib 文件保存在 markdown 笔记文件的附录文件夹中，可以从 mendeley 中新建一个目录，然后 import 这个 bib 文件\n6. 将 markdown 笔记用 typora 导出为 html，然后用 `htmlEmbedding.py` 脚本导出为 standalone 的网页，作为附件添加到 mendeley 相应笔记条目的 files 中\n\n最后mendeley中效果如下所示：\n\n![image-20210425131836781](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210425131836781.png)\n\n### collectCitations.py\n\n新增了“根据 markdown 的 frontmatter 信息自动添加 entry 到 bib 文件的方法”，新版本的代码如下：\n\n```python\nimport bibtexparser\nfrom bibtexparser.bwriter import BibTexWriter\nfrom bibtexparser.bibdatabase import BibDatabase\nimport os\nimport re\nimport yaml \nimport datetime\n\nclass Bib:\n    def __init__(self, fps):\n        '''\n        fps: dict, text.md filepath & library.bib filepath\n        {'text':fp1, 'library':fp2}\n        '''\n        self.fp = fps['text']\n        self.wks, self.fname = os.path.split(self.fp)\n        base, _ = os.path.splitext(self.fp)\n        self.assets = base +'/'  # markdown 文件的附件目录\n        self.bibpath = fps['library']\n        self.data = []  # 每一个entry都是dict，一起装到list里边\n    \n    def load(self):\n        '''\n        load the library.bib\n        '''\n        fp = self.bibpath\n        with open(fp, encoding='utf-8') as bibtex_file:\n            bib_database = bibtexparser.load(bibtex_file)\n        self.library = bib_database.entries_dict\n        print('{} entries loaded!'.format(len(self.library)))\n    \n    def collect(self):\n        '''\n        collect citation key from text.md\n        '''\n        fp = self.fp\n        with open(fp, 'r', encoding='utf-8') as f: \n            data = f.read()\n        pattern = \"\\[@[^]]+]\"\n        # 使用方括号列举不能匹配的字符]，确保只找到一个[@???]\n        a = re.findall(pattern, data)\n        test = ''.join(a)\n        pattern = '@([\\w\\-:.#$%&-+?<>~/]+);?'\n        # citationkey中可能包含一些符号类字符，以上都是pandoc允许的\n        # https://pandoc.org/MANUAL.html#extension-citations\n        # 注意对中括号内的`-`符号进行转义\n        b = re.findall(pattern, test)\n        self.keys = list(set(b))\n        print('{} citations fetched!'.format(len(self.keys)))\n        \n        # 根据收集到的key，到library里边把具体的信息 ditc 拿到\n        data = []\n        for key in self.keys:\n            data.append(self.library[key])\n        self.data = data  # 方便后面再 append\n        \n    def save(self):\n        '''\n        save the new subcollection bib file to assets folder\n        https://bibtexparser.readthedocs.io/en/master/tutorial.html\n        '''\n        if not os.path.exists(self.assets):\n           os.mkdir(self.assets)\n        db = BibDatabase()\n        db.entries = self.data\n        writer = BibTexWriter()\n        fp2 = os.path.join(self.assets, 'refs.bib')\n        with open(fp2, 'w', encoding='utf-8') as bibfile:\n            # 注意编码是 utf-8，默认的 gbk 会出错\n            bibfile.write(writer.write(db))\n        print('[Save to] {}'.format(fp2))\n\nclass AddEntry(Bib):\n    '''\n    把 markdown 本身生成一条 entry 放到 汇总的 bib 文件中。\n    '''\n    def __init__(self, fps):\n        super(AddEntry, self).__init__(fps)\n    \n    def fetchYAML(self):\n        # 先读取 markdown 文件\n        fp = self.fp \n        with open(fp, 'r', encoding='utf-8') as f: \n            data = f.readlines()\n        # 获取文件头的 front matter 信息\n        idx = 0\n        i = 0\n        cout = []\n        while idx<2:\n            line = data[i]\n            if line.startswith('---'):\n                idx += 1\n                cout.append(i)\n            i += 1\n        start, end = cout\n        info = data[start+1: end]\n        info = ''.join(info)\n        info_dict = yaml.load(info, Loader=yaml.FullLoader)\n        # 'date': datetime.datetime(2021, 4, 25, 10, 53, 32)\n        self.info = info_dict\n\n    def makeEntry(self):\n        '''\n        使用bibtexparser\n        将front matter转换为bib中的一个entry\n        然后 append 到 self.data中\n        主要是有：\n        1. citation key\n        2. abstract (excerpt)\n        3. author (default is Xie, Xiaodong) \n        4. journal (default is sheldon-notes)\n        5. title\n        6. year\n        7. volumn (month)\n        8. number (day)\n        ---\n        file 可以自己手动添加导出的html文件\n        '''\n        info = self.info  # 一个字典\n        entry = {}\n        entry['year'] = str(info['date'].year)  \n        # entry中的内容必须都是字符串\n        entry['volumn'] = str(info['date'].month)\n        entry['number'] = str(info['date'].day)\n        # https://blog.csdn.net/cmzsteven/article/details/64906245\n        entry['title'] = info['title']\n        entry['abstract'] = info['excerpt']\n        entry['journal'] = 'sheldon-notes'\n        entry['author'] = 'Xie, Xiaodong'\n        entry['ENTRYTYPE'] = 'article'\n        entry['keywords'] = ','.join(info['tags'])\n        entry['ID'] = 'xxd_{}-{}-{}'.format(entry['year'], \n                                         entry['volumn'], \n                                         entry['number']\n                                         )\n        self.data.append(entry)\n\n\nif __name__=='__main__':\n    # 指定 markdown 文件路径\n    md = r'D:\\log\\main\\source\\_posts\\cell_PAINT_imaging.md'\n    fps = {\n        'text': md,\n        'library': r\"D:\\papers\\library.bib\"\n    }\n    # 初始化对象\n    xxd = AddEntry(fps)\n    # 加载 library.bib 总库\n    xxd.load()\n    # 收集 markdown 中引用了的条目\n    xxd.collect()\n    # 收集 markdown 中的 front matter信息\n    xxd.fetchYAML()\n    # 根据 front matter 制作一条新的 entry\n    xxd.makeEntry()\n    # 保存子集 refs.bib 到 markdown 附件目录\n    xxd.save()\n```\n\n\n### newPost.bat\n\n用于在`_posts`目录下新建一个带 front matter 的markdown 文件的小脚本。\n\n```bash\n:: 2021-01-12 10:31:58\n:: 生成具有FrontMatter的markdown文件\n:: 具有title、excerpt、date、tags、categories等\n\necho off\nset d=%date:~0,4%-%date:~5,2%-%date:~8,2% %time:~0,8%\nREM 收集完时间参数后，声明采用UTF-8编码\nchcp 65001\nset /p t=Please input the filename (no space):\n\necho --->>%t%.md\necho title: %t%>>%t%.md\necho excerpt: This is a summary of this draft>>%t%.md\necho date: %d%>>%t%.md\necho tags: >>%t%.md\necho categories: >>%t%.md\necho - 草稿 >>%t%.md\necho --->>%t%.md\necho 从这里开始写>>%t%.md\n```\n\n\n### htmlEmbedding.py\n\n把 typora 导出的 html 中的图片转为 base64 编码字节嵌入网页中，使之能够离线访问。\n\n```python\nimport os,sys, shutil\nfrom bs4 import BeautifulSoup\nimport base64 \nfrom urllib.request import unquote, urlopen\nimport PySimpleGUI as sg\n\nclass app:\n    '''通用版简易工具用户界面\n    1. 使用字典传入工具函数集\n    2. 按钮平铺，每个按钮对应一个函数功能\n    '''\n    def __init__(self,app_name,func_dict):\n        self.funcs = func_dict\n        self.layout = self.design()\n        self.window = sg.Window(app_name, \n                                self.layout,\n                                location=(400,400),\n                                grab_anywhere=True,\n                                disable_minimize=True,\n                                keep_on_top=True)\n        self.run()\n    \n    def design(self):\n        layout = [[sg.Button(x) for x in self.funcs.keys()],\n                [sg.Text(\"欢迎使用\",size=(20,1),key=\"_INFO_\")]]\n        return layout\n    \n    def run(self):\n        while True:\n            event,values = self.window.Read()\n            for key in self.funcs.keys():\n                if event==key:\n                    fp = sg.popup_get_file('打开需要转化的HTML文件')\n                    if fp is not None:\n                        info = self.funcs[key](fp)\n                        self.window.Element(\"_INFO_\").Update(key+\":\"+info)\n            if event is None:\n                break\n        self.window.Close()\n\n# 先使用typora导出html，再使用此代码搞定图片链接编码成base64\n\ndef img_base64(html_filepath):\n    wks,html_filename = os.path.split(html_filepath)\n    with open(os.path.join(wks,html_filename),\"r\",encoding='utf-8') as f:\n        content = f.read()\n    # soup = BeautifulSoup(content,features=\"lxml\")\n    soup = BeautifulSoup(content,features=\"html.parser\")\n    # 2020-06-12 21:03:12\n    # 代码在新机器中运行出错，lxml不行，只好用html.parser\n    # 参考 https://www.cnblogs.com/awakenedy/p/9753326.html\n    imgs = soup.find_all(\"img\")\n    for img in imgs:\n        imgpath = img['src']  \n        # 2020-12-09 17:20:19 当附件图像存在于中文文件夹时，使用markdown-to-standalone-html转化html会乱码，需要处理一下\n        # 参考（https://blog.csdn.net/mouday/article/details/80278938）\n        imgpath = unquote(imgpath, encoding='utf-8')\n        pref = imgpath.split(\".\")[-1]\n        \n        if not imgpath.startswith(\"http\"):\n            with open(os.path.join(wks,imgpath),'rb') as f:\n                base64_data = base64.b64encode(f.read())\n        else:\n            try:\n                response = urlopen(imgpath)\n                # 使用requests模块会报错。\n                base64_data = base64.b64encode(response.read())\n                # 2021-03-29 14:15:16  如果图片已经上传到阿里云，则是链接形式\n            except:\n                print('Internet Connection Error')\n                continue\n\n        img['src'] = \"data:image/{};base64,\".format(pref)+str(base64_data,'utf-8')\n        # 注意base64是bytes，使用str转化时要带encoding参数\n        # 对img的修改都是inplace的，从属于soup对象\n    ex_filename = os.path.join(wks,html_filename)\n    with open(ex_filename,\"w\", encoding='utf-8') as f:\n        ### 覆盖保存html文件\n        ## 2021-03-29 16:26:15 \n        # 注意hexo server也会渲染html文件，但是会报错，所以导出html文件后转移到桌面\n        f.write(str(soup))\n    shutil.move(ex_filename, os.path.expanduser('~/Desktop/ex-{}'.format(html_filename)))\n    return \"转化了%d张图像\"%(len(imgs))\n\nif __name__ == \"__main__\":\n    app_name = \"img_base64\"\n    func_dict = {\"base64\":img_base64}\n    worker = app(app_name,func_dict)\n    worker.run()\n    # os.system(\"pause\")\n```\n\n","tags":["bib"],"categories":["python"]},{"title":"为了网页正常显示脚注","url":"/2021/04/17/tryButterflyTheme/","content":"\n把自己的主题从 stun 更换为 butterfl[^b]，显然是因为后者更好看一点（我这个颜控😂）。当然初衷是想显示脚注，因为我偶尔（也可能是极少数情况）会希望自己插入的参考文献能够完整显示在网页上（需求同这位老哥[^1]）。\n\n但是 butterfly 原生设置并不支持，我查到资料说，得安装 hexo-renderer-markdown-it-plus 插件[^2]：\n\n```bash\nnpm un hexo-renderer-marked --save\nnpm i hexo-renderer-markdown-it-plus --save\n```\n\n然后具体使用，用方括号，放 key：\n\n```\nThis is something[^key]\n```\n\n注意 key 前面得有一个 `^` 符号，key的命名就是英文字母加数字即可，可以等同于 citation key 。不用考虑排序问题，参考文献目录会根据在文中出现的顺序自动排序的。\n\n后来又发现 butterfly 中开启 search 的插件有点麻烦，果断换回来 stun，发现 footnote 也能正确显示了，非常棒！\n\n然后我还取消了 post 末尾的 `--- END ---`\n\n```yaml\n# The widget of post.\npost_widget:\n  # Whether to show tags at the bottom of post.\n  tags: true\n  # Whether to show \"------ END ------\" at the bottom of post.\n  end_text:\n    enable: false\n    # Whether to show a horizon line before \"------ END ------\".\n    horizon_line: false\n```\n\n\n\n[^1]: https://zhanghuimeng.github.io/post/add-footnote-plugin-for-hexo-blog/\n[^2]: https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plus\n[^b]: https://github.com/jerryc127/hexo-theme-butterfly\n\n","categories":["blog"]},{"title":"markdown导出为带引文目录的word","url":"/2021/04/17/markdown-word/","content":"\n文献阅读笔记汇总之后，我还想导出为 word 文档，而且把参考引文目录插入进去，为此写了一段 python 代码，基于 subprocess 模块调用 pandoc 工具来实现：\n\n```python\nimport os, subprocess \n\nclass Word:\n    '''\n    ==本程序仅支持windows系统==\n    借助 pandoc 工具（版本为2.11），\n    将 markdown 转化为 word，\n    并且带有指定格式的引文目录，\n    导出文件默认保存到 markdown 同目录下\n    '''\n    def __init__(self, paths):\n        '''\n        paths: 各种文件路径组成的字典\n        paths['ref'] = 参考文献 bib 文件路径\n        paths['style'] = 引文格式样式 csl 文件路径\n        paths['template'] = word 模板文件路径\n        paths['text'] = markdown 文件路径\n        '''\n        self.paths = paths \n        base, _ = os.path.splitext(paths['text'])\n        self.paths['output'] = base + '.docx'\n    \n    def run(self):\n        '''\n        基于 subprocess 调用 pandoc\n        '''\n        wks, fname = os.path.split(self.paths['text'])\n        # 由于 markdown 对于插图使用的相对路径，\n        # 所以需要进入到 text 所在的目录执行操作\n        \n        # pandoc 更新到 2.11 版本后，无需外部 pandoc-citeproc\n        # 支持对 emoji 代码进行转换\n        command = 'cd \"{mdir}\" && \"pandoc\"\\\n            --citeproc\\\n            --from markdown+emoji\\\n            --bibliography=\"{ref}\"\\\n            --csl=\"{style}\"\\\n            --reference-doc=\"{template}\"\\\n            \"{text}\" -o \"{output}\"'.format(\n            ref = self.paths['ref'],\n            style = self.paths['style'],\n            template = self.paths['template'], \n            text = fname, \n            output = self.paths['output'],\n            mdir = wks, \n        )\n        # More about `--citeproc`\n        # https://pandoc.org/MANUAL.html#citations\n        command_test = 'pandoc --version'\n        ret = subprocess.run(command, \n                             shell=True, \n                             stdout=subprocess.PIPE, \n                             stderr=subprocess.PIPE, \n                             encoding=\"utf-8\", \n                             timeout=30,\n                             )\n        if ret.returncode == 0:\n            print(\"success:\",ret)\n        else:\n            print(\"error:\",ret)\n\nif __name__=='__main__':\n    # 一般只需要修改需要导出为 word 的 markdown 文件路径即可\n    markdown_filepath = r\"D:\\log\\main\\source\\_posts\\read-20210417.md\"\n    paths = {\n        'text': markdown_filepath,\n        'ref': r\"D:\\papers\\library.bib\",\n        'style': r\"D:\\papers\\.styles\\acs-nano.csl\",\n        'template': r\"D:\\papers\\.word-template\\template.docx\",\n    }\n    xxd = Word(paths)\n    xxd.run()\n```\n\n注意如果文档内容太多，pandoc 执行时间较长，可能触发 timeout =30 的设定。","tags":["subprocess","pandoc","word"],"categories":["python"]},{"title":"收集 markdown 中的参考文献并汇总到 bib","url":"/2021/04/17/markdown-bib/","content":"\n有时候可以使用 markdown 对一个主题明确的范围的文献做粗略的记录，其实本身就是起到了一个汇总整理的作用，然后其中已经使用了很多 citation key，其实完全可以根据 key 反过去从总的 library.bib 库中把这些文献记录收集出来，形成一个新的 refs.bib，所以我又写了一段代码。 \n\n这个工具，我希望是能够收集 markdown 文件中的 citations，然后生成一个 subcollection 的 bib 文件，存放在 markdown 文件的附录中。\n\n大致上，bib 文件的读取和保存交由一个第三方的库 [bibtexparser](https://bibtexparser.readthedocs.io/en/master/tutorial.html) 来搞定，citation key 的提取我自己用 [re 正则](https://www.runoob.com/regexp/regexp-rule.html)模块写了一下，应该也问题不大。具体代码如下：\n\n```python\nimport bibtexparser\nfrom bibtexparser.bwriter import BibTexWriter\nfrom bibtexparser.bibdatabase import BibDatabase\nimport os\nimport re\n\nclass Bib:\n    def __init__(self, **fps):\n        '''\n        text.md filepath & library.bib filepath\n        {'text':fp1, 'library':fp2}\n        '''\n        self.fp = fps['text']\n        self.wks, self.fname = os.path.split(self.fp)\n        base, _ = os.path.splitext(self.fp)\n        self.assets = base +'/'  # markdown 文件的附件目录\n        self.bibpath = fps['library']\n    \n    def load(self):\n        '''\n        load the library.bib\n        '''\n        fp = self.bibpath\n        with open(fp, encoding='utf-8') as bibtex_file:\n            bib_database = bibtexparser.load(bibtex_file)\n        self.library = bib_database.entries_dict\n        print('{} entries loaded!'.format(len(self.library)))\n    \n    def collect(self):\n        '''\n        collect citation key from text.md\n        '''\n        fp = self.fp\n        with open(fp, 'r', encoding='utf-8') as f: \n            data = f.read()\n        pattern = \"\\[@[^]]+]\"\n        # 使用方括号列举不能匹配的字符]，确保只找到一个[@???]\n        a = re.findall(pattern, data)\n        test = ''.join(a)\n        pattern = '@([\\w\\-:.#$%&-+?<>~/]+);?'\n        # citationkey中可能包含一些符号类字符，以上都是pandoc允许的\n        # https://pandoc.org/MANUAL.html#extension-citations\n        # 注意对中括号内的`-`符号进行转义\n        b = re.findall(pattern, test)\n        self.keys = list(set(b))\n        print('{} citations fetched!'.format(len(self.keys)))\n        \n    def save(self):\n        '''\n        save the new subcollection bib file to assets folder\n        https://bibtexparser.readthedocs.io/en/master/tutorial.html#call-the-writer\n        '''\n        if not os.path.exists(self.assets):\n            os.mkdir(self.assets)\n        data = []\n        for key in self.keys:\n            data.append(self.library[key])\n        db = BibDatabase()\n        db.entries = data\n        writer = BibTexWriter()\n        fp2 = os.path.join(self.assets, 'refs.bib')\n        with open(fp2, 'w', encoding='utf-8') as bibfile:\n            # 注意编码是 utf-8，默认的 gbk 会出错\n            bibfile.write(writer.write(db))\n        print('[Save to] {}'.format(fp2))\n```\n\n使用则非常简单，基本上只需要修改 markdown 文件路径即可：\n\n```python\nxxd = Bib(\n    text=r\"D:\\log\\main\\source\\_posts\\read-20210417.md\",\n    library=r\"D:\\papers\\library.bib\"\n)\nxxd.load()\nxxd.collect()\nxxd.save()\n```\n\n感觉我现在写代码都成套路了，一个类，上来就是先把 open 和 save 考虑好，然后中间处理过程写成类的方法。\n\n然后正则表达式匹配文本之类，真的是有点难，主要是记不住。但是后面我如果自己想写什么搜索引擎的话，肯定是离不开这个的。","tags":["bib","re","正则式"],"categories":["python"]},{"title":"大变局之我见","url":"/2021/04/17/ChinaWillWin/","content":"\n这些年国内外大事多了起来，关键处，还是中美竞争。虽然我们官方不愿意用竞争这样的字眼，更爱用合作共赢的措辞，但事实就是这样。\n\n既然是竞争，那终究是残酷的，所以这样一个阶段各种文明人难以理解的事情一一发生，底线一再被突破，也都非常正常。说白了还是那句话，规矩建立在实力的基础上。\n\n从前几年各种商业上的制裁，甚至软禁一个公司的高层来打压企业发展，到现在BCI抵制新疆棉花。真正让世人见识到什么叫做“欲加之罪，何患无辞。”\n\n可是，西方已经习惯了那一套傲慢与偏见，表面人道主义，实际金钱至上。精英们的伪善被现实不断打脸，前有美国抗疫不力还封锁疫苗，后有日本核污废水直排太平洋。明明有很多更好的方案，但最后的决策就是这么赤裸裸。他们已经走不出那个“舒适圈”去适应一个新世界了。\n\n所以问题是：\n\n1. 中国的上位是否会伴随战火纷飞？\n2. 未来中国是否会重蹈这些覆辙？\n3. 个人在这样的大势中该如何立身？\n\n关于第一个问题，世界大战级别的我估计可能性较小。因为核武能灭世，搞起来谁都讨不了好。不过局部战争或者地区骚乱应该会有一些。所以没啥事不要去人多的地方，也不要到处乱跑。\n\n至于第二个问题，这是没有答案的，这取决于中国共产党是否能坚持住自洁自律，不忘初心，始终人民至上，事事为民。大体上，我感觉在我有生之年，共产党都不会有太大问题。但是更长远的情况，就取决于党的教育宣传和其他党建是否落到实处了。\n\n第三个问题稍微实在一点。\n\n首先厘清概念，个人就专指我这样的普通草根吧，出身一般，受过点高等教育，没有多少存款，也没多少人脉，干着一份也没多少工资的工作，什么都是普普通通的背景板一样的人。\n\n而立身，就是普通人怎么普通的安稳的度过一生。吃喝拉撒，生老病死，买房买车，教育医疗，无过于此。\n\n所以关于这个问题的答案，其实恰恰又是与任何大势无关的。因为普通人，基本没法把握这种势（没得选）。既然如此，一直如此，那就是事业上做好力所能及的事情，平日里拥有健康的生活方式，每一天都尽量充实一点，人生的幸福感与成就感并不会比做大事的弄潮儿差。\n","tags":["happiness"],"categories":["life"]},{"title":"校园食记","url":"/2021/04/16/food_track/","content":"\n食堂菜被称作中国的第九大菜系，这里记录了十来天自己在学校食堂的饮食，没有图片，权当是看看自己为啥会长胖。\n\n\n**4月6日（星期二）**\n\n早，二餐，豆腐脑（甜），梅干菜烧饼\n午，七餐，西兰花，宫保鸡丁，青菜\n晚，四餐，港式烧腊（双拼）\n\n**4月7日（星期三）**\n\n早，二餐，玉米汁，山东煎饼（土豆丝，海带丝）\n午，六餐，农家菜（豇豆，腊肉笋，青菜油豆腐）\n晚，七餐，平菇，粉条，蒜苔\n\n**4月8日（星期四）**\n\n早，二餐，肉夹馍，水煮蛋，豆浆\n午，七餐，茄子，萝卜，黄瓜\n晚，七餐，蒜苔，土豆牛肉片，包菜\n\n**4月9日（星期五）**\n\n早，一餐，八宝粥，天健肉饼\n午，二餐，金针菇肥牛粉丝（牛百碗）\n晚，四餐，叉烧饭\n\n**4月10日（星期六）**\n\n早，家里，荔枝味果奶，草莓，吐司\n午，七餐，蒜苔，鸡腿，土豆烧排骨，香菇\n晚，玉兰苑，木桶饭，茄子鸡肉双拼\n\n**4月11日（星期天）**\n\n早，家里，青椒鸡蛋杂粮饼\n午，家里，山药炖排骨\n晚，家里，煎豆腐，青菜拌饭，胡萝卜丝炒鸡蛋\n\n**4月12日（星期一）**\n\n早，家里，银耳汤，自制吐司\n午，七餐，水煮肉片，冬瓜，西兰花\n晚，五餐，石锅肉糜茄子\n\n**4月13日（星期二）**\n\n早，家里，面包，牛奶，鸡蛋\n午，七餐，红烧鱼块，水煮肉片，西兰花\n晚，四餐，港式烧腊（双拼）\n\n**4月14日（星期三）**\n\n早，二餐，豆花，山东煎饼（土豆丝，海带丝）\n午，七餐，笋片，水煮肉片，土豆排骨\n晚，七餐，炸小虾，红烧鱼块，杏鲍菇肉片\n\n**4月15日（星期四）**\n\n早，家里，面包，鸭翅根，AD钙奶\n午，七餐，鸡腿，西红柿炒蛋，笋片\n晚，五餐，石锅鸡柳\n\n**4月16日（星期五）**\n\n早，家里，面包，AD钙奶\n午，七餐，炸鸡，茄子，豇豆，白菜\n晚，三餐，麻辣香锅","tags":["food","食堂"],"categories":["life"]},{"title":"K均值聚类和三点结构过滤","url":"/2021/04/15/KMeans_clustering_filter/","content":"\n从 picasso 中 pick 出来的 6HB 三点结构，会有一些因为折纸变形而不是典型的“三点一线”，为了避免这些结构对后面做图像平均的干扰，我写了一个过滤器。由于每个结构固定由三个子簇组成，所以我选择了`K均值聚类`来处理（见`applyKMeanFilter`递归函数），并且得到每个子簇的中心坐标，然后基于三个点坐标来判断形状（`cluster_filter`函数）。而全部数据的读取、分组、遍历、过滤、收集和保存，则交由`SpotFilter`这个类来搞定。这部分代码知识点浓度太高，没有时间一一罗列，幸好平时注释也还算清楚，所以直接贴代码👇：\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans \nimport h5py \nimport os, shutil\nimport itertools  # 方便做排列组合\nimport matplotlib.pyplot as plt\n\n\ndef applyKMeanFilter(region, cluster_number=3, **params):\n    '''\n    递归对单个region（dataframe）进行原位处理，\n    返回符合条件的cluster_labels(a list), 如[(0, 2, 3)]\n    以及提取过后的region_new\n    **params:\n    params['bias']: 中心偏置距离，不超过20 nm  \n    params['interval']: 端距，固定为240 nm  \n    params['tolerance']: 端距容忍度，默认30nm  \n    '''\n    if cluster_number>10 or len(region)<cluster_number:\n        # 最多10个cluster\n        return False\n    ### 计算一个pick region里边的聚类中心坐标\n    region = region.copy()\n    # 避免settingWithCopyWarning\n    X = region.filter(items=['x','y'])\n    X = np.array(X)\n    kmeans = KMeans(n_clusters=cluster_number, random_state=0).fit(X)\n    # 注意如果样本数量小于n_clusters也会报错\n    \n    region['Cluster'] = kmeans.labels_\n    gp = region.groupby(by=['Cluster'])\n    cluster_centers = {}\n    for key,df in gp:\n        cluster_centers[key] = np.array([df.x.mean(), df.y.mean()]) \n        # 字典的值做成array，方便后面计算 \n    \n    labels = cluster_filter(cluster_centers, **params) \n    # 自定义过滤规则\n    \n    if len(labels)==0 and labels:\n        labels, region = applyKMeanFilter(region, \n                                  cluster_number=cluster_number+1, \n                                  **params) \n        # 注意这里要有返回值，不然没东西    \n    return labels, region\n\n\ndef cluster_filter(centers, \n                   bias=30, \n                   interval=240, \n                   tolerance=30, \n                   pixelsize = 160, \n                   show=False, \n                   **params\n                  ):\n    '''\n    centers: dict，聚类中心坐标，带cluster_label, {label:[x, y]}\n    bias: 三点pattern中心的最大偏置距离 \n    interval: 三点pattern两端最小距离\n    tolerance: 端距离的容忍变化范围\n    show: 可以对各个组合进行作图\n    '''\n    ehi = np.array([bias, interval, tolerance])/pixelsize\n    bias, interval, tolerance = ehi\n    tris = []\n    for keys in itertools.combinations(centers, 3):\n        triangle = [centers[key] for key in keys]  \n        # 把元组变成list，方便后面pop\n        cc = np.mean(triangle, axis=0)  \n        # 计算三角形中心\n        \n        if show:\n            allPoints = np.array(list(centers.values()))\n            plt.scatter(allPoints[:,0], allPoints[:,1])\n            ttt = np.array(triangle)\n            plt.plot(ttt[:,0], ttt[:,1],'r-.')\n            plt.text(cc[0], cc[1], '* center')\n            plt.show()\n            \n        cid = 100 # 预定义一个错误的索引，cid=0，1，2\n        for idx, p in enumerate(triangle):\n            d = np.linalg.norm(p - cc)\n            if d<bias:\n                # 若偏置距离足够小，认为此点为线段中点\n                cid = idx\n        if cid>2:\n            # 如果没有一个顶点的偏置距离在容忍范围内，则进入下一个循环\n            continue\n        else:\n            # 计算两个端点距离\n            tt = triangle.copy()  \n            # 由于triangle要回收，pop会破坏原list，所以这里复制下\n            b = tt.pop(cid)  \n            # pop返回终点，剩下两个是端点\n            dt = np.linalg.norm(tt[0] - tt[1])\n            if abs(dt-interval)<tolerance:\n                tris.append(keys)  \n                # 回收符合规则的组合，也就是centers的label即可\n    return tris\n\n\nclass SpotFilter:\n    def __init__(self, fp):\n        '''\n        fp: picasso hdf5 files with `_picked.hdf5` suffix \n        and the `group` column existed!\n        '''\n        self.filepath = fp\n        self.data = pd.read_hdf(fp, key='locs')\n        self.gp = self.data.groupby('group')\n        print('{} structure loaded!'.format(len(self.gp)))\n    \n    def run(self, **params):\n        newdata = []\n        for key, region in self.gp:\n            labels, region_ = applyKMeanFilter(region)\n            if labels:\n                # 如果labels返回值为False，说明n_clusters到10了都没找到\n                region_new = region_[region_['Cluster'].isin(labels[0])]\n                newdata.append(region_new)\n        self.newdata = pd.concat(newdata, ignore_index=True)\n        gp = self.newdata.groupby('group')\n        print('{} structures left!'.format(len(gp)))\n\n    def sampling(self, frac=0.5):\n        '''\n        可以保存部分结构，frac是sampling的抽样比例\n        '''\n        gids = []\n        for key, _ in self.gp:\n            gids.append(key)\n        gids = pd.Series(gids, name='groupID')\n        gids2 = gids.sample(frac=frac)\n        print('{} structures fetched!'.format(len(gids2)))\n        data = []\n        for gid in gids2:\n            data.append(self.gp.get_group(gid))\n        self.newdata = pd.concat(data, ignore_index=True)\n    \n    def save(self, suffix='_filtered'):\n        '''\n        suffix 是hdf5文件后缀\n        '''\n        df = self.newdata\n        base, _ = os.path.splitext(self.filepath)\n        fp = base + suffix + '.hdf5'\n        fp_yaml = base + suffix + '.yaml'\n        OBJ = [df[x].values for x in df.columns]\n        DTYPE = [(x, df[x].dtype) for x in df.columns]\n        g = np.rec.array(OBJ, dtype=DTYPE)\n        with h5py.File(fp,'w') as f:\n            f.create_dataset('locs', data=g)\n        # 拷贝yaml文件记录\n        shutil.copy(base+'.yaml', fp_yaml)\n        print('[Saved to] {}'.format(fp))\n```\n\n运行起来也非常简单，只要把符合要求的数据文件的路径输入进`Spot_Filter`类中，初始化一个对象，然后调用方法即可：\n\n![image-20210415114955338](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210415114955338.png)\n\n注意，如果只是想对原来的数据做随机抽样而不做结构筛选，可以把 `a.run()` 替换为 `a.sampling(frac=0.5)` 。\n\n","tags":["KMeans","itertools","sklearn","聚类","过滤","抽样"],"categories":["python"]},{"title":"文献阅读日志汇总小工具","url":"/2021/04/14/summarize_tool_for_reading_notes/","content":"\n阅读文献产生的零碎的记录，也需要时不时汇总然后 review 一下。因此我自定义了规则，但凡阅读文献的记录块，其三级标题带 `:apple:` 标记，所以我改写了之前汇总实验日志的代码，非常小的修改，增加了代码的复用性，方便以后还有什么其他的类型的记录需要汇总收集，如下所示：\n\n```python\nimport os, glob, time \n\ndef markdown_parser(fp, emoji_tag=':apple:', **params):\n    with open(fp, 'r', encoding='utf-8') as f:\n        a = f.readlines()\n        \n    # 找到所有的三级标题\n    tids = []\n    for idx, line in enumerate(a):\n        if line.startswith('### '):\n            tids.append(idx)\n            \n    # 对三级标题进行遍历，带emoji_tag的收集该部分内容\n    data = []\n    for idx, tid in enumerate(tids):\n        if a[tid].startswith('### {}'.format(emoji_tag)):\n            if idx+1 < len(tids):\n                data.extend(a[tids[idx]:tids[idx+1]])\n                # 如果最后一个标题是apple，+1会报错\n            elif idx == len(tids)-1: \n                data.extend(a[tids[idx]:])\n    \n    # 如果当天日志没有阅读部分内容，返回False\n    if len(data)>0:\n        return data\n    else:\n        return False \n \ndef summarize(wks, prefix='read', title_prefix='阅读记录汇总', **params):\n    # 准备汇总文件相关信息\n    tt = time.time()\n    tl = time.localtime(tt)\n    dd = time.strftime('%Y%m%d',tl)\n    summary_filename = \"{}-{}.md\".format(prefix, dd)\n    timeStamp = time.strftime('%Y-%m-%d %H:%M:%S',tl)\n    frontMatter = \"---\\ntitle: {}-{}\\nexcerpt: \\ndate: {}\\ntags: \\ncategories: \\n- 工作\\n---\\n\".format(title_prefix, dd, timeStamp)\n    all_info = [frontMatter]\n    \n    # 读取文件列表并按照创建时间排序，最新的id为-1\n    fileList = glob.glob(wks+'/*.md')\n    logs = sorted(fileList, key=os.path.getctime)\n    \n    # 收集带有exp前缀的文件id\n    sids = []\n    for idx, fp in enumerate(logs): \n        _, fname = os.path.split(fp)\n        if fname.startswith(prefix):\n            sids.append(idx)\n\n    # 按照已有记录收集新近创建的日志文件\n    parts = logs[sids[-1]+1:]\n    \n    # 遍历文档完成带`apple`标签部分段落的收集\n    for fp in parts:\n        _, fname = os.path.split(fp)\n        fdate, _ = fname.split(\".\")\n        memos = markdown_parser(fp, **params)\n        if memos:\n            all_info.append(\"\\n\\n## {}\\n\\n\".format(fdate))\n            # 防止前面的内容没有换行，二级标题失效\n            all_info.extend(memos)\n    \n    # 写入所有收集的信息到文档\n    fpp = os.path.join(wks, summary_filename)\n    with open(fpp, 'w', encoding='utf-8') as f: \n        for part in all_info:\n            f.write(part)\n    print('{}成功!'.format(title_prefix))\n\n\nif __name__==\"__main__\":\n    ## the path to where you store logs\n    wks = r'd:\\log\\main\\source\\_posts'  \n\n    ## summarize the `:apple:` reading parts\n    summarize(wks, prefix='read', title_prefix='阅读记录汇总', emoji_tag=':apple:')\n\n    ## summarize the `:memo:` log parts\n    # summarize(wks, prefix='exp', title_prefix='实验记录汇总', emoji_tag=':memo:')\n```\n\n","tags":["yaml","markdown"],"categories":["python"]},{"title":"jupyterlab尝鲜","url":"/2021/04/13/try_jupyter_3/","content":"\n发现 jupyter lab 已经发布了 3.0 的版本，[具有不少新特性](https://finance.sina.com.cn/tech/2021-01-07/doc-iiznezxt1033961.shtml)，遂决定安装一个试试：\n\n```bash\nconda install -c conda-forge jupyterlab=3\n```\n\n结果却是报错：\n\n```\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\n```\n\n刚开始我还以为是自己的镜像源设置有问题，后来到网上查了一圈，最后决定更新一下：\n\n```bash\nconda update --all\n```\n\n把所有的包尽可能地更新到最新版本，终于解决了问题，**而且 jupyter lab 也自动更新到了 3.0 版本**。然后是安装中文语言包，由于 PyPI 和 TUNA 的镜像都还没有，[还得自己下载轮子](https://blog.csdn.net/yy_diego/article/details/114281868)：\n\n```bash\nwget https://jfds-1252952517.cos.ap-chengdu.myqcloud.com/jupyterhub/jupyterlab_language_pack_zh_CN-0.0.1.dev0-py2.py3-none-any.whl\npip install xxx.whl\n```\n\n然后就是[安装各种插件](https://www.cnblogs.com/feffery/p/13364668.html)，我先到 jupyterlab 的插件管理器里边找了找，但是发现直接点击按钮安装会报错，总之就是 jupyter labextension 还是有些问题，所以我只能选择 pip 的方式安装插件，首先我就安装了一个 [code snippet 的插件](https://github.com/jupytercalpoly/jupyterlab-code-snippets)，可以保存自己高度复用的一些代码块。\n\n```bash\npip install jupyterlab-code-snippets\n```\n\n> snippets 的代码块会以 json 文件保存到 启动目录的 snippets 子目录下。\n\n第二个插件就是代码补全了，到网上找了一下，决定使用很多人推荐的 [kite](https://github.com/kiteco/jupyterlab-kite#installing-the-kite-extension-for-jupyterlab)。然后执行代码：\n\n```bash\npip install \"jupyterlab-kite>=2.0.2\"\n```\n\n> 安装报错，显示文件名或拓展名太长，可以参考[如何在Windows 10中更改默认256个字符路径限制(MAX_PATH)](https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html)修改注册表解决。\n>\n> HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem>LongPathsEnabled=1\n>\n> 使用该插件需要保持 kite 引擎处于开启状态，第一次使用时会初始化，indexing 时间比较长。\n\n第三个插件是 [matplotlib 交互插件](https://github.com/matplotlib/ipympl#readme)，因为我经常作图，可以使用 jupyter-matplotlib 插件：\n\n```\npip install ipympl\n```\n\n在使用这个插件的时候，需要输入魔法命令：\n\n```ipynb\n%matplotlib widget\n```\n\n注意，如果要导出 notebook 为嵌入网页，这种交互式的图表不会显示，再次打开 notebook 也不会显示，所以平时最好还是使用 `%matplotlib inline`。\n\n第四个插件是显示执行时间的插件 [jupyterlab-execute-time](https://github.com/deshaw/jupyterlab-execute-time)，方便我知道每一个 cell 具体的运行时间戳，有助于回溯记录：\n\n```\npip install jupyterlab_execute_time\n```\n\n开启这个功能，需要到  Settings->Advanced Settings Editor->Notebook 里边添加用户设置`{\"recordTiming\": true}`，因为默认的这个 false。\n\n> 更多关于 jupyterlab 的插件请参考[这个 github 项目](https://github.com/mauhai/awesome-jupyterlab)。\n\n最后，创建一个打开 jupyterlab 并且指定工作目录的快捷方式：\n\n![image-20210413214741109](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210413214741109.png)\n\n```\n[...] C:\\Users\\$USERNAME$\\anaconda3\\Scripts\\jupyter-lab-script.py \"D:\\fire\\02-jupyterSpace\"\n```\n\n修改快捷方式的目标内容如上所示即可。\n\n","tags":["jupyterlab"],"categories":["python"]},{"title":"单指数分布数据的作图","url":"/2021/04/12/singleExpDist_plot/","content":"从博士到现在，originLab 用的越来越少， python 用得越来越多，不仅仅是数据分析，作图也是如此，这篇文章简单记录了绘制单指数直方分布图表的搞法。\n\n### 作图效果\n\n先放效果图：\n\n![image-20210412141534347](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210412141534347.png)\n\n左侧是一个单指数下降的直方分布（关于 τ~on~ ），右侧是一个累积分布（关于 τ~off~ ），数据点我就用圆圈 marker 来表示。然后曲线是单指数拟合（$y=a*(1-np.exp(-x/b))+c$）的曲线，而虚线是根据拟合函数的参数，计算出来的 τ 时间。\n\n### 读取数据\n\n```python\nimport os, glob\nimport pandas as pd\n\nwks = '/path/to/your/data'  \n# 存放数据的目录\nfiles = glob.glob(os.path.join(wks, '*dark.hdf5'))  \n# 批量读取带特定后缀的文件\ndata = pd.read_hdf(files[2], key='locs').filter(items=['len','dark'])\n# 使用 pandas 模块读取数据文件，并且过滤出来其中两列，列名分别为 len 和 dark\n```\n\n这一段代码可圈可点：\n\n1. 使用`os.path.join`组合路径，然后使用`*`通配符，借助 `glob` 函数批量读取带有特定后缀的文件，返回符合特征的文件路径列表，这种处理文件路径的方式比较轻松。不过 `glob` 读文件的列表顺序并不是按照命名来的，这个要注意下。\n2. hdf5 文件是常见的数据文件，python 中有 `h5py`模块来完成对此类文件的读取和写入，我这里为了方便，直接使用 pandas 模块中提供的方法来读取数据，适用于数据文件内容为表格或者矩阵类型的情况。\n3. 原始表格比较大，很多列的信息其实这里并没有被用到，所以我使用 `pd.DataFrame` 对象所拥有的 `filter`方法实现对数据列的过滤。\n\n### 直方分布作图代码\n\n```python\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(2,1.3))\n# 定义 figure 的大小，\n# 注意 figsize 参数接收元组 (width, height)\n# 宽和高的单位是 inch\n\nres = plt.hist(data['len'], bins=10, color='lightgray',edgecolor='gray')\n# 对某一列数据做直方分布图\n# 直方的数量（bins）定为 10 \n# 直方的颜色（color）为 浅灰色\n# 直方边缘的颜色（edgecolor）为 普通灰色\n# plt.hist 会返回直方分布统计的数据\n\nparams, err = refFit(res)\n# 使用自定义函数来对直方分布统计数据进行拟合\n\nX, y = extractFromHist(res)\n# 自定义函数提取直方分布统计数据\na, b, c = params\nX2 = np.linspace(0.07, 1,100)\ny2 = func(X2, a, b, c)\n# 带入参数到拟合函数，得到具体的拟合数值\nplt.plot(X2, y2)\n# 绘制拟合曲线\n\nplt.xlim(0.07,0.9)\nplt.xticks([0.2*x+0.1 for x in range(5)])\n# 定义横坐标范围和坐标轴标签\n\nymin, ymax = 0, 250\nplt.ylim(ymin, ymax)\nplt.yticks(([120*x for x in range(3)]))\n# 定义纵坐标范围和标签\n# plt.show()\nplt.plot([b,b],[ymin, ymax], 'r', linestyle='dashed')\n# 用虚线绘制 Tau 所在位置\nplt.savefig(os.path.join(wks,'plot_result.svg'))\n# 保存作图结果为 svg 矢量图文件\nprint(b, err)\n# 打印关键的参数\n```\n\n运行结果如下：\n\n![image-20210412143949078](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210412143949078.png)\n\n绘制 figure 还是有很多具体参数需要根据具体情况来调整的。一般我都是先使用默认最简单的 `plt.plot` 命令作图看看情况，然后调整：\n\n1. figure 整体的大小\n2. 图的一些颜色和样式\n3. 横纵坐标轴\n\n差不多了我就会导出为矢量图文件，后面可以使用 Adobe Illustrator 软件做更为细致的调整。比如字体字号之类的。\n\n### 累积分布作图代码\n\n```python\nres = plt.hist(data['dark'], bins=300, cumulative=True, color='lightgray',edgecolor='gray',density=True)\nplt.show()\n\nparams, err = refFit(res)\nX, y = extractFromHist(res)\na, b, c = params\nX2 = np.linspace(X[0], X[-1],100)\ny2 = func(X2, a, b, c)\nsmsm = sampling(X, y, frac=0.2)\nX3 = smsm.x \ny3 = smsm.y\nplt.figure(figsize=(2,1.3))\nplt.plot(X3, y3, 'o', mfc='lightgray', mec='black', mew=0.25, markersize=8)\nplt.plot(X2, y2)\nplt.xlim(0, 320)\nplt.xticks([80*x for x in range(5)])\n\nymin, ymax = -0.1, 1.1\nplt.ylim(ymin, ymax)\nplt.yticks(0, 1)\nplt.plot([b,b],[ymin, ymax], 'r', linestyle='dashed')\n\n# plt.show()\nplt.savefig(os.path.join(wks,'plt_result-2.svg'))\nprint(b, err)\n```\n\n作图效果如下：\n\n![image-20210412144636727](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210412144636727.png)\n\n这里就不作赘述，只提一下`plt.plot`中的几个参数：\n\n1. mfc：marker face color，可以理解为 marker 的填充颜色\n2. 'o'：定义圆圈作为 marker\n3. mec： marker 的 edge color，边缘颜色\n4. mew：marker 的轮廓线条粗细\n5. markersize：marker的大小 \n\n其实这些东西，如果使用 jupyter notebook，都可以直接使用 `?plt.plot`来查看。\n\n### 自定义函数\n\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef func(x, a, b ,c):\n    '''\n    文献中提供的on_time, dark_time的cdf曲线拟合函数\n    '''\n    return a*(1-np.exp(-x/b))+c\n\ndef refFit(test):\n    '''\n    利用curve_fit进行曲线拟合计算参数\n    返回拟合函数的参数，以及±误差值\n    '''\n    X, y = extractFromHist(test)\n    p0 = (1000, 10, 20)  \n    # 初始化参数还挺重要的\n    params, cv = curve_fit(func, X, y, p0)\n    perr = np.sqrt(np.diag(cv))\n    return params, perr[1]\n\ndef sampling(X, y, frac=0.3):\n    '''\n    基于dataframe的方法对数据进行随机采样\n    '''\n    data = pd.DataFrame()\n    data['x'] = X \n    data['y'] = y \n    return data.sample(frac=frac)\n\ndef extractFromHist(test):\n    '''\n    test: plt.hist收集的直方分布数据\n    '''\n    test_count = test[0]\n    test_bin = test[1]\n    interval = test_bin[1]-test_bin[0]\n    test_value = [x-interval/2 for x in test_bin[1:]]\n    X = test_value\n    y = test_count\n    return X, y\n```\n\n","tags":["histogram","matplotlib"],"categories":["python","plot"]},{"title":"自动化DNA-PAINT数据分析流程","url":"/2021/04/08/auto_paint_data_analysis_pipeline/","content":"\n多组数据的尝试性分析过程中，常常需要调整一些参数，之前过于依赖 GUI，往往只能一次分析一个数据，效率较低。为了减少工作量，我决定使用 python 脚本，通过调用 picasso 提供的命令行工具，实现多个分析步骤的自动化整合。\n\n第一步先导入需要的包（没错我就是【调包侠】:laughing:）：\n\n```python\nimport os, glob\n```\n\n### 如何调用命令行工具\n\n很多软件都会提供命令行工具。一般而言，就是可以在 【cmd 窗口】中，靠输入命令文本运行的意思。而使用 python 的 `os` 或者 `subprocess` 等模块，则可以实现自动化的调用。在这个项目中，我打算使用最简单的 `os.system` 来实现命令行工具的调用，具体代码如下：\n\n```python\nbase_command = \"conda activate picasso && python -m picasso\"\n# 注意把 anaconda 添加到用户和系统的环境变量中\n\ndef undrift(fp, **params):\n    '''\n    对loc之后的数据做RCC自动漂移校正\n    '''\n    os.system(base_command+' undrift \"{}\"'.format(fp))\n    # 地址还是加上引号，否则里边的空格会造成错误\n\ndef cluster(fp, radius=0.25, min_density=10, **params):\n    '''\n    使用dbscan对`_undrift.hdf5`数据进行聚类得到单点\n    '''\n    os.system(base_command+' dbscan \"{}\" {} {}'.format(fp, radius, min_density))\n\ndef link(fp, distance=0.5, tolerance=1, **params):\n    '''\n    对聚类后的数据进行link\n    '''\n    os.system(base_command+' link -d {} -t {} \"{}\"'.format(distance, tolerance, fp))\n\ndef dark(fp, **params):\n    '''\n    对link之后的数据求dark\n    '''\n    os.system(base_command+' dark \"{}\"'.format(fp))\n```\n\n这里边有几个需要注意的点：\n\n1. 命令文本要遵守命令行工具的调用规范\n2. 注意把 anaconda 添加到用户和系统的环境变量中\n3. 把文件地址传参进入文本，注意加双引号，否则路径的空格会造成错误\n\n### 如何整合多个过程\n\n注意每个命令行工具对某个文件处理后，都会生成新的文件，而且修改后缀。然后下一个步骤又会对这个新生成的文件进行处理。为了方便这种 IO 和调参，我采用了字典的方式来存放函数，字典的键名就是文件的后缀特征，键值就是要处理这种文件需要调用的函数，具体如下：\n\n```python\nif __name__ == \"__main__\":\n    wks = \"/path/to/your/data\"\n    \n    func_dict = {\n        'locs': undrift,\n        'undrift': cluster,\n        'dbscan': link,\n        'link': dark,\n    }\n\n    for key in func_dict.keys():\n        fps = glob.glob(os.path.join(wks,\"*{}.hdf5\".format(key)))\n        for fp in fps:\n            func_dict[key](fp, \n                           radius=0.3,\n                           min_density=20,\n                           distance=0.4,\n                           tolerance=3,\n                           )\n```\n\n这里需要注意几点：\n\n1. 函数字典中，键值就是函数名，已经预先定义好了\n2. 为了方便调参，每个函数的参数都包含 `**params`，因此在调用时必须是在函数小括号内以`key=value`的形式传参\n3. glob 支持 unix style 的文件路径通配， 我一般用 `*` 通过匹配任意字符串\n\n","tags":["os","glob"],"categories":["python"]},{"title":"标准化DNA-PAINT数据分析处理流程","url":"/2021/04/07/standard_PAINT_analysis_pipeline/","content":"\n博士后阶段做 DNA-PAINT 相关工作，在数据分析这块经历颇丰，刚开始各种不会用 picasso 这个软件，写了很多小代码。到后来想着要建立标准化流程方便“传道”于后来者，又不断减少代码含量，几番迭代之后得到这份 protocol，基本算是完结。\n\n### 单分子定位\n\n:one: **将 lif 转为 tif**\n\n用 ImageJ 软件打开 Leica 成像视频数据，保存为 tif，方便后面使用 picasso 软件分析。\n\n> 注意每个视频文件应当保存到一个新的空白文件夹内部，后续分析的所有中间文件都存储于该目录下。\n\n:two: **记录视频基本信息**\n\n在 ImageJ 中打开了视频之后，点击图像窗口，按快捷键 `Ctrl+I`，打开 Info 窗口，保存为 txt，方便提取其中关键信息。可以使用`collectInfo.py`脚本：\n\n```python\ndef collectInfo(fp):\n    import yaml\n    recs = {}\n    with open(fp, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    a = 'Image|ATLCameraSettingDefinition|'\n    b = 'Image|ATLCameraSettingDefinition|WideFieldChannelConfigurator|WideFieldChannelInfo|'\n    for line in lines:\n        if line.startswith\\\n        ('{}CycleCount'.format(a)):\n            _, v = line.split('= ')\n            recs['Frame'] = float(v)\n        elif line.startswith\\\n        ('{}CycleTime'.format(a)):\n            _, v = line.split('= ')\n            recs['FrameInterval'] = float(v)\n        elif line.startswith\\\n        ('{}EMGainValue'.format(a)):\n            _, v = line.split('= ')\n            recs['EMGain'] = float(v)\n        elif line.startswith\\\n        ('{}BlackValue'.format(b)):\n            _, v = line.split('= ')\n            recs['Offset'] = float(v)*pow(2,16)\n        elif line.startswith\\\n        ('{}WFLaserChannelInfo|WFLaserChannelInfo_Wavelength'.format(b)):\n            _, v = line.split('= ')\n            recs['Laser'] = int(v) \n        elif line.startswith\\\n        ('{}WFLaserChannelInfo|WFLaserChannelInfo_CurrentValue'.format(b)):\n            _, v = line.split('= ')\n            recs['LaserPower'] = '{}%'.format(float(v))\n    base, ext = fp.split('.')\n    with open(base+'.yaml', 'w', encoding='utf-8') as f: \n        yaml.dump(recs, f)\n        \nif __name__ == \"__main__\":\n    fp = r\"E:\\TIRF\\DNA-PAINT-3-points-ssDNA-6HB\\Info for ssDNA.txt\"\n    collectInfo(fp)\n```\n\nfp 为指向 Info 的文件路径。执行该脚本后，会从`Info of ?.txt`文本中提取视频的一些关键信息，保存到`Info of ?.yaml`文件中，如下所示：\n\n```yaml\nEMGain: 100.0\nFrame: 5000.0\nFrameInterval: 0.0402221993301064\nLaser: 638\nLaserPower: 100.0%\nOffset: 1010.0154116121166\n```\n\n注意里边的 EMGain、Offset 参数都是后边使用 picasso 软件做 localize 的时候需要的。FrameInterval 的单位是 sec 。\n\n:three: **运行 picasso-localize 模块并打开视频文件**\n\n在 picasso 工具箱目录中有启动相应模块的 bat 脚本文件，双击运行，弹出窗口，然后把 ImageJ 导出的 tif 格式的视频文件拖拽到 localize 的窗口内完成 open 。\n\n:four: **设定 localize 窗口显示的 contrast**\n\n为了方便后面设置合适的参数，对单分子荧光光斑的选中情况进行预览，需要手动设置一下 contrast 以达到最佳的效果。按快捷键 `Ctrl+C` 打开。\n\n:five: **设置 localize 的基本参数**\n\n快捷键`Ctrl+P`打开单分子定位参数配置选项，参数设置参考下表：\n\n| 参数名            | 参数值     | 备注                   |\n| ----------------- | ---------- | ---------------------- |\n| box side length   | 7          | 默认（固定）           |\n| Min. Net Gradient | 5000       | 通过 preview 确定      |\n| EM Gain           | 100        | 通过视频 Info 确定     |\n| Baseline          | 1000       | 通过视频 Info 确定     |\n| Sensitivity       | 16.9       | 相机参数（固定）       |\n| QE                | 0.9        | 荧光基团参数（固定）   |\n| Pixelsize         | 160        | 1 px = 160 nm （固定） |\n| Fit               | LQ，Gpufit | GPU加速（固定）        |\n\n:six: **单分子定位**\n\n设置好参数后，按快捷键`Ctrl+L`开始运行。运行结束后，数据自动保存到视频文件所在目录下，文件后缀`?_locs.hdf5`。\n\n### 漂移校正\n\n打开 picasso-render 模块，把后缀`?_locs.hdf5`的数据文件拖拽入 render 窗口，按`Ctrl+D` 打开显示设置窗口，调节`zoom`和`contrast`等参数。然后按`Ctrl+T` 设置圈选工具及相关参数。再按`Ctrl+P`圈选 marker 的漂移图案。选中若干个之后按`Ctrl+Shift+U`，执行漂移校正，校正后`Ctrl+S`手动保存定位数据，文件名后缀命名为`?_undrift.hdf5`。\n\n> 目前实验室仍使用荧光小球作为界面漂移的 fiducial marker，在整个视频中保持常亮，每一帧都有点累积到，所以在 render 窗口中 marker 的图案是最亮而且 pattern 一致的（设置比较大的contrast）。\n\n### 圈选结构\n\n重新用 render 打开名为`?_undrift.hdf5`的文件，然后按`Ctrl+D` 打开显示设置窗口，调节`zoom`和`contrast`等参数。按`Ctrl+T` 设置圈选工具及相关参数（对于三点六螺旋，一般 diameter = 2.5 px）。再按`Ctrl+P`手动圈选 所PAINT结构 的图案。如果背景很低且密度合适，可以按`Ctrl+Shift+P`自动选中相似的结构。一般一个数据能够圈选出 200 - 300 个结构为宜。圈选临时停止或完成都建议及时保存圈选记录（File > Save pick regions），会将记录保存到一个 yaml 文件。还需要保存圈选的localization（快捷键`Ctrl+Shift+S`），得到一个名为`?_picked.hdf5`文件\n\n### 图像平均\n\n使用 picasso-average 模块打开名为`?_picked.hdf5`的文件，拖拽打开即可。然后直接按`Ctrl+A`执行图像平均。程序运行结束后，手动保存数据，得到一个名为`?_avg.hdf5`文件。使用`getAverageImage.py`脚本获取平均化的结构超分辨图像，代码如下：\n\n```python\nimport os, glob \nimport h5py\nimport pandas as pd \nimport numpy as np\nfrom skimage import io \n\ndef readHDF5(filepath):\n    '''\n    将hdf5数据文件转为pandas.dataframe\n    '''\n    dataset = h5py.File(filepath, 'r')\n    data = {}\n    for key in dataset.keys():\n        print('dataset>{}'.format(key))\n        data[key] = pd.DataFrame(dataset[key][()])\n        return data\n\ndef exportImage(filepath, locs, zoom=20):\n    ylim = (locs.y.min(), locs.y.max())\n    xlim = (locs.x.min(), locs.x.max())\n    row = round((ylim[1]-ylim[0])*zoom)\n    col = round((xlim[1]-xlim[0])*zoom)\n    img = np.zeros(shape=(row, col), dtype='int64')\n    for idx, rec in locs.iterrows():\n        if rec['y']>=ylim[0] and rec['y']<ylim[1] and rec['x']>=xlim[0] and rec['x']<xlim[1]:\n            # 过滤掉在range在的点\n            img[int((rec['y']-ylim[0])*zoom),int((rec['x']-xlim[0])*zoom)] += 1\n            # 注意坐标转换\n    io.imsave(filepath, img.astype('int16'))  #注意使用16-bit格式保存\n    print('平均化图像已保存至 {}'.format(filepath))\n\ndef main(filepath, zoom=20):\n    data = readHDF5(filepath)\n    locs = data['locs'].filter(items=['x','y','cluster'])\n    exportImage(filepath+\"_pixelSize-{}nm.tif\".format(160/zoom), locs, zoom=zoom)\n\nif __name__ == \"__main__\":\n    fp = r\"E:\\TIRF\\DNA-PAINT-3-points-ssDNA-6HB\\ssDNA_locs_render.hdf5_cluster_filtered_picked_picked_avg.hdf5\"\n    main(fp, zoom=40)\n```\n\n注意 fp 为指向`?_avg.hdf5`文件的路径，main 函数中的 zoom 参数代表生成图像的放大倍数，比如原始图像 1 pixel = 160 nm，放大 40 倍之后， 1 pixel = 4 nm。得到一个 tif 图像后，使用 ImageJ 做后续处理。\n\n### 分辨率测量\n\n得到平均化的 tif 图像之后，用 ImageJ 打开，先设置 scale（Analyze > Set Scale），然后选择直线工具，在图像上沿着结构画线。以六螺旋三点结构为例，则该线段需要贯穿三点中心。然后按`Ctrl+K`执行 Plot 功能，得到沿该线段的强度变化值，可以在 Plot 窗口中保存 data 到 csv 文件，然后用 OriginLab 打开，使用 Peak Analyzer 完成空间分辨率的测量（半峰宽）。\n\n> 文献中常用定位精度（Localization Precision）的概念，与空间分辨率不是一回事。一般我们认为空间分辨率就是高斯分布的半峰宽（FWHM），而定位精度（σ）与 FWHM 的数学转换关系如下：\n>\n> FWHM = 2 × sqrt(2 × ln2) × σ ≈ 2.355 × σ \n\n### 基础 Kinetics 分析 \n\n该部分属于进阶数据分析，一般DNA-PAINT用户无需进行此环节。\n\n:one: **聚类分簇（Cluster）**\n\n一般被 PAINT 出来的结构包含若干个点簇，可以被认为是独立的结合位点（包含一条或者紧密排布的若干条 docking sequence）。PAINT 的结合动力学分析需要基于单个的结合位点开展。手动圈选 200 - 300 个结构之后，可以使用 render 中的聚类工具来实现分簇。 render 中提供了以下工具：\n\n1. K-Means （基于指定数目的聚类，需手动 filter）\n2. DBSCAN （基于密度聚类，需设定范围和密度阈值）\n3. HDBSCAN （基于密度聚类，默认参数即可，但速度很慢，需额外安装 `hdbscan` ）\n\n**推荐使用 K-Means** 完成聚类。圈选了结构之后，可点击 Tools > Cluster in pick (k-means) 打开该工具。 指定聚类数目为 3 （以三点结构为例），gap = 1，然后开始遍历筛选圈中的结构。\n\n> 注意筛选过程中，可以修改聚类数目，使得杂点归入新类，再去勾选杂点类，实现过滤。另外使用该方法聚类依赖于计算机性能，建议不要圈选超过 200 个结构（Core i7 8th，16 GB 内存），否则在遍历过程中程序会崩溃（calculating pick properties 有 bug，已修复）。\n\n聚类结束后，会生成`?_cluster.hdf5`和`?_pickprops.hdf5`文件，可以使用 OriginLab 导入 `?_pickprops.hdf5`文件，对 len_mean 和 dark_mean 两列数据进行查看，当其分布接近正态分布时，直接取均值，当其分布接近指数分布时，使用单指数拟合求取 τ 值。\n\n> 注意每个 cluster 中可能包含了多个 blink 事件， 每个 blink 事件都有对应的 T~on~ 和 T~off~，而 len_mean 则是多个 blink 事件的 T~on~ 的均值， dark_mean 以此类推。当对单个 blink 事件的 T~on~ 和 T~off~ 做直方统计，则一般为指数分布。\n\n### cluster 转 pick \n\n得到了结构分簇信息之后，可以将每个簇的中心坐标提取出来，设置半径，实现对分簇的圈选，再结合 render 中的 Show Info 功能快速查看（快捷键`Ctrl+I`）NeNA 定位精度以及 Kinetics 信息，非常方便。这里可以使用`getClusterCenter.py`脚本完成坐标提取并自动生成指定 radius 的圈选 yaml 文件记录。代码如下：\n\n```python\nimport pandas as pd \nimport yaml \n\ndef cluster2picks(fp, radius):\n    '''\n    fp: 使用filterPicks.py 获得的 localizations 文件地址\n    '''\n    data = pd.read_hdf(fp, key='locs')\n    gp = data.groupby(by=['group','cluster'])\n    recs = {'Centers':[], 'Diameter':2*radius,'Shape':'Circle'}\n    for key, df in gp:\n        x = float(df['x'].mean())\n        y = float(df['y'].mean())\n        recs['Centers'].append([x, y])\n    with open(fp+'_2picks.yaml', 'w') as f:\n        yaml.dump(recs, f)\n\nif __name__ == \"__main__\":\n    fp = r\"E:\\TIRF\\a-20210407\\*\\*_cluster.hdf5\"\n    radius = 0.25\n    cluster2picks(fp, radius)\n```\n\nfp 为指向`?_cluster.hdf5`文件（使用 K-Means 聚类生成）的路径，radius 为圈选圆框的半径，单位为像素（目前实验室分辨率是 30-40 nm，相当于 0.25 px）。该脚本会执行结束后在同目录下生成`?_2picks.yaml`记录，可将`?_cluster.hdf5`文件拖入 render 中打开，去除  group 信息（Postprocess > remove group info），然后从 render 中导入该圈选记录（File > Load pick regions），然后查看Info。\n\n### 单 Blink 事件分析\n\n基础 kinetics 分析中只能获取某一个分簇的平均 T~on~ 和 T~off~，而文献中实际是对单个闪烁事件进行统计分析。因此要从更为原始的数据进行分析。\n\n使用 render 打开 `?_undrift.hdf5` 文件，导入分簇圈选记录`?_2picks.yaml`，然后`Ctrl+Shift+S` 保存新的 picked_localization，修改后缀为`?_single.hdf5`。\n\n打开 conda 并激活 picasso 环境：\n\n```bash\nconda activate picasso\n```\n\n然后使用 picasso 提供的命令行工具对`?_single.hdf5` 进行 link 计算：\n\n```bash\npython -m picasso link -d 0.5 files\n```\n\nfiles 为指向一个或者多个`?_single.hdf5` 文件路径（ a unix style path pattern），该命令的参数说明可以 `python -m picasso link -h` 查看。d 代表最大距离（像素单位）。该步骤完成后带后缀`?_link.hdf5`的文件。\n\n再在该命令行中继续执行 dark 计算：\n\n```bash\npython -m picasso dark files\n```\n\nfiles 为指向一个或者多个`?_link.hdf5` 文件路径。运行结束后生成`?_dark.hdf5`文件。此文件可以使用 picasso-filter 快速查看内容，表格中应该包含了 len 和 dark 两列。接下来可以使用 OriginLab 导入`?_dark.hdf5`文件，对 len 和 dark 两列的直方分布或者累积直方分布做单指数拟合，求得 τ~on~ 和 τ~off~ 。\n\n### 计算 K~on~ 和 K~off~\n\n得到了 PAINT 中单个 docking 位点的两个时间（ τ~on~ 和 τ~off~）之后，需要换算才能得到 K~on~ 和 K~off~ 两个参数。可以使用`getKineticsFromTime.py`脚本：\n\n```python\ndef getKinetics(t_b, t_d, c_imager):\n    '''\n    t_b: 统计得到的平均bright time，单位秒\n    t_d: 统计得到的平均dark time\n    c_imager: imager的浓度，单位为nM\n    '''\n    c_imager = c_imager*1e-9 # 完成浓度单位转化\n    k_off = 1/(t_b)\n    k_on = 1/(c_imager*t_d)\n    message = '''# Kinetics\n    t_off: {} (s)\n    t_on: {} (s)\n    k_off: {} (1/s)\n    k_on: {} (/M/s)\n    '''.format(t_d, t_b, k_off, k_on)\n    print(message)\n\nif __name__==\"__main__\":\n    c_imager = 5  # 单位为 nM\n    frame_interval = 0.32  # 单位为秒\n    t_b = 2.41*frame_interval\n    t_d = 370.44*frame_interval\n    getKinetics(t_b, t_d, c_imager)\n```\n\n此脚本需要输入 PAINT 实验中使用的 Imager 的浓度，拍摄视频的每帧时间间隔（不同于曝光时间）。`t_b` 就是 bright time，`t_d` 就是 dark time，通过前面的分析获得。\n\n### 提取强度时间曲线\n\n有些时候需要选取具有代表性的某个 docking 位点的强度时间曲线，了解实际的信号闪烁情况，可以使用`getSpotTrace.ipynb`脚本（在 jupyter notebook 中运行）：\n\n```python\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nimport matplotlib.pyplot as plt\nimport os, glob\n\nimport yaml \nimport pandas as pd \nimport numpy as np \nfrom skimage import io \n\n\ndef load_picks(filepath):\n    with open(filepath, 'r', encoding=\"utf-8\") as f:\n        data = yaml.load(f, Loader=yaml.BaseLoader)\n    picks = pd.DataFrame(data['Centers'], columns=['x','y'], dtype='float32')\n    return picks\n\n\ndef load_drift(filepath):\n    with open(filepath, 'r') as f:\n        lines = f.readlines()\n    N = int(len(lines)/2)\n    recs = []\n    for i in range(N):\n        if i>0:\n            recs.append(lines[2*i])\n    recs = list(map(lambda x: list(map(lambda x:float(x), x.replace(\"\\n\",\"\").split(\" \"))), recs))\n    recs = pd.DataFrame(recs, columns=['dx', 'dy'], dtype=\"float64\")\n    return recs\n\n\ndef load_movie(filepath):\n    return io.imread(filepath, plugin='pil')\n\n\nclass Spots:\n    def __init__(self, pick_region_yaml, drift_txt, origin_tif, picked_locs_hdf5):\n        self.picks = load_picks(pick_region_yaml)\n        self.drift = load_drift(drift_txt)\n        self.movie = load_movie(origin_tif)\n        self.locs = load_pick_locs(picked_locs_hdf5)\n        self._gp = self.locs.groupby('group')\n        self.load_info = 'Movie shape: {}\\nPicks number: {}\\n'\\\n        .format(self.movie.shape, len(self.picks))\n        print(self.load_info)\n    \n    def fetch(self, sid):\n        row = self.picks.iloc[sid]\n        x, y = row\n        xx = x - self.drift['dx']\n        yy = y - self.drift['dy']\n        xx = xx.astype('int64')\n        yy = yy.astype('int64')\n        on_frame = self._gp.get_group(sid)['frame'].values\n        intensity, crop = self.getRegionIntensity(xx, yy, on_frame)\n        status = self.mark_status(on_frame)\n        return intensity, status, crop\n    \n    def getRegionIntensity(self, xx, yy, on_frame):\n        intensity = []\n        crop = []\n        for idx, x in enumerate(xx):\n            y = yy[idx]\n            frame = self.movie[idx]\n            region = frame[y-3:y+4, x-3:x+4]\n            if idx in on_frame:\n                value = region.max()\n            else:\n                value = region.min()\n            intensity.append(value)\n            crop.append(region)\n        return intensity, crop\n    \n    def mark_status(self, on_frame):\n        N = len(self.movie)\n        on_ = 0.1*self.movie.ptp()\n        off_ = 0\n        status = []\n        for i in range(N):\n            if i in on_frame:\n                v = on_\n            else:\n                v = off_\n            status.append(v)\n        return status\n```\n\n把上述写好的类和方法放到一个 cell 中执行编译，接下来开始调用：\n\n```python\nfp1 = r\"?_2picks.yaml\" \nfp2 = r\"?_drift.txt\"\nfp3 = r\"movie.tif\"\nfp4 = r\"?_single.hdf5\" \npicks = Spots(fp1, fp2, fp3, fp4)\n```\n\n注意这里的 fp 一共有四个，分别指向不同的文件：\n\n1. `?_2picks.yaml` : 聚类分簇转换得到的单点圈选记录文件\n2. `?_drift.txt` : 漂移校正产生的记录文件\n3. `movie.tif` : 原始视频文件\n4. `?_single.hdf5` : 根据`?_2picks.yaml` 保存的 picked_localization 文件\n\nSpots 对象初始化后，可以 single docking site 的 ID 进行索引和查看强度时间曲线：\n\n```python\nintensity, status, crop = picks.fetch(1)\nplt.figure(figsize=(12,3))\nplt.plot(intensity-min(intensity))\nplt.plot(status)\nplt.show()\n```\n\n### 其他需求的说明\n\npicasso 软件产生的中间数据主要是表格形式，且均采取 hdf5 文件格式保存，可以自行使用 OriginLab 导入进行后续数据分析，也可以使用 python 结合 pandas 模块进行数据处理。这里提供 python 对 picasso 的数据 I/O 接口函数：\n\n```python\nimport numpy as np \nimport pandas as pd \nimport h5py\n\ndef load_hdf5(fp):\n    '''\n    fp：picasso软件分析产生的localization数据（hdf5格式）\n    可以使用 pandas 模块打开并进行后续数据处理\n    '''\n    return pd.read_hdf(fp, key='locs')\n\ndef dataFrame2HDF(fp, df):\n    '''\n    df: 一个用pd.read_hdf读取localization得到的中间数据帧\n    该数据需要转换为 np.rec.array 才能保存为 picasso render\n    能够打开的 hdf5 文件。\n    fp: 要保存的文件路径\n    '''\n    OBJ = [df[x].values for x in df.columns]\n    DTYPE = [(x, df[x].dtype) for x in df.columns]\n    g = np.rec.array(OBJ, dtype=DTYPE)\n    with h5py.File(fp,'w') as f:\n        f.create_dataset('locs', data=g)\n```\n\n","categories":["python"]},{"title":"汇总实验日志的小工具","url":"/2021/04/02/summarize-lab-notes/","content":"\n我习惯每天一个 markdown 文件记录一切内容。再加上懒得用手写实验记录，所以实验记录也囊括在这个文件中。然而实验记录终究要打印出来存档，所以会定期汇总一下。为了方便汇总，凡是实验日志部分的内容，我会在三级标题开头添加 :memo: (`:memo:`) 作为标记，方便自己查找，如下图所示：\n\n![image-20210401203738957](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210401203738957.png)\n\n但是一周或者半个月下来，这一段一段的内容，不断复制粘贴到一个新的 markdown 文件中，其实也比较繁琐。所以我干脆写了一个 python 程序来实现自动化的实验日志的汇总，具体代码如下：\n\n```python\nimport os, glob, time \n\ndef markdown_parser(fp, emoji_tag=':memo:'):\n    with open(fp, 'r', encoding='utf-8') as f:\n        a = f.readlines()\n        \n    # 找到所有的三级标题\n    tids = []\n    for idx, line in enumerate(a):\n        if line.startswith('### '):\n            tids.append(idx)\n            \n    # 对三级标题进行遍历，带emoji_tag的收集该部分内容\n    data = []\n    for idx, tid in enumerate(tids):\n        if a[tid].startswith('### :memo:'):\n            if idx+1 < len(tids):\n                data.extend(a[tids[idx]:tids[idx+1]])\n                # 如果最后一个标题是memo，+1会报错\n            elif idx == len(tids)-1: \n                data.extend(a[tids[idx]:])\n    \n    # 如果当天日志没有实验部分内容，返回False\n    if len(data)>0:\n        return data\n    else:\n        return False \n \ndef summarize(wks):\n    # 准备汇总文件相关信息\n    summary_filename = \"exp-{}.md\".format(time.strftime('%Y%m%d',time.localtime(time.time())))\n    timeStamp = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))\n    frontMatter = \"---\\ntitle: 实验记录汇总-\\nexcerpt: \\ndate: {}\\ntags: \\ncategories: \\n- 工作\\n---\\n\".format(timeStamp)\n    all_info = [frontMatter]\n    \n    # 读取文件列表并按照创建时间排序，最新的id为-1\n    logs = sorted(glob.glob(wks+'/*.md'), key=os.path.getctime)\n    \n    # 收集带有exp前缀的文件id\n    sids = []\n    for idx, fp in enumerate(logs): \n        _, fname = os.path.split(fp)\n        if fname.startswith('exp'):\n            sids.append(idx)\n\n    # 按照已有记录收集新近创建的日志文件\n    parts = logs[sids[-1]+1:]\n    \n    # 遍历文档完成带`memo`标签部分段落的收集\n    for fp in parts:\n        _, fname = os.path.split(fp)\n        fdate, _ = fname.split(\".\")\n        memos = markdown_parser(fp)\n        if memos:\n            all_info.append(\"\\n\\n## {}\\n\\n\".format(fdate))\n            # 防止前面的内容没有换行，二级标题失效\n            all_info.extend(memos)\n    \n    # 写入所有收集的信息到文档\n    with open(os.path.join(wks, summary_filename), 'w', encoding='utf-8') as f: \n        for part in all_info:\n            f.write(part)\n    print('实验日志汇总成功!')\n\n\nif __name__==\"__main__\":\n    wks = r'd:\\log\\main\\source\\_posts'  \n    # the path to where you store logs\n    summarize(wks)\n```\n\n运行这段代码后，会在日志目录下生成一个汇总的 markdown 文件，可以使用 typora 导出为 word 文档，稍微调整图片大小还有其他格式，插入页码，然后打印出来存档即可。\n\n","categories":["python"]},{"title":"关于博客的思考","url":"/2021/04/01/thinkAboutBlog/","content":"### 初心\n\n2021年4月1日\n\n我在 github pages 上弄了一个自己的博客网站，从2020年12月底到现在，已经有23篇文章。中间二月份和三月份基本上没动笔，Anyway，平均算下来每个月8篇，相当于每周至少两篇博客文章。我觉得这个事情还是要持之以恒地做。\n\n要坚持就离不开合理的内容规划。内容上基本还是定格在一些有用的东西上，稍微硬核一点，都是数据分析，小工具代码之类的内容。虽然我会尽量放一些可能别人也能用得着的代码，但是其实适用范围还是很有限。所以从内容上看，这个博客的命运就是无人问津，只是我自娱自乐。但如果能自娱自乐很长时间，那也是一件了不起的事情。而且既然是自娱自乐的性质，那很多东西可以稍微随意一些。这取决于我弄这个博客的初心到底是什么？\n\n![image-20210401092829792](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210401092829792.png)\n\n人或多或少有一些“流传千古”的愿望，这也可能是自我价值实现的最高层次。我们繁殖，是想把基因流传下去；我们立业，是想把理想传承下去。然而像我这样的普通人，数以亿万计的普通人，终其一生也不过是历史洪流中不起眼的小水滴。可即便如此，小人物同样有小人物的贡献，有小人物的故事，有小人物的思考。而且小人物也有权利去记录和传递这些东西，就像每个人的生育权不能被剥夺一样。所以我的初心，大致就是在行使我生命赋予的基本权利吧。这个话说得很空虚，**其实我也不是希望通过这种方式成为网红或者大咖，只不过是用一种 easy accessible 的方式帮助自己留下一些有价值的记忆，免得自己老了之后都没有跟子孙后代吹牛逼的资本。**\n\n在信息爆炸的时代，只言片语的信息只会从碎片逐渐变成垃圾，最终被遗忘。之前流行过一句话，“比死亡更可怕的是被遗忘”。当然了我也不敢奢求自己百年之后还能被谁记住。跟让人记住相比，让互联网记住，显然是一件更简单的事情。因为从技术上讲，不过是一堆二进制编码存储到某个服务器上。\n\n为了降低风险，我使用了 hexo + github pages 的方案，一来是因为 github 在全世界范围内太通用了，估计能够持续比较长的时间；二来我所有的内容，其实都先存储在本地，然后部署到 github 上，如果 github 不行，我还可以用 gitee 或者 gitlab。而国内也有很多技术博客网站，比如 CSDN 或者简书，这些公司总给人不牢靠的感觉，或许是广告太多了吧。","categories":["blog"]},{"title":"安装和配置gitlab网站","url":"/2021/04/01/use-gitlab/","content":"### gitlab的部署\n\ngitlab的部署安装其实也非常简单，主要参考下面链接。\n\nhttps://blog.csdn.net/qq_35844177/article/details/106876923\n\n关于`ufw`防火墙设置可以额外参考：\n\nhttps://zhuanlan.zhihu.com/p/139381645\n\n弄好了之后，需要把我以前架设的网站的apache2服务关闭（不然进入设置好的域名就不是gitlab）：\n\n```\nsudo /etc/init.d/apache2 stop\n```\n\nhttps://www.cnblogs.com/supe/p/8010612.html\n\n然后还可以[禁止apache2开机自启动](https://www.sunzhongwei.com/since-launch-of-ubuntu-2004-ban-apache-boot)\n\n```\nsudo systemctl disable apache2\n```\n\n再设置gitlab开机自启动\n\n```\nsudo systemctl enable gitlab-runsvdir.service\n```\n\n接下来访问之前设置的域名（http://10.161.135.46)就可以登陆gitlab了，第一次登陆要修改密码，而管理员账户名默认就是root。\n\n![](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/%E6%B7%B1%E5%BA%A6%E6%88%AA%E5%9B%BE_%E9%80%89%E6%8B%A9%E5%8C%BA%E5%9F%9F_20201203185416.png)\n\n而其他用户注册之后，必须要管理员approve才能登陆，这一点需要注意下：\n\n![](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/%E6%B7%B1%E5%BA%A6%E6%88%AA%E5%9B%BE_%E9%80%89%E6%8B%A9%E5%8C%BA%E5%9F%9F_20201203191928.png)\n\n新用户登录会让你选择角色，其实如果是研究生或者博士后，就是Developer，老师就是Lead，估计也没啥用。\n\n![image-20201203192128308](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201203192128308.png)\n\n然后我新建了自己第一个仓库，但是它提示我需要设置ssh的公钥，我觉得麻烦，如果需要clone到本地或者同步，可以使用http。\n\n![image-20201203193159877](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201203193159877.png)\n\n我自己以后肯定是要做PI的，所以这个东西积累起来对我自己后面也是有好处的。我还是要积极推动起来，别因为一些困难就停滞了，因为这个本质上确实是一件好事。只是前期我就需要自己身先士卒打个样，大家有个模板就比较好操作。\n\n### 修复gitlab\n\n三个月之前在本地（台式机加路由器）架设了gitlab网站（`_gitlab_`局域网访问`192.168.0.80`），但是没能推广起来，后来自己也有很多事情这个就荒废了。我的gitlab用户名和密码Edge浏览器已经记住我把自己设置成了超级管理员，发现别人新建的private project我也能查看：\n\n![image-20210326142323957](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210326142323957.png)\n\ngitlab的故障无非就是因为长时间工作出现的存储硬盘挂载错误，导致网站能打开，但是仓库显示503错误。我重启了一下电脑就好了。\n\ngitlab加上电子实验记录这个事情，我打算是先打个样，然后跟樊老师推荐一下，看看能不能在实验室里边搞起来。电脑和网站都是现成的，只需要弄一个固定IP即可。\n\ngitlab默认是英文界面，但是可以自己选择中文：\n\nhttps://www.centoscn.vip/2816.html\n\n### GitLab杂学\n\nGitLab我是下定决心要好好使用的，局域网就局域网，美得很，主要是作为一个上传存档的作用，所以也不需要很好地联网。\n\n**gitlab登录界面定制**\n\nhttps://blog.csdn.net/weixin_43606948/article/details/85222755\n\n管理员登录admin area-》Preferences》Appearance\n\n**修改gitlab仓库存放目录**\n\n修改/etc/gitlab/gitlab.rb\n\nhttps://blog.csdn.net/weixin_43606948/article/details/84890409\n\n```\n#配置gitlab的数据存储位置为/home目录下，保证硬盘安全\ngit_data_dirs({ \"default\" => { \"path\" => \"/home/gitlab/git-data\" } })\n```\n\n重新生成配置以及启动gitlab\n\n```\nsudo gitlab-ctl reconfigure\n```\n\n**gitlab本地备份**\n\nhttps://blog.csdn.net/weixin_43606948/article/details/84890522\n\n修改/etc/gitlab/gitlab.rb\n\n```\ngitlab_rails['backup_path'] = '/home/backups'\n```\n\n修改配置后，记得：\n\n```\ngitlab-ctl reconfigure\n```\n\n备份的命令：\n\n```\ngitlab-rake gitlab:backup:create\n```\n\n自动备份则通过任务计划crontab 实现：\n\n```\n#输入命令crontab -e\nsudo crontab -e  \n#输入相应的任务\n0 2 * * * /opt/gitlab/bin/gitlab-rake gitlab:backup:create\n```\n\n编写完 /etc/crontab 文件之后，需要重新启动cron服务\n\n```\nsystemctl restart crond\n```\n\n可设置只保留最近7天的备份，编辑配置文件 /etc/gitlab/gitlab.rb\n\n```\n# 数值单位：秒\ngitlab_rails['backup_keep_time'] = 604800\n```\n\n**gitlab数据恢复**\n\nhttps://blog.csdn.net/weixin_43606948/article/details/84890629\n\n假如备份文件地址如下：\n\n`/home/backups/1499244722_2017_07_05_9.2.6_gitlab_backup.tar`\n\n需要先停止 unicorn 和 sidekiq ，保证数据库没有新的连接，不会有写数据情况：\n\n```\n# 停止相关数据连接服务\ngitlab-ctl stop unicorn\ngitlab-ctl stop sidekiq\n# 指定恢复文件，会自动去备份目录找。确保备份目录中有这个文件。\n# 指定文件名的格式类似：1499242399_2017_07_05_9.2.6，程序会自动在文件名后补上：“_gitlab_backup.tar”\n# 一定按这样的格式指定，否则会出现 The backup file does not exist! 的错误\n对包加权限\nchmod -R 777 *\ngitlab-rake gitlab:backup:restore BACKUP=1499242399_2017_07_05_9.2.6\n# 启动Gitlab\ngitlab-ctl start\n```\n\n**gitlab批量导入用户**\n\n先管理员登录系统管理界面，Profile Setting里边获得Private token\n\n回到shell 控制台，通过cd /home 切换到home 目录 （这里的目录可以自行定）\n\n在/home 下创建 userinfo.txt 文件,内容格式如下\n\n```\n12345678 test1@phpsong.com test1 张三\n12345678 test2@phpsong.com test2 李四\n密码 邮箱 用户名 别名\n```\n\n在/home 下创建gitlabAddUser.sh 脚本，主要利用`curl`命令结合gitlab的api端口完成：\n\n```sh\n#!/bin/bash\n#gitlab用户批量录入脚本\nuserinfo=\"userinfo.txt\"\nwhile read line\ndo\n\tpassword=`echo $line | awk '{print $1}'`\n\tmail=`echo $line | awk '{print $2}'`\n\tusername=`echo $line | awk '{print $3}'`\n\tname=`echo $line | awk '{print $4}'`\n\tcurl -d \"password=$password&email=$mail&username=$username&name=$name&private_token=feGWSDFSEFWEFWEG\" \"http://190.168.0.80/api/v3/users\"\ndone <$userinfo\n```\n\n### 在本地服务器上搞GitLab Pages\n\n如果能够把个人博客跟GitLab结合起来，那就非常完美了。不过这肯定会增加服务器的负担，所以我只是收集一下资料，并不会真去尝试。\n\nhttps://my.oschina.net/doctorlzr1988/blog/3044964\n\n1. **开启功能**\n\n编辑 /etc/gitlab/gitlab.rb文件，修改如下两行：\n\n```\n##! Define to enable GitLab Pages\npages_external_url \"http://R7102/\"\ngitlab_pages['enable'] = true\n```\n\n注意的这里的pages_external_url，配置的是Pages使用的域名。如果你没有域名，就先随便写个主机名什么的。之后我们可能通过配置Nginx来解决。\n\n最好通过 **gitlab-ctl restart** 重启GitLab，使得GitLab Pages功能生效。\n\n2. **安装配置GitLab Runner**，\n\n为了能够自动发布Pages，我们需要安装GitLab Runner，然后通过GitLab CI做到Pages内容的自动更新。\n\n[https://packages.gitlab.com/runner/gitlab-runner](https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fpackages.gitlab.com%2Frunner%2Fgitlab-runner)\n\n下载GitLab Runner的安装包，安装好之后，就可以通过命令为我们的项目添加runner了，在命令行中输入以下命令，按照提示一步一步完成即可。\n\n```\ngitlab-runner register\n```\n\n其中需要填写URL和Token，可以在GitLab项目的概览->Runners找到，类似下图：\n\n![52f8c7f53da1ab5708b0c1262b15126a5c2.jpg](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/52f8c7f53da1ab5708b0c1262b15126a5c2.jpg)\n\n3. **配置Pages服务的CI**\n\n这个东西可以参考[hexo站点的](https://hexo.io/zh-cn/docs/gitlab-pages)，主要是在根目录下创建`.gitlab-ci.yml`文件：\n\n```yaml\nimage: node:10-alpine # use nodejs v10 LTS\ncache:\n  paths:\n    - node_modules/\n\nbefore_script:\n  - npm install hexo-cli -g\n  - npm install\n\npages:\n  script:\n    - hexo generate\n  artifacts:\n    paths:\n      - public\n  only:\n    - master\n```\n\n4. **如何访问Pages页面呢？**\n\n如果你有在上面配置开启Pages时配置了域名了，那么直接访问：**http://你的Git账号.域名/工程名**，例如：\n\n*http://xiaowang.mypages.com/project01*\n\n但是如果你不想那么麻烦还得配置域名的话，这里也有解决方法，由于GitLab Pages服务是部署到Nginx中，我们可以同配置Nginx来通过IP地址访问。\n\n首先要找啊找，找到Pages的发布位置，和GitLab内置Nginx的位置，分别如下：\n\n1、Pages部署目录：/var/opt/gitlab/gitlab-rails/shared/pages\n\n2、内置Nginx目录：/var/opt/gitlab/nginx\n\n然后编辑nginx目录下的conf/gitlab-pages.conf文件，内容如下：\n\n```bash\nserver {\n  listen 6869; ## 端口根据需要填写\n  server_name 10.21.100.200; ## IP根据实际情况填写\n  server_tokens off; ## Don't show the nginx version number, a security best practice\n \n  ## Disable symlink traversal\n  disable_symlinks on;\n \n  access_log  /var/log/gitlab/nginx/gitlab_pages_access.log gitlab_access;\n  error_log   /var/log/gitlab/nginx/gitlab_pages_error.log;\n \n  # Pass everything to pages daemon\n  location / {\n    # 指向pages的发布目录\n    root /var/opt/gitlab/gitlab-rails/shared/pages;\n    index index.html;\n  }\n \n  # Define custom error pages\n  error_page 403 /403.html;\n  error_page 404 /404.html;\n}\n```\n\n配置好后重启Nginx：gitlab-ctl restart nginx\n\n访问页面：`http://IP:端口/gitlab账号/工程名/public/`，例如：`http://10.21.100.200:6869/xiaowang/project01/public/#/`","categories":["gitlab"]},{"title":"standalone-HTML-image-embedding","url":"/2021/04/01/standalone-HTML-image-embedding/","content":"喜欢用typora导出带有主题的html文件分享给别人，但是不能做到standalone，此代码可以把图片附件以base64形式嵌入网页中。\n\n```python\nimport os,sys, shutil\nfrom bs4 import BeautifulSoup\nimport base64 \nfrom urllib.request import unquote, urlopen\nimport PySimpleGUI as sg\n```\n\n首先导入一些需要用到的包，我是打算通过 typora 导出带有主题风格的 HTML 网页，然后再用 `BeautifulSoup` 这个模块读取网页，并且找到其中的插图附件的标签，完成从 url 到 base64 字节嵌入。把图像文件用二进制读取后完成转码。由于有些插图是指向相对路径下的本地文件，有些是在远程服务器上的文件，所以对于后者还需要 download 下来内容，就要用到 `urllib`中的一些 模块了。最后为了方便使用这个小工具，还基于 `pySimpleGUI` 模块弄了一个简单的用户界面。这个用户界面有比较好的接口设置，也可以用于其他类似的小工具，GUI 部分的代码如下：\n\n```python\nclass app:\n    '''通用版简易工具用户界面\n    1. 使用字典传入工具函数集\n    2. 按钮平铺，每个按钮对应一个函数功能\n    '''\n    def __init__(self,app_name,func_dict):\n        self.funcs = func_dict\n        self.layout = self.design()\n        self.window = sg.Window(app_name, \n                                self.layout,\n                                location=(400,400),\n                                grab_anywhere=True,\n                                disable_minimize=True,\n                                keep_on_top=True)\n        self.run()\n    \n    def design(self):\n        layout = [[sg.Button(x) for x in self.funcs.keys()],\n                [sg.Text(\"欢迎使用\",size=(20,1),key=\"_INFO_\")]]\n        return layout\n    \n    def run(self):\n        while True:\n            event,values = self.window.Read()\n            for key in self.funcs.keys():\n                if event==key:\n                    fp = sg.popup_get_file('打开需要转化的HTML文件')\n                    if fp is not None:\n                        info = self.funcs[key](fp)\n                        self.window.Element(\"_INFO_\").Update(key+\":\"+info)\n            if event is None:\n                break\n        self.window.Close()\n```\n\n然后是最为核心的 HTML 图片转码嵌入的函数，具体代码如下：\n\n``` python\n# 先使用typora导出html，再使用此代码搞定图片链接编码成base64\ndef img_base64(html_filepath):\n    wks,html_filename = os.path.split(html_filepath)\n    with open(os.path.join(wks,html_filename),\"r\",encoding='utf-8') as f:\n        content = f.read()\n    # soup = BeautifulSoup(content,features=\"lxml\")\n    soup = BeautifulSoup(content,features=\"html.parser\")\n    # 2020-06-12 21:03:12\n    # 代码在新机器中运行出错，lxml不行，只好用html.parser\n    # 参考 https://www.cnblogs.com/awakenedy/p/9753326.html\n    imgs = soup.find_all(\"img\")\n    for img in imgs:\n        imgpath = img['src']  \n        # 2020-12-09 17:20:19 当附件图像存在于中文文件夹时，转化html会乱码，需要处理一下\n        # 参考（https://blog.csdn.net/mouday/article/details/80278938）\n        imgpath = unquote(imgpath, encoding='utf-8')\n        pref = imgpath.split(\".\")[-1]\n        \n        if not imgpath.startswith(\"http\"):\n            with open(os.path.join(wks,imgpath),'rb') as f:\n                base64_data = base64.b64encode(f.read())\n        else:\n            response = urlopen(imgpath)\n            # 使用requests模块会报错。\n            base64_data = base64.b64encode(response.read())\n            # 2021-03-29 14:15:16  如果图片已经上传到阿里云，则是链接形式\n\n        img['src'] = \"data:image/{};base64,\".format(pref)+str(base64_data,'utf-8')\n        # 注意base64是bytes，使用str转化时要带encoding参数\n        # 对img的修改都是inplace的，从属于soup对象\n    ex_filename = os.path.join(wks,html_filename)\n    with open(ex_filename,\"w\", encoding='utf-8') as f:\n        ### 覆盖保存html文件\n        ## 2021-03-29 16:26:15 \n        # 注意hexo server也会渲染html文件，但是会报错，所以导出html文件后转移到桌面\n        f.write(str(soup))\n    shutil.move(ex_filename, os.path.expanduser('~/Desktop/ex-{}'.format(html_filename)))\n    return \"转化了%d张图像\"%(len(imgs))\n```\n\n可以看到这个函数做出了许多修改，前后跨度将近一年，可以看到一个软件要真的好用，还是有很多细节的。调用则非常简单，代码如下：\n\n\n```python\nif __name__ == \"__main__\":\n    app_name = \"img_base64\"\n    func_dict = {\"base64\":img_base64}\n    worker = app(app_name,func_dict)\n    worker.run()\n```\n\n如果希望不打开 IDE 执行这段代码，可以自行编写 bat 或者 sh 脚本来运行。","tags":["HTML","BeautifulSoup","base64","urllib"],"categories":["python"]},{"title":"如何用originLab完成peak分析","url":"/2021/03/31/originLab-peakAnalysis/","content":"如下图所示，得到一个峰形图，肉眼可见，有5个峰，且每个峰都比较接近高斯分布：\n\n![image-20210331183703282](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210331183703282.png)\n\n打开OriginLab软件，`Data>Import From File`，把csv文件中的数据导入OriginLab，如下图所示：\n\n![image-20210331183834108](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210331183834108.png)\n\n查看数据表，选中`Distance`和`Count`两列，注意一个是 X 列， 一个是 Y 列数据。然后`Analysis > Peaks & Baseline > Peak Analyzer`如下图所示：\n\n![image-20210331184146716](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210331184146716.png)\n\n弹出一个`Peak Analyzer`的对话框，一路点`Next`，到`Find Peaks`这一步的时候，注意设置`Peak Filtering`选项，如下图，然后继续Next，直至输出结果：\n\n![image-20210331184544554](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210331184544554.png)\n\n在`Integration_Result`的表格中可以查看汇总的信息：\n\n![image-20210331185031798](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210331185031798.png)\n\n可以看到半峰宽，最好能到27.8 nm，说明目前成像的分辨率，只能是这样了。\n\n","tags":["peak分析"],"categories":["OriginLab"]},{"title":"jupyterNotebook开启插件","url":"/2021/01/17/jupyterNotebook-extension/","content":"\n参考[知乎上的教程](https://zhuanlan.zhihu.com/p/258976438?utm_oi=803714813804044288)，主要用pip命令，结合清华镜像源进行安装：\n\n```bash\npip install jupyter_contrib_nbextensions -i https://pypi.tuna.tsinghua.edu.cn/simple\njupyter contrib nbextension install --user\npip install jupyter_nbextensions_configurator -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n然后打开jupyter notebook，在Nbextensions里边进行设置，开启下面的插件：\n\n1. Code prettify: 代码美化\n2. Collapsible Headings: 标题折叠\n3. Table of Contents: 目录\n4. Codefolding: 代码折叠\n5. ExecuteTime: 显示执行时间\n6. Snippets Menu: 常用代码块抽屉\n7. Hinterland: 代码补全\n\n### 配置Snippets Menu插件\n\n其实就是写入一个json，这个插件的配置页提供了样本：\n\n```json\n{\n    \"name\" : \"My favorites\",\n    \"sub-menu\" : [\n        {\n            \"name\" : \"Menu item text\",\n            \"snippet\" : [\"import something\",\n                         \"\",\n                         \"new_command(3.14)\",\n                         \"other_new_code_on_new_line('with a string!')\",\n                         \"stringy(\\\"if you need them, escape double quotes with a single backslash\\\")\",\n                         \"backslashy('This \\\\ appears as just one backslash in the output')\",\n                         \"backslashy2('Here \\\\\\\\ are two backslashes')\"]\n        },\n        {\n            \"name\" : \"TeX can be written in menu labels $\\\\alpha_W e\\\\int_0 \\\\mu \\\\epsilon$\",\n            \"snippet\" : [\"another_new_command(2.78)\"]\n        }\n    ]\n}\n\n```\n\n修改如下：\n\n```json\n{\n    \"name\" : \"常用模块\",\n    \"sub-menu\" : [\n        {\n            \"name\" : \"图表绘制\",\n            \"snippet\" : [\"import numpy as np\",\n                         \"import pandas as pd\",\n                         \"%matplotlib inline\",\n                         \"%config InlineBackend.figure_format = 'svg'\",\n                         \"import matplotlib.pyplot as plt\",\n                         \"import seaborn as sns\",\n                         \"import os, glob\"]\n        },\n        {\n            \"name\" : \"数据处理\",\n            \"snippet\" : [\"import pickle\",\n                         \"from skimage import io\",\n                         \"from scipy.signal import savgol_filter\"]\n        }\n    ]\n}\n\n```\n","tags":["jupyter"],"categories":["python"]},{"title":"createDraft脚本的使用","url":"/2021/01/17/createDraft/","content":"\n### createDraft的作用和源代码\n\n为方便后期批量自动化分类汇总日志，写了一个bat脚本，给markdown文件添加`Front Matter`。具体代码如下：\n\n```bash\n:: 2021-01-12 10:31:58\n:: 生成具有FrontMatter的markdown文件\n:: 具有title、excerpt、date、tags、categories等\n\necho off\nset /p t=Please input the filename (no space):\n:: title可在FrontMatter中自行定义\n\nset d=%date:~0,4%-%date:~5,2%-%date:~8,2% %time:~0,8%\n:: %date%可以取当前日期，如2021/01/12 周二\n:: x是开始位置，y是取得字符数\n:: %time%取当前时间，如10:21:21.68\n:: 等于号右边所有的东西都会被赋值给变量，包括空格\n\n:: 下面开始写入FrontMatter到markdown文件\necho --->>%t%.md\necho title: %t%>>%t%.md\necho excerpt: This is a summary of this draft>>%t%.md\necho date: %d%>>%t%.md\necho tags: >>%t%.md\necho categories: >>%t%.md\necho - draft >>%t%.md\necho --->>%t%.md\n```\n\n### 工具的使用\n\n双击`createDraft.bat`批处理脚本，可在同目录下自动生成具有`Front Matter`的markdown文件用于记录日志。\n\n![image-20210115080404837](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210115080404837.png)\n\n如上图，需要输入markdown的文件名，注意不要有空格，也不要有中文字符，建议使用英文和数字组合。\n\n### Front Matter说明\n\n![image-20210115080929540](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210115080929540.png)\n\n当一篇日志结束后，需要拟定一个合适的标题，写一小段摘要，并设置不超过5个标签，以及所属分类。关于[Front Matter具体参考此说明](https://hexo.io/zh-cn/docs/front-matter)。\n\n### 日志导出\n\n使用typora编辑markdown，如果要把内容发送给别人，建议导出pdf。如果是要打印出来，建议导出为docx：\n\n![image-20210115081505065](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210115081505065.png)\n\n这样能够显示部分Front Matter的信息，比如标题和日期。因为如果是实验记录，就需要打印出来，这两个东西就必须显示。\n\n![image-20210115081655563](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210115081655563.png)\n\n而且在word中还可以再次对图片大小之类的进行二次编辑调整之后再打印出来。\n\n","tags":["typora","bat"],"categories":["blog"]},{"title":"DNA-PAGE-Analysis","url":"/2021/01/17/DNA-PAGE-Analysis/","content":"\n有时候需要对电泳的条带进行半定量分析，这种一般是基于图像使用ImageJ的插件进行分析处理的。不过我自己写了一个逻辑简单，操作上更加实用的小工具。\n\n![image-20210117192630917](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210117192630917.png)\n\n胶图条带的累积亮度，理论上跟碱基数目成正比，所以需要对亮部的灰度值扣背景再求和，得到一个相对摩尔数。\n\n```javascript\n/**\n * 基于胶图对DNA进行半定量\n * 获取条带的累积亮度Count\n * Count = A*N\n * A是一个常数，N是核苷酸物质的量（等于DNA链的n乘以其长度）\n * 用Rectangle工具选择单个条带\n * 运行此脚本，得到Count数值，打印到Log窗口\n * 复制到电子表格进行后续计算\n */\n run(\"8-bit\"); \n run(\"Duplicate...\", \" \");\n // 先对矩形局部取背景\n getStatistics(area, mean, min, max, std, histogram);\n setAutoThreshold(\"Default dark\");\n run(\"Create Selection\");\n run(\"Measure\");\n area = getResult(\"Area\", 0);\n mean = getResult(\"Mean\", 0);\n count = (mean-min)*area;\n print(count);\n selectWindow(\"Results\");\n run(\"Close\");\n```\n","categories":["ImageJ"]},{"title":"简化版FRET分析","url":"/2021/01/12/simpleFRETanalysis/","content":"\n脚本使用方法详见代码内注释，需要注意的是，要根据实验室具体仪器参数设置好光子数转换的相关参数。\n\n- EM_Gain = 500;  // 拍摄时设置的参数\n- Offset = 204;  // 相机内置\n- Sensitivity = 16.9;  // 相机内置\n- QE = 0.9;  //相机内置，也与荧光染料相关\n\n```javascript\n/**\n * Author: Xiaodong Xie\n * Date: 2021/1/12\n * Email: sheldon_sinap@163.com\n * Function：此ImageJ脚本专用于简易的荧光FRET分析\n * \n * ## 使用步骤  \n * 1. 分别将相同视野采集的donor和receptor两个通道的原始16-bit图像用ImageJ打开\n * 2. 右键点击图像进行rename，修改图像名称为donor或者receptor并且保存为tif\n * 3. 使用\"Process > Find Maxima...\"功能分别对donor和recoptor进行预览，获取合适的 prominence 参数（h）\n * 4. 在此脚本中修改对应的 prominence 参数\n * 5. 点击run运行此脚本\n * 6. 手动保存程序计算Results到csv文件\n * \n */\nrequires(\"1.53c\");\n\n// photon conversion参数\nEM_Gain = 500;  // 拍摄时设置的参数\nOffset = 204;  // 相机内置\nSensitivity = 16.9;  // 相机内置\nQE = 0.9;  //相机内置，也与荧光染料相关\n\n // 在此修改donor和receptor的prominence参数\n h_donor = 250;\n h_receptor = 250;\n\n // 运行主体过程 ===\n donor = \"donor.tif\";\n receptor = \"receptor.tif\";\n selectWindow(receptor);\n selectWindow(donor);  \n// 选择窗口，如果没有rename会报错提示并终止程序\n image_format = bitDepth();\n if image_format!=16{\n     showMessage(\"16-bit image required!\");\n }\n\n\n// 下面的方式更加方便无需再额外自定义函数\nselectWindow(donor);\nrun(\"Find Maxima...\", \"prominence=\"+h_donor+\" output=[Single Points]\");\nselectWindow(receptor);\nrun(\"Find Maxima...\", \"prominence=\"+h_receptor+\" output=[Single Points]\");\nrun(\"Merge Channels...\", \"c1=[\"+receptor+\" Maxima] c2=[\"+donor+\" Maxima] create\");\nrun(\"RGB Color\");\nrun(\"8-bit\");\n// 对merge之后的locations再进行find maxima\nlocations = pFinder(10);  \nselectWindow(\"Composite\");\nclose();\nselectWindow(\"Composite (RGB)\");\nclose();\n\n\n N = lengthOf(locations)/2;\n for(i=0;i<N;i++){\n     x = locations[2*i];\n     y = locations[2*i+1];\n     selectWindow(donor);\n     value_donor = getPixel(x,y);\n     selectWindow(receptor);\n     value_receptor = getPixel(x,y);\n     ratio = (value_receptor)/(value_donor+value_receptor);  // 自定义比值计算公式\n     n_donor = getPhotonNumber(value_donor, EM_Gain, Offset, Sensitivity, QE);\n     n_receptor = getPhotonNumber(value_receptor, EM_Gain, Offset, Sensitivity, QE);\n     n_ratio = n_receptor/(n_donor+n_receptor);  // 自定义比值计算公式\n     setResult(\"X\", i, x);\n     setResult(\"Y\", i, y);\n     setResult(\"value_donor\", i, value_donor);\n     setResult(\"value_receptor\", i, value_receptor);\n     setResult(\"value_ratio\", i, ratio);\n     setResult(\"photon_num_donor\", i, n_donor);\n     setResult(\"photon_num_receptor\", i, n_receptor);\n     setResult(\"photon_num_ratio\", i, n_ratio);\n }\n updateResults();  //刷新Results窗口显示收集到的数据\n selectWindow(\"Results\");   //请自行保存数据表格\n\n// 自定义函数 ===\nfunction pFinder(h){\n    // find all particles by bultin ImageJ function and return their locations\n    run(\"Find Maxima...\", \"prominence=\"+h+\" strict output=[List]\");\n    selectWindow(\"Results\");\n    locs = newArray(2*nResults);\n    for(i=0;i<nResults;i++){\n        locs[2*i]=getResult(\"X\",i);\n        locs[2*i+1]=getResult(\"Y\",i);\n    }\n    run(\"Close\");\n    return locs;\n}\n\nfunction getPhotonNumber(value, EM_Gain, Offset, Sensitivity, QE){\n    // 根据EMCCD的灰度值计算实际光子数\n    p_number = (value-Offset)*Sensitivity/EM_Gain/QE;\n    return p_number;\n}\n```\n\n","categories":["ImageJ"]},{"title":"集成typora和hexo","url":"/2021/01/10/GUIforLog/","content":"\n想打造一个开机自启动的小工具，然后想写一篇markdown笔记，直接create，就自动在网站的`_posts`目录下创建好文件，并且调用typora打开。\n\n### 把typora加入到系统环境变量\n\n![image-20210109154449024](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210109154449024.png)\n\n先用`Everything`搜索一下`typora.exe`所在的位置\n\n![image-20210109154604826](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210109154604826.png)\n\n设置好之后，直接在命令行中就输入`typora`就可以打开这个程序。如果typora后面跟了markdown文件路径，就可以直接打开这个文件。\n\n![image-20210109154824658](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210109154824658.png)\n\n### 使用python集成过程\n\n集成这一系列过程，就是把各种命令行操作简化为一两个按钮。具体代码如下：\n\n```python\nimport os\nimport webbrowser\nimport PySimpleGUI as sg\nimport subprocess\n\nclass xxd:\n    '''\n    集成typora和hexo的一个小工具\n    可以实现本地轻松创建markdown \n    然后自动部署hexo网站\n    '''\n    def __init__(self, root_dir):\n        '''\n        root_dir: hexo站点所在根目录路径 \n        如： \"D:\\hexo\\blog\"\n        '''\n        self.dir = root_dir\n        # self.disk = self.dir[:2]  # 取盘符\n        self._posts = self.dir+'/source/_posts/'\n        self.p = []  # 存放子进程\n        os.chdir(self.dir)\n    \n    def newPost(self, filename):\n        '''\n        filename: markdown文件名，\n        英文或数字组合\n        '''\n        os.system(f'hexo new {filename}')\n        self.filepath = os.path.join(self._posts, filename+'.md')\n    \n    def typora(self):\n        '''\n        用typora打开新建的markdown文件\n        typora.exe需要被添加进入环境变量\n        '''\n        command = ['typora', self.filepath]\n        self.p.append(subprocess.Popen(command))\n    \n    def depoly(self):\n        os.system(f'hexo g && hexo d')\n\n\ndef the_gui(root_dir, site_url):\n    # 初始化wiki对象\n    job = xxd(root_dir)\n\n    sg.theme('LightGrey3')\n    font = ('Arial', '11')\n\n    layout = [[sg.InputText(key='filename', size=(22, 1), font=('Arial', 16)),],\n              [sg.Button('Create', font=font), \n               sg.Button('Website', font=font), \n               sg.Button('Folder', font=font), \n               sg.Button('Depoly', font=font)],\n              ]\n    window = sg.Window('Log Tool', layout)\n\n    while True: \n        event, values = window.read()\n        if event is sg.WIN_CLOSED:\n            if len(job.p)>0:\n                for p in job.p:\n                    p.kill()\n            break\n        elif event == 'Create':\n            filename = values['filename']\n            if len(filename)>3:\n                job.newPost(filename)\n                job.typora()\n        elif event == 'Website':\n            webbrowser.open(site_url)\n        elif event == 'Folder':\n            os.startfile(job._posts)\n        elif event == 'Depoly':\n            job.depoly()\n    window.close()\n\nif __name__ == \"__main__\":\n    root_dir = r\"D:\\hexo\\blog\"\n    site_url = 'https://sheldonxxd.github.io/'\n    the_gui(root_dir, site_url)\n```\n\n需要注意的是，有些命令我调用的`os.system`，有一些则使用的`subprocess.Popen`，主要是防止阻塞。\n\n然后用户要自己定义好网站的root_dir，以及site_url。\n\n### 用户界面效果\n\n非常简单的甚至可以说丑陋的用户界面：\n\n![image-20210110060145381](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210110060145381.png)\n\n使用方法非常简单：\n\n1. 文本框中输入markdown的文件名filename，不用带后缀，然后点击`Create`，后台就会执行`hexo new filename`命令，创建一个新的markdown，并自动调用typora打开这个新建的文档\n2. 点击`Website`可以直达你的github pages\n3. 点击`Folder`可以直接打开`_posts`文件夹，方便对之前的markdown做修改\n4. 点击`Depoly`相当于`hexo g && hexo d`，生成新的内容并部署到github pages","tags":["gui","typora"],"categories":["python","blog"]},{"title":"二值化序列间隔长度统计","url":"/2021/01/07/BinarySequenceIntervalCount/","content":"\n假如有一个对象，能够随机闪烁发光，产生一段脉冲信号被检测器收集到。现在把信号强度分成high和low两类，分别用1（ON）和0（OFF）表示。接下来需要统计在这段信号序列中，1和0持续片段的长度。而且考虑仪器噪声波动，需要设置gap参数。\n\n### 样本说明\n\n比如对于序列`a`，信号作图显示如下：\n\n![image-20210107121859054](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210107121859054.png)\n\n其信号值如下：\n\n```\n0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1\n```\n\n当gap=0时，返回值为：\n\nON = [1, 3, 2, 3, 3]\n\nOFF = [5, 1, 4, 6, 2]\n\n当gap=1时，返回值为：\n\nON = [5, 2, 3, 3]\n\nOFF = [5, 4, 6, 2]\n\n……\n\n检验结果的时候，ON和OFF求和等于序列的长度，而且gap=1时，OFF中不会出现小于等于gap的数。\n\n### xxd的analyze算法\n\n此算法非常简单，但是只能统计gap=0的情况。\n\n```python\ndef analyze(labels):\n    '''\n    labels是ndarray, 里面好多True和Fasle值\n    True代表ON, False代表OFF\n    实现对状态持续时间的统计\n    '''\n    darks = []\n    ons = []\n    b, d = 0, 0\n    for idx, status in enumerate(labels):\n        if status>0:\n            b += 1\n            if d>0:\n                darks.append(d)\n            d = 0            \n        else:\n            d += 1\n            if b>0:\n                ons.append(b)\n            b = 0\n    ## 循环结束后最后一段要进行收集    \n    if d>0:\n        darks.append(d)\n    else:\n        ons.append(b)\n    # 返回on或者dark time的均值\n    return ons, darks\n```\n\n测试结果如下：\n\n![image-20210107122525529](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210107122525529.png)\n\n### hwj的bi_count算法\n\n此算法逻辑略为复杂，但是能统计gap>0的情况。\n\n```python\ndef bi_count(a, gap=1):\n    '''\n    Author: Wenjuan HU\n    Date: 2021-01-07\n    '''\n    count_0, count_1 = 0, 0\n    list_0 = []\n    list_1 = []\n    for i in range(len(a)):\n        if a[i] == 0:\n            count_0 += 1\n            if count_0 > gap and count_1 >0 :\n                list_1.append(count_1)\n                count_1 = 0\n        elif a[i] == 1:\n            count_1 += 1\n            if count_0 > gap:\n                list_0.append(count_0)\n                count_0 = 0\n            else:\n                count_1 += count_0\n                count_0 = 0\n    if count_0 > gap:\n        list_0.append(count_0)\n    else:\n        list_1[-1] = list_1[-1]+count_0\n    if count_1 > 0:\n        list_1.append(count_1)\n    return list_0, list_1\n```\n\n测试结果如下：\n\n![image-20210107122606314](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210107122606314.png)\n\n### 小结\n\n这是一个编程能力测试的好case，反正我是做不出来的弱鸡（if...else一多就懵逼），所以必须向强大的hwj女士致敬！！！","tags":["algorithm"],"categories":["python"]},{"title":"安装和设置HP网络打印机","url":"/2021/01/06/installPrinter/","content":"\n实验室打印机还是有的，不过平时我主要在电脑上看文献，所以使用的频率非常低。恰巧今天要提交一些纸质材料，不得不打印出来。一问，大家都是抱着自己的笔记本电脑，跟打印机通过usb连接再打印文件的。而我的电脑同时连着两个硬盘，各种程序（处理硬盘中的实验数据）和文件都还打开着。所以**搬电脑**这种事情我可干不来。\n\n为了方便自己方便大家，我便把打印机连接到了基于自己的路由器架设的无线局域网中（printer无法连单位的企业WiFi）。然后在路由器中还给它分配了固定IP：\n\n![image-20210106100103687](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106100103687.png)\n\n接下来就是给自己的电脑安装驱动，并且连接这台网络打印机。为此，我还写了一个教程。\n\n### 安装打印机驱动\n\n建议使用windows系统，macOS或linux需要下载对应的驱动程序。\n\n先安装版本型号匹配的打印机驱动：\n\n![image-20210106083943008](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106083943008.png)\n\n在安装界面中，需要连接到网络打印机，我已经设置好网络，大家务必先连接到`_gitlab_`这个WiFi，密码是`fan123456`。**注意此WiFi为局域网，不能访问公网，**但可以访问内部gitlab网站（http://192.168.0.80）\n\n> 夹带了私货。gitlab是我架设在一台ubuntu主机上的东东，我觉得对于实验室内部而言，有这么一个协同和版本控制的工具还蛮好的。\n\n![image-20210106084204281](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106084204281.png)\n\n然后在驱动安装界面输入打印机的IP地址`192.168.0.188`\n\n![image-20210106084251923](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106084251923.png)\n\n马上就能搜索到这台打印机的信息，点击下一步继续安装和配置（需要一点时间）。\n\n![image-20210106084320075](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106084320075.png)\n\n最后显示打印机安装成功界面：\n\n![image-20210106084722170](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106084722170.png)\n\n直接点击`完成`，并打印测试页。如果没有成功，请检查您的系统是否自动断开了没有网络连接的`_gitlab_`的WiFi。\n\n### 设置双面打印\n\n具体参考《[惠普打印机双面打印自动变手动的解决办法](https://www.jb51.net/hardware/other/313421.html)》，对打印机的属性进行修改：\n\n![image-20210106085902856](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106085902856.png)\n\n然后修改打印机首选项，为了经济环保，默认双面打印：\n\n![image-20210106090109404](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106090109404.png)\n\n### Word文档打印效果\n\n然后随便打印一个word文档，按`Ctrl+P`，可以看到打印机默认配置就是双面打印了：\n\n![image-20210106090248311](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106090248311.png)\n\n### 小结\n\n然后我把教程发布到微信群，让大家打印东西能够方便一点，获得点赞不计其数，那我也感觉蛮开心的。\n\n![image-20210106100712376](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210106100712376.png)\n\n安装配置网络打印机这种事情，其实非常简单。但是它能够服务到的人也是最多的。而我写的可能自鸣得意的代码，反倒是无人问津最多我老婆喊666。当然了我也并不追求多人问津，能自鸣得意自娱自乐也是不错的。关键是说一定要抓住真正的需求（不管是自己的还是客户的），才能做出创造大量价值的事情，从而获得满足感和幸福感。","tags":["打印机","需求"],"categories":["blog"]},{"title":"获取直方分布是如此的简单","url":"/2021/01/05/easy-histogram/","content":"\n查看一列数据的直方分布是统计里边最基本的操作了，过去我常常使用seaborn作直方分布图，它会进行自动的核密度估计，但是我当我想拿到一个简单的histogram的数据的时候，常常还需要自己使用`collections.Counter`先来计数一下。直到今天，我使用`plt.hist`命令偷懒，发现了一些端倪：\n\n![image-20210105192718893](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105192718893.png)\n\n注意，jupyter notebook 中output的内容，除了有figure之外，还有一个tuple，看上去`plt.hist`命令可以返回直方分布柱状图的作图数据。如果勤快点加上`plt.show`，就看不到了：\n\n![image-20210105192912328](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105192912328.png)\n\n### plt.hist的返回值就是直方分布统计数据\n\n不过我还是用一个变量接收了`plt.hist`命令的返回，轻而易举就拿到了直方分布的统计数据，后续自己想怎么作图就怎么作图：\n\n![image-20210105193018138](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105193018138.png)\n\n### 通过bins和range参数控制直方分布统计\n\n不过你可能注意到，直方分布的分组边界是凌乱的。这个时候可以通过设置bins和range参数来控制：\n\n![image-20210105193503927](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105193503927.png)\n\n### 直接把频次换算成分布概率密度\n\n通过`?plt.hist`可以在jupyter notebook中快速查看该函数的文档，立马得知可以通过density这个参数，选择输出分布密度，相当于做了一个归一化，使得不同数量的样本之间具备更好的可比性。\n\n![image-20210105193937326](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105193937326.png)\n\n### 查看累积频率\n\n![image-20210105194511640](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105194511640.png)\n\n只要在参数里边修改cumulative为`True`，就可以输出一个累积频率分布。这比我这两天类似累活可要轻松多了。\n\n### 绘制帕累托图\n\n然后我想把累积频率绘制成折线，下边放分布，结果如下：\n\n![image-20210105194913469](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105194913469.png)\n\n需要注意的是，a是前边`plt.hist`命令的返回值，索引为1才对应横坐标，而且`len(a[1])` 比` len(a[0])` 大 1，因为`a[1]` 存放的是边界，所以我对这个array进行了切片，而且还移位。\n\n### 小结\n\n我再也不用Counter了，再也不用dataframe的`.cumsum()`了，使用plt.hist不能再方便。感觉像是打通了任督二脉。matplotlib这个库确实是🐂🍺！","tags":["histogram","matplotlib"],"categories":["python"]},{"title":"我为何从ImageJ转向python","url":"/2021/01/05/myCodingHistory/","content":"\nImageJ是学术界非常常用的图像处理软件。它插件丰富，功能强大，体积小巧，开源跨平台，我在读博士刚开始的时候，主要就是用它解决一些成像数据分析的问题。可是到后来，需要处理的数据量越来越大，对自动化的需求越来越高，ImageJ的宏语言（Macro Language）已经无法满足需求，我便逐步迈入了python的深坑。下面举一个例子，作为我这种转型的见证。\n\n### 需求\n\n有一个视频数据（xyt），每一帧都有一些单分子荧光信号，需要提取它们的位置（xp, yp），然后再提取该位置的强度时间曲线进行分析。\n\n### ImageJ Macro的代码实现\n\n```javascript\n/*\n * 2021-1-4\n * # batch find maxima and export the list\n */\n\n h = 750;  // 先自行用Find Maxima调节参数预览一下，选择合适的数值\n\n // 把记录文件存放子目录下，\n // 如果子文件夹不存在，创建一个\n image= getTitle();\n imageName = split(image, \".\");\n fpath = getDirectory(\"image\")+imageName[0]+\"_frames/\";\n if (!File.exists(fpath)){\n \tFile.makeDirectory(fpath);\n }  \n \n N = nSlices;\n setBatchMode(\"hide\");  // 使用batchmode稍微快一点\n for (i = 0; i < N; i++) {\n \t// main(fpath, i);\n \tmain2(i);\n }\n setBatchMode(\"show\");\n\n roiManager(\"save\", fpath+\"RoiSet.zip\");\n // 把roi管理器中的记录打包保存\n // 后续可以使用python中的read_roi模块进行处理\n\n function main(fpath, i){\n \t// 对每一帧进行Find Maxima处理，\n \t// 输出List到Results窗口\n \t// 把Results窗口内容保存为csv，命名为frame/id.csv\n \tframeID = i+1;\n \tsetSlice(frameID);\n \trun(\"Find Maxima...\", \"prominence=750 output=List\");\n \tsaveAs(\"results\", fpath+frameID+\".csv\");\n// \tselectWindow(\"Results\");\n// \trun(\"Close\");\n//  不用关，每次都会刷新Results窗口\n }\n\n function main2(i){\n \t// 对每一帧进行Find Maxima处理，\n \t// 输出List到Results窗口\n \t// 把Results窗口内容保存为csv，命名为frame/id.csv\n \tframeID = i+1;\n \tsetSlice(frameID);\n \trun(\"Find Maxima...\", \"prominence=750 output=[Point Selection]\");\n \troiManager(\"Add\");\n \t// 使用ROI管理器保存数据，减少IO，速度快上好几倍\n \trun(\"Select None\");\n }\n```\n\n以上代码，只是循环调用ImageJ自带的`Find_Maxima`功能，刚开始我是打算输出到Results窗口，然后保存每一帧的结果到csv文件，结果发现这样太慢了（大概9分钟）。后来决定把结果保存到ROI管理器，能够稍微加速一点（大概4分钟）。但是即便这么大段的代码，也只不过实现了位置信息的收集，还没办法实现强度时间曲线的提取、整理汇总和可视化。而且保存为roi，后续还得`pip install read_roi`，利用python脚本进行后续的处理。\n\n### python的代码实现\n\n主要依赖`skimage.io`实现视频数据的读取，`skimage.morphology.h_maxima`实现**Find_Maxima**的功能。为了学习写类，我把几个大步骤都写成了类：\n\n1. 首先是读取数据，smFRET\n2. 然后是Find_Maxima，finder\n3. 接着就是要把找到的maxima对应位置的强度时间曲线提取出来，所以有一个extractor\n\n```python\nfrom skimage import io\nfrom skimage.morphology import h_maxima\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass smFRET():\n    def __init__(self, filepath):\n        self.filepath = filepath\n        self.movie = io.imread(filepath, plugin='pil')\n        self.frames, self.width, self.height = self.movie.shape\n\n        \nclass finder(smFRET):\n    def __init__(self, filepath):\n        super(finder, self).__init__(filepath)\n    \n    def run(self, h):\n        '''\n        h这个参数可以使用ImageJ预览movie，用`Find_Maxima`命令获取\n        '''\n        self.maxima = []\n        # movie整体太大，一次性处理会出内存问题，所以按帧扫描\n        for frame in self.movie:\n            self.maxima.append(h_maxima(frame, h))\n        self.maxima = np.array(self.maxima)\n        self.mapping = self.projection()\n    \n    def projection(self):\n        '''\n        做一个z轴的projection\n        '''\n        return self.maxima.sum(axis=0)\n\n    \nclass extractor(finder):\n    def __init__(self, filepath, h):\n        '''\n        filepath: tif视频文件路径\n        h: 这个参数可以使用ImageJ预览movie，用`Find_Maxima`命令获取\n        '''\n        super(extractor, self).__init__(filepath)\n        self.run(h)\n        \n    def preview(self, vmax=100, vmin=4):\n        plt.imshow(self.mapping, vmax=vmax, vmin=vmin, cmap=\"jet\")\n        plt.colorbar()\n        plt.show()\n        \n    def threshold(self, vmax, vmin):\n        '''\n        根据find.mapping的预览结果设定vmax和vmin作为threshold\n        '''\n        # 要转化为float才能做broadcast计算\n        mm = self.mapping.astype(\"float32\")  \n        rows, cols = np.where((mm-vmin)*(mm-vmax)<=0)\n        self.locs = np.vstack([rows, cols]).T\n    \n    def choose(self, pid, show=False):\n        '''\n        pid: 单分子信号的索引，从0开始，不大于self.locs的长度\n        '''\n        row, col = self.locs[pid]\n        line = self.movie[:, row, col]\n        labels = self.maxima[:, row, col]*line.ptp()/10+line.mean()\n        # 把maxima的label也提取出来，看看啥情况\n        if show:\n            plt.plot(line)\n            plt.plot(labels, c='r')\n            plt.show()\n        return line, self.maxima[:, row, col]\n```\n\n严格来说，我这代码写得并不漂亮，因为我平时很多时候都是写零散的面向过程的代码，面向对象并不是我熟悉的领域。不过我喜欢一个对象可以保留很多数据和方法，还能够继承的特点。所以在一些heavy的场合，我也会使用。而使用起来也非常简单：\n\n```python\next = extractor(filepath, h=1000)\next.preview(vmax=100, vmin=4)\n# 可以先看到定位的结果，找到合适的vmax和vmin作为后续的threshold\n```\n\n![image-20210105182125942](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105182125942.png)\n\n```python\next.threshold(vmax=100, vmin=4)\nline,labels = ext.choose(30, show=True)\n```\n\n![image-20210105182112769](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105182112769.png)\n\n使用python代码，只需要1分40秒就可以完成任务（事实上文件读取和finder这两部分采取多进程多线程甚至numba，还能大幅提速），而且所有抽提出来的坐标信息都在ext对象中保存着，想看哪个，就可以使用choose的方法取值并查看强度时间曲线和对应的label。\n\n### 小结\n\nImageJ的Macro Language是我第一个成功应用并解决了科研上实际问题的编程语言，让我尝到了使用编程来高效解决一些laborious的图像数据分析工作的甜头，同时也让我树立了一些自信心。但是它毕竟过于简单，难以胜任更多更复杂的任务。虽然我现在已经很少使用ImageJ了，但是对它以及背后成千上万的开发者，我报以崇高的敬意。","tags":["summary"],"categories":["python","ImageJ"]},{"title":"单颗粒粒子光斑的高斯拟合","url":"/2021/01/05/GaussianFit-2D/","content":"\n像PALM、STORM等超分辨荧光成像，都是基于单分子定位。一般来说，单分子的荧光成像在xy平面上近似高斯分布（有兴趣可了解点扩散函数），可以用高斯拟合得到单分子的精确定位信息，从而突破光学衍射极限。这里就尝试使用python对二维粒子光斑实现高斯拟合。\n\n### 粒子光斑图像\n\n![image-20210105082931680](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105082931680.png)\n\n如上图所示，我模拟了两个圆形粒子光斑，左边的中心坐标是`(3, 3)`，右边的是`(3.2, 2.9)`，相当于有`0.22`个像素的偏移（若每个像素160 nm，实际偏移约35 nm），反映在粒子光斑上，就是箭头所指的像素块稍微亮度提高了一些。有了这样一个直观的感受，接下来就看看如何构造一个高斯光斑。\n\n### 二维高斯光斑的构造\n\n代码我是直接从[stackOverflow](https://stackoverflow.com/questions/21566379/fitting-a-2d-gaussian-function-using-scipy-optimize-curve-fit-valueerror-and-m)上抄来，稍微增加了一些注释，具体如下：\n\n```python\nimport numpy as np\n\ndef twoD_Gaussian(meshgrid, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n    '''\n    meshgrid: 包含x和y，能够表示网格所有的索引(i,j)\n    xo, yo: 高斯分布的中心位置 \n    amplitude: 高斯分布的最大强度\n    sigma_x, sigma_y: 影响高斯光斑的长宽\n    theta: 控制角度，如果是一个椭圆，数值范围0-PI\n    offset: 补偿值，可以使所有的数值都是正数\n    '''\n    x, y = meshgrid\n    xo = float(xo)\n    yo = float(yo)    \n    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n                            + c*((y-yo)**2)))\n    # 注意返回值最好要ravel，不然curve_fit会报错\n    return g.ravel()\n```\n\n有了这样一个标准的构造函数，接下来就可以使用`curve_fit`对已有的数据进行拟合，反过来估计这个函数中一些参数，其中就包括高斯分布中心的位置`(xo, yo)`。\n\n### 粒子光斑的高斯拟合\n\n```python\nfrom scipy.optimize import curve_fit\n\n# Create x and y indices\nx = np.linspace(0, 6, 7)\ny = np.linspace(0, 6, 7)\nx, y = np.meshgrid(x, y)  # meshgrid生成了一整张索引map\n#create data\ndata = twoD_Gaussian((x, y), 10, 3, 3, 2, 2, 0, 10)\n# add some noise to the data and try to fit the data generated beforehand\ninitial_guess = (10, 3, 3, 2, 2, 0, 10)\ndata_noisy = data + 0.2*np.random.normal(size=data.shape)\n# 方便起见，构造一个带噪声的数据\n\npopt, pcov = curve_fit(twoD_Gaussian, (x, y), data_noisy, p0=initial_guess) \n# 需要拟合的参数直接全部就放到p0去了\n```\n\n然后查看popt中的结果：\n\n![image-20210105084727546](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210105084727546.png)\n\n可以看到，加入噪声后做高斯拟合，估计的参数`xo = 3.249`，`yo = 2.916`，跟前面`(3.2, 2.9)`又有微小的差异（大概5-8个 nm的偏移），所以说，单分子定位精度离不开非常好的信号质量。\n\n","tags":["gauss","fit"],"categories":["python"]},{"title":"累积分布函数和S型曲线拟合","url":"/2021/01/04/cdf/","content":"\n查看数据分布情况，以及使用特定的函数模型对已观测的数据进行拟合是非常常见的操作。这里从正态分布入手，到应用**Logistic**模型完成累积分布曲线的拟合，并根据拟合后模型参数重新得到样本的均值。\n\n### 导入依赖包\n\n本case在jupyter notebook中运行，可先导入以下包：\n\n```python\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfrom collections import Counter\nfrom scipy.optimize import curve_fit\n```\n\n### 制作正态分布数据集\n\n先制作一个数据集，均值设置为10，变异度为5。\n\n```python\nmu = 10\nsigma = 5\nX = mu + sigma*np.random.randn(1000)\n```\n\n简单作图可以看到如下的数值分布：\n\n![temp](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp.svg)\n\n如果就X数据集直接计算均值，发现与10还是有一些差距：\n\n![image-20210104191011726](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210104191011726.png)\n\n### 获取分布计数\n\n主要使用`collections.Counter`函数：\n\n```python\na = Counter(X)\nb = dict(a)\nX2 = []\nfor key in b:\n    X2.append([key, b[key]])\nX2 = pd.DataFrame(X2, columns=['value', 'count'])\nX3 = X2.sort_values(by='value')\nX3['cum_count'] = X3['count'].cumsum()  # 计算累积计数值\nsns.lineplot(x='value', y='cum_count', data=X3)\nplt.show()\n```\n\n![temp2](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp2.svg)\n\n### 选取模型进行曲线拟合\n\n这里主要利用`scipy.optimize.curve_fit`来实现曲线拟合，需要选择合适的函数模型。对于S型曲线，比较常用的是**Logistic**模型，关于S型曲线拟合，可以参考《[轻松构建多种模型拟合S型曲线](https://zhuanlan.zhihu.com/p/147467845)》。\n\n```python\ndef func(x, a, b, c):\n    '''\n    使用Logistic模型来拟合\n    该模型有3个参数\n    当x=b/c时，曲线处于拐点，可用于估计正态分布的峰值\n    https://zhuanlan.zhihu.com/p/147467845\n    '''\n    return a/(1+np.exp(b-c*x))\n\nparams = curve_fit(func, X3['value'], X3['cum_count'])\npopt, pcov = params\nprint(popt[1]/popt[2])\n```\n\n![image-20210104192236878](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210104192236878.png)\n\n可以看到拐点对应的value，跟我们最初设置的`mu=10`非常接近了。\n\n### 查看拟合效果\n\n```python\nX4 = [func(i, popt[0],popt[1],popt[2]) for i in X3['value']]\nX3['fit'] = X4\n\nsns.lineplot(x='value', y='cum_count', data=X3)\nsns.lineplot(x='value', y='fit', color='r', data=X3)\nplt.savefig(os.path.join(os.path.expanduser('~/Desktop/'), 'temp.svg'))\n```\n\n![temp3](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp3.svg)\n\n> 注意一下，后面如果命名临时文件，简易带上时间戳，不然图床会覆盖保存。\n\n### 参考链接\n\n1. [轻松构建多种模型拟合S型曲线](https://zhuanlan.zhihu.com/p/147467845)\n\n2. [Exponential Fit with Python](https://swharden.com/blog/2020-09-24-python-exponential-fit/)\n\n3. [累积分布函数](https://baike.baidu.com/item/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0)\n\n4. [python 曲线拟合](https://blog.csdn.net/Fuxiu279/article/details/88429801)\n\n","tags":["curveFit","data","cumulative","distribution"],"categories":["python"]},{"title":"使用ImageJ提取某点的强度时间曲线","url":"/2020/12/30/getIntensityCurve/","content":"\n有时候拿到原始视频数据(xyt)之后，需要在ImageJ中稍微预览一下，了解大概情况。其中一个重要的view是得看一下某个位置的信号的闪烁情况，所以需要提取该位置(x,y)的强度随时间变化的曲线。为了方便，我写了一个简单的ImageJ Macro脚本来实现此功能。脚本运行后，用户在图像上点击某个位置，就会把该位置保存到ROI管理器，然后提取该位置的Intensity(t)，并绘制图像。\n\n### 代码\n\n代码的几个要注意的地方：\n\n1. 需要自定义选取框的大小，对于单分子荧光光斑，4就足够了\n2. 可以设置是否多次选取，如果是多次，就默认选取3个点\n3. 提取的intensity是选取框的均值，这样能有效抑制背景噪声波动\n\n在实现上，这个代码还要解决以下问题：\n\n1. 监听并返回用户点击鼠标事件\n2. 自动切换Slice，即逐帧同xy坐标取值\n\n\n```javascript\n/*\n * 该脚本适用于手动分析TIRF视频数据中某点的强度随时间变化情况\n * Author: Xiao-Dong Xie\n * 2020年12月31日\n */\n\ntime_interval = Stack.getFrameInterval();  \n// 内置函数，获取stack的帧时间间隔，适用于原始TIRF数据\nboxWidth = 4;\n// 点选取值框大小\nmultiCheck = true; \n// 如需多次检查，修改此处为true，可将main置入for循环中\nmultiCheckNum = 3;\n\n// ================================================\nif (multiCheck) {\n\tfor (i = 0; i < multiCheckNum; i++) {\n\t\tmain(boxWidth, i); \n\t}\t\n}\nelse {\n\tmain(boxWidth, 0);\n}\n\n// =================================================\n\nfunction main(boxWidth, cid){\n\t/** 点击stack上一个点，返回time profile\n\t *  boxWidth: 全选框大小\n\t *  cid：选点的序号\n\t */\n\tloc = chooseParticle();\n\tx = loc[0];\n\ty = loc[1];\n\tmakePoint(x, y, \"small yellow hybrid\");\n\troiManager(\"add\"); // 把该点加入roi管理器\n\twait(300);\n\tn = getSliceNumber();\n\tlong = nSlices;\n\tv = newArray(long);\n\tt = newArray(long);\n\tfor (i = 0; i < long; i++) {\n\t\tsetSlice(i+1);\n\t\t// va = getPixel(x, y);\n\t\t// 用取小区域均值的方式更好\n\t\tva = getRectRegionMeanValue(x, y, boxWidth);\n\t\tv[i] = va;\n\t\tt[i] = i*time_interval;\n\t}\n\tsetSlice(n);\n\tPlot.create(\"Time Profile \"+cid, \"Time (sec)\", \"Intensity\", t, v);\n\tPlot.show();\n}\n\nfunction chooseParticle(){\n    // 鼠标单击图像上某个点，即返回其坐标\n    leftButton = 16;\n    flags = 0;\n    while(flags&leftButton == 0) getCursorLoc(x,y,z,flags);\n    return newArray(x,y);\n}\n\nfunction getRectRegionMeanValue(x, y, width){\n\t//以(x,y)坐标为中心，获取边长为width的正方形区域的平均强度\n\t// 不返回最大的强度，是因为波动比较大\n\tmakeRectangle(x-width/2, y-width/2, width, width);\n\tgetStatistics(area, mean, min, max, std, histogram);\n\treturn mean;\n}\n```\n\n### 结果\n\n使用的时候，把视频数据（tif格式）打开后，再把`getIntensityCurves.ijm`脚本文件拖入Fiji打开，然后在console中点击`run`运行，然后在视频图像上选取你想提取数值的点。结果如下：\n\n![image-20201231082554717](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201231082554717.png)","tags":["TIRF"],"categories":["ImageJ"]},{"title":"免费下载网易云音乐mp3的小工具","url":"/2020/12/30/downloadCloudMusic/","content":"\n直接上代码吧，逻辑非常简单，就是从网易云分享链接中提取music的ID，然后构造外链：`http://music.163.com/song/media/outer/url?id={musicID}.mp3`，再直接用requests模块请求这个外链，下载内容保存为mp3格式的文件即可。为了方便实用，我还利用`pySimpleGUI`做了一个非常简易的用户界面，方便操作。\n\n```python\nimport requests\nimport re\nimport os\nimport PySimpleGUI as sg\n\ndef extractMusiceID(url):\n    pattern = re.compile(r'id=\\d+')\n    mID,uID = pattern.findall(url)\n    mID = mID.split(\"=\")[1]\n    uID = uID.split(\"=\")[1]\n    print(\"User:{}\\nMusic:{}\".format(uID,mID))\n    return mID\n\ndef downloadMusic(mID,name):\n    url = \"http://music.163.com/song/media/outer/url?id={}.mp3\".format(mID)\n    # 需要使用header，不然url重定向之后抓不到\n    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n    r = requests.get(url,headers=header)\n    data = r.content\n    wks = os.path.expanduser(\"~/desktop\")  # 默认下载到桌面\n    fpath = os.path.join(wks,\"{}.mp3\".format(name))\n    with open(fpath,\"wb\") as f:\n        f.write(data)\n\ndef main(url,name):\n    mID = extractMusiceID(url)\n    downloadMusic(mID,name)\n\nif __name__ == \"__main__\":\n    \n    layout = [\n            [sg.Text(\"曲名\"),sg.Input(key='-Name-')],\n            [sg.Text(\"链接\"),sg.Input(key='-Url-')],\n            [sg.Button('Download'),sg.Exit()]\n            ]\n\n    window = sg.Window('网易云音乐下载', layout)\n\n    while True:\n        event, values = window.Read() \n        if event == 'Download':\n            name = values['-Name-']\n            url = values['-Url-']\n            main(url,name)\n        if event in (None, 'Exit'):      \n            break\n    window.Close()\n```\n\n运行之后，弹出一个窗口，填写两行信息，Download就会把mp3下载到桌面：\n\n![image-20201230122701166](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122701166.png)\n\n然后输入歌曲名字，以及分享链接。链接可以从网易云音乐中获取：\n\n![image-20201230122626573](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122626573.png)\n\n效果如下：\n\n![image-20201230122805612](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122805612.png)\n\n### 来一首《梦伴》\n\n\n<iframe src=\"//player.bilibili.com/player.html?aid=23415950&bvid=BV1Kp411Z7ci&cid=39036220&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" > </iframe>\n","tags":["gui","requests"],"categories":["python"]},{"title":"pandas模块常用功能备忘录","url":"/2020/12/29/pandasUsageSummary/","content":"\n使用python进行数据分析，离不开`numpy`，`pandas`，`matplotlib`三个包。为了作图好看，我还常用`seaborn`。不过`numpy`使用起来还是没有pandas那么爽，所以我还是用pandas和seaborn最多。所以趁着还有一点写博客的热情，先整理一下pandas的常用功能。\n\n当然了， 介绍最详细的还是官网文档：\n\nhttps://pandas.pydata.org/docs/\n\n### 读取数据\n\npandas可以读取各类数据文件，比如`csv`、`xlsx`、`hdf5`等。而读取的命令一般输入`pd.read`代码自动补全提示就会出来很多。我这里也不多说，想查看，可以输入一下命令：\n\n```python\nimport pandas as pd\nfor func in dir(pd):\n\tif func.startswith('read'):\n\t\tprint(func)\n```\n\n使用方法基本上都是要输入filepath，然后`pd.read_hdf`还需要输入hdf5的dataset的key名。我在处理picasso产出的中间数据时，还经常用到以下命令：\n\n```python\ndef hdf2dataframe(filepath):\n    '''\n    只适用于picasso分析过程中生成的hdf5文件数据结构\n    输出一个字典，包含了不同的dataframe\n    '''\n    data = {}\n    dataset = h5py.File(filepath, 'r')\n    for key in dataset.keys():\n        df = pd.DataFrame(dataset[key][()],dtype=\"float64\")  # 注意数据精度\n        data[key] = df\n    return data\n```\n\n这个自定义函数可以把hdf5数据中不同的dataset转化为pandas的DataFrame，并保存到字典中。但事实上当你知道dataset的key名很固定，基本都是`locs`的时候，只需要下面一行命令就可以了：\n\n```python\ndata = pd.read_hdf('cluster_filtered.hdf5','locs')\n```\n\n### 保存数据\n\n对于`pd.DataFrame`对象，可以把数据表保存到各种格式的文件中。下面简单举几个例子：\n\n```python\n# 假设data是一个dataFrame\ndata.to_csv(filepath, index=None)  # 保存为csv文件\ndata.to_excel(filepath, index=None)  # 保存为xlsx文件\ndata.to_pickle(filepath)  # 保存为pkl文件\ndata.to_hdf(filepath, 'locs')  # 保存为hdf5文件\n```\n\n其中csv和xlsx文件都可以使用excel打开，我经常使用，但为了避免多余的index列，我通常会指定`index=None`。处理的中间数据若需要保存，使用pkl文件是最方便的，没有其他参数需要声明。保存为hdf5文件跟打开的时候类似，还要写key名。\n\n### pd.DataFrame对象\n\n数据帧是pandas最重要的概念，也是最常用的一种数据结构。简单来说，就跟你在excel中看到的表格一样的。从形状上讲，就是一个二维矩形。\n\n使用`pd.read_??`方法读取数据文件，返回的基本上都是数据帧，然后你就可以进行各种愉快的操作。当然你也可以把程序运行中其他类型的数据转化为一个DataFrame，下面介绍几种方式：\n\n```python\nimport numpy as np\na = np.random.randint(10, size=(5,3))  # 产生一个3x5的随机数矩阵\nb = pd.DataFrame(a, columns=list('abc'))  \n# 数据有5列，必须给column名，\n# index名也可以指定，若无则会默认从0开始\n```\n\n以上就是从numpy二维数组直接转成数据帧的操作，就是要注意columns的命名。效果如下：\n\n<img src=\"https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229201642898.png\" alt=\"image-20201229201642898\"  />\n\n另外也有一些更傻瓜的方法：\n\n```python\na = pd.DataFrame()   # 可以先创建一个空的数据帧\na['a'] = [x for x in range(3)]\na['b'] = np.random.randint(10, size=(3,))\n```\n\n先创建一个空的数据帧对象，然后赋值也是可以的，但是要注意每一个column的数据长度要一致。这个操作有点像字典。没错，你也可以直接把字典转化为数据帧：\n\n```python\na = {'a':[1,2,3], 'b':[4,5,6]}\nb = pd.DataFrame(a)\n```\n\n### 数据帧的取值\n\n```python\na = np.random.randint(10, size=(3,5))  # 产生一个5x3的随机数矩阵\nb = pd.DataFrame(a, columns=list('abcde'))  # 数据有三列，给column名\nb.loc[0,'c']  # 注意是方括号，第一个是index name， 第二个是column名，返回单个值\nb.iloc[0] # 注意方括号，这里只能输入index名，返回一行数据（pd.Series）\n```\n\n如上所示，数据帧就是一个矩阵，取值给索引就好，不过在我的实践过程中，很少直接取某个特定位置的数值。遍历倒是常用，不过需要注意的是，为了加快速度，建议使用如下的遍历方式：\n\n```python\nfor idx,row in df.iterrows():\n    # row是数据帧的某一行\n    doSomeThingWith(row)\n```\n\n### 数据帧分组\n\n```python\ngp =data.groupby(by=['column_name_1', 'column_name_2'])\n```\n\n一般分组完了，还可以遍历分组，做一些操作：\n\n```python\nfor key, df in gp:\n    # key 是分组数据对应 column_name_1', 'column_name_2' 这两列的值\n    # df 是分组的子数据帧\n    doSomeThingWith(df)\n```\n\n### 数据筛选\n\n```python\ndata2 = data.filter(items=['column_1', 'column_2'])\n# 类似的还有一个drop方法\n```\n\n上述代码可以从data原数据帧中取出两列值。\n\n```python\nlabel = [0,2,3]\nregion_new = region[region['Cluster'].isin(labels)]  \n```\n\n上述代码可以按`Cluster`这一列的值进行筛选，如果值存在于label列表中，则被提取出来。\n\n```python\ndf = localization[(localization['x']>(x-radii))&\\\n                 (localization['x']<(x+radii))&\\\n                 (localization['y']>(y-radii))&\\\n                 (localization['y']<(y+radii))]\n```\n\n上述代码根据数据帧中的数值指定范围进行筛选，注意方括号，然后多个判断条件的布尔运算要用小括号。\n\n### 数据排序\n\n`data.sort_values(by=??)`，偶尔会用。\n\n### 数据翻转\n\n`data.T`\n\n把row和column翻转一下，偶尔会用。\n\n### 数据合并\n\n`pd.concat([df1, df2])`\n\n把一些符合条件的DataFrame合并，注意前提是它们具有相同的column，然后可以放到一个列表里面。\n\n下面还提供一个例子，怎么把series叠加做成数据帧。\n\n```python\na = pd.Series([1,2,3], index=list('abc'))  # 序列\nb = pd.Series([4,5,6], index=list('abc')) \nc = pd.concat([a,b]) # 纵向叠加series\nd = pd.concat([a,b], axis=1) # 横向叠加\ne = d.T # 翻转\n```\n\n### 数据融化\n\n`pd.melt`\n\n可以把新建一个column，把其中某些column转成新的column的value，也非常实用，在seaborn作图的时候，用到过一些。\n\n### 随机采样\n\n`data.sample([n, frac, ...])`\n\n可以指定随机返回的数量，也可以指定比例。加入data的长度是1000，n<1000, frac在0-1之间取小数。\n\n### 后记\n\n写到这里，想写点什么的冲动消退，想想我又干了一件蠢事。什么模块什么功能特别是具体到哪个函数的用法，真的不要去试图记忆甚至汇总它们。如果你用的别人的，请查看他们的文档。如果是你自己写的，请自己写好文档。让这些电子记录成为你的“第二大脑”，这不就是我的初心么？所以像这种事情，只有这一次，以后的每一篇博文，都要对应我自己在数据分析或者日常生活中遇到的具体的可以用编程的方式解决的问题。","tags":["pandas"],"categories":["python"]},{"title":"基于python的DNA定量浓度计算器","url":"/2020/12/29/oligoCalculator/","content":"\n### 起因\n\n使用离线的OligoCalculator只能一个个序列拷贝进去然后输入A260的值，再计算。这样做太繁琐了，如果数量比较少还好。但如果一次性测定比较多的链，很容易复制出错。所以我在《[基于紫外可见光吸收光谱的DNA分子定量](https://sheldonxxd.github.io/2020/12/28/basicUVspectrumAnalysis/index.html)》文章的基础之上，进一步参照[Thermo的说明](https://www.sigmaaldrich.com/china-mainland/technical-documents/articles/biology/quantitation-of-oligos.html)写了一个能够批量自动根据吸光度计算样品浓度的python版本的oligo calculator。\n\n> 要插入自己之前的文章的url，记得加上index.html，不然无法跳转超链接。\n\n### 输入\n\n![image-20201229100558994](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229100558994.png)\n\n注意这个表格文件前面三列都是《基于紫外可见光吸收光谱的DNA分子定量》文章中代码处理后生成的，而Label、Sequence、Modufication以及DiluteRatio需要自己补充。拿到了这样一张表之后，可以使用下面的表进行计算。\n\n### 代码\n\n```python\nimport pandas as pd\n\ndef getNearestBaseCombination(seq):\n    n = len(seq)\n    data = []\n    for i in range(n-1):\n        data.append(seq[i:i+2])\n    return data\n\ndef getRepeatSingleBase(seq):\n    return list(seq[1:-1])\n\ndef getMolarConc(Abs, Seq, Length=1):\n    '''\n    === 输入参数 ===\n    Abs: Float，测定UV-Vis吸收光谱的峰值，一般为A260\n    Seq: String，寡核苷酸样品的序列 \n    Length: Float，光程，一般为1 cm\n    === 返回值 ===\n    E_seq: 单纯计算序列获得的消光系数，如有修饰基团，请到IDT上查询其消光系数后做浓度校正\n    Conc: Float, 测定样品的摩尔浓度，以uM（umol/L）为单位\n    如要获取原液样品，可自行乘以稀释倍数\n    '''\n    e = {'A':15400, 'C':7400, 'G':11500, 'T':8700, \n        'AA':27400, 'AC':21200, 'AG':25000, 'AT':22800,\n        'CA':21200, 'CC':14600, 'CG':18000, 'CT':15200,\n        'GA':25200, 'GC':17600, 'GG':21600, 'GT':20000,\n        'TA':23400, 'TC':16200, 'TG':19000, 'TT':16800}\n    \n    # 消光系数表格， 参考链接如下\n    # https://www.sigmaaldrich.com/china-mainland/technical-documents/articles/biology/quantitation-of-oligos.html\n    cut = getNearestBaseCombination(Seq)\n    repeat = getRepeatSingleBase(Seq)\n    E_seq = 0\n    for c in cut:\n        E_seq += e[c]\n    for r in repeat:\n        E_seq -= e[r]\n    # E的单位是L ⋅ mol-1 ⋅ cm-1\n    Conc = Abs/(E_seq*Length)*1e6\n    # Conc的单位是 uM\n    return E_seq,Conc\n\ndef main(filepath):\n    '''\n    filepath：输入汇总表格文件路径\n    表格中必须包含样品的吸光值（PeakValue）,序列（Sequence）和稀释倍数(DiluteRatio)三列信息\n    建议在`extractUVspectrum.py`生成的excel文件中补充序列和稀释倍数即可\n    计算完成后，在表格中新增消光系数和原液浓度两列信息\n    '''\n    data = pd.read_excel(filepath)\n    concs = []\n    Es = []\n    for idx, row in data.iterrows():\n        e,c = getMolarConc(row['PeakValue'], row['Sequence'])\n        c = c*row['DiluteRatio']\n        concs.append(c)\n        Es.append(e)\n    data['E(L⋅mol-1⋅cm-1)'] = Es\n    data['Conc(uM)'] = concs\n    data.to_excel(filepath, index=None)\n    print('浓度计算完成，已刷新表格数据！')\n\nif __name__ == \"__main__\":\n    filepath = \"/path/to/ssDNA定量结果.xlsx\"\n    main(filepath)\n```\n\n### 结果\n\n![image-20201229100438254](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229100438254.png)\n\n计算完成后，新出现了两列数据，分别是不同样本所对应的消光系数和稀释前原液的浓度。需要注意，**如果有修饰，自行到[IDT查询修饰基团的消光系数](https://sg.idtdna.com/site/Catalog/Modifications)，跟此表中的消光系数相加，然后根据琅勃比尔定律对浓度进行校正**。\n\n![image-20201229103945287](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229103945287.png)\n\n如上图所示，在excel表格中进行操作即可，最后得到最终的准确的定量浓度结果。","tags":["DNA","calculator"],"categories":["python"]},{"title":"基于紫外可见光吸收光谱的DNA分子定量","url":"/2020/12/28/basicUVspectrumAnalysis/","content":"\n我们实验室采集UV-Vis吸收光谱，只能一个一个样品地来。有时候我要对几十条DNA单链做定量的时候，一个个去记录 A_260nm 的数值比较烦（而且有时候峰值会有微弱漂移），所以打算用python写个小工具来实现峰值的提取。\n\n### 数据文件分析\n\n通过仪器配套软件可以导出csv文件，命名格式是`Sample??.样品.csv`。\n\n![image-20201228134634805](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228134634805.png)\n\n但是csv文件内数据比较脏，具体如下：\n\n![image-20201228134427947](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228134427947.png)\n\n第一行和最后一行都不是正经的内容，而第二行的column name也很随意，所以后面在使用pandas模块读取csv文件的时候要做一些处理。\n\n### 完整代码\n\n要实现功能除了得正确读取数据之外，还有以下几个要点：\n\n1. 使用glob收集csv文件（列表顺序未必等同于上样顺序）\n2. 提取sample的ID（对应着你测定样品的顺序）\n3. 使用`np.argmax`获取峰值所在序列的索引，然后取峰值\n4. 根据sampleID对数据进行排序\n\n具体代码如下：\n\n```python\nimport os, glob\nimport numpy as np\nimport pandas as pd\nimport re\n\ndef exportMaxPeak(wks):\n    '''\n    输入包含UV吸收光谱导出csv文件的路径\n    输出A260nm的峰值数据表excel到目录下\n    注意不要导出blank的光谱\n    '''\n    files = glob.glob(os.path.join(wks,'*.csv'))\n    data = []\n    pattern = 'Sample(\\d+).样品.csv'\n    for f in files:\n        # 处理文件名提取ID\n        _, fname = os.path.split(f)\n        # 读取光谱数据\n        label = re.search(pattern, fname)\n        sid = label.group(1) # 直接提取数字\n        sid = float(sid)\n        df = pd.read_csv(f, header=1)\n        mid = np.argmax(df[\"A\"])\n        max_wav = df[\"nm\"][mid]\n        max_OD = df[\"A\"][mid]\n        # 收集数据到列表\n        data.append([int(sid), max_wav, max_OD])\n    # 处理数据表格\n    data = pd.DataFrame(data, columns=[\"Sample\",\"Wavelength\", \"PeakValue\"])\n    data.sort_values(by='Sample', inplace=True)  # 排序\n    data_path = os.path.join(wks,\"A260-Max.xlsx\")\n    data.to_excel(data_path, index=False)\n    print(\"数据已保存到{}!\".format(data_path))\n\nif __name__=='__main__':\n    # 修改变量wks的值为你存放数据的目录\n    wks = r'D:\\Lab\\Data\\UV-Vis\\20201228'\n    exportMaxPeak(wks)\n```\n\n### 处理效果\n\n![image-20201228135130971](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228135130971.png)\n\n然后再基于生成的这个汇总表，把实际sample的lable以及序列填写上去，再使用`Oligo Calculator`即可计算出来样品的浓度了。\n\n![image-20201228135309897](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228135309897.png)\n\n### 使用OligoCalculator工具\n\n![image-20201228131109199](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228131109199.png)\n\nOligoCalculator工具[点此链接](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/Oligo%20caculator.exe)下载。\n\n把序列输入进去（注意去除一些修饰），然后输入紫外测得的A260峰值，点击`Caculate`即可计算。","tags":["spectrum","DNA"],"categories":["python"]},{"title":"搭建阿里云OSS图床","url":"/2020/12/27/mergeAllNotes/","content":"\n我的笔记很多东西都比较零碎，这样不适合构建一个比较完整有序的笔记系统（第二大脑）。自从上手了github-pages之后，决定全面转向markdown+github，这样可以到处访问查看，为了解决插图文件大小问题，还特地购买了阿里云的对象存储服务。一系列的操作主要参考《[【Typora】typora+picgo+阿里云oss搭建图床](https://www.cnblogs.com/myworld7/p/13132549.html)》。\n\n### 购买阿里云OSS并配置图床\n\n可以直接搜索[包月的套餐](https://common-buy.aliyun.com/?spm=5176.7933691.1309819..68b22a66FQKm7f&commodityCode=ossbag&request=%7B%22region%22%3A%22china-common%22%7D#/buy)：\n\n![image-20201227162121971](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227162121971.png)\n\n我选择的是40GB，大陆通用，两年。购买之后进入控制台新建一个bucket（水桶）。\n\n![image-20201227161929457](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161929457.png)\n\n注意权限设置为公共可读，不然上传发布到github-pages上就看不到了。\n\n### 下载和配置picgo\n\ntypora支持picgo图床上传工具，直接到github上下载最新版本\n\nhttps://github.com/Molunerfinn/PicGo/releases\n\n把阿里云的Access Key以及其他信息复制填入配置选项即可：\n\n![image-20201227161730671](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161730671.png)\n\n### 配置typora\n\n![image-20201227161535243](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161535243.png)\n\n到这一步，就可以开心的在本地使用typora写markdown，截图使用`snipaste`，直接复制粘贴过来就开始上传到自己的图床。不过第一张图片会先打开picgo程序，所以稍微慢一点。\n\n> 想想我之前还写了一个python脚本缩小图片大小，现在完全没有这个必要了，啊哈哈哈哈！","tags":["图床"],"categories":["blog"]},{"title":"使用python提取txt文本中关键信息并汇总到表格","url":"/2020/12/26/collectFilesUsingPython/","content":"\n使用python处理文件、文件夹和路径之类的事情，主要是用到`os`、`glob`、`re`和`shutil`等模块。\n\n### 批量创建子目录\n\n在处理数据的时候，不可避免地会涉及到各种文件的批量处理。比如我在处理和分析数据的时候，需要根据单个原始数据创建子目录用于存放后续分析产生的各种中间数据。所以我写了下面这段代码：\n\n```python\nimport os\nfrom glob import glob\n\ndef makeSubdirectory(wks):\n    '''\n    wks: 某次DNA-PAINT实验存放lif文件的目录\n    此程序会对lif文件自动新建同名子目录\n    '''\n    lifs = glob(os.path.join(wks, \"*.lif\"))\n    for lif in lifs:\n        filepath, ext = os.path.splitext(lif)  \n        # 分离文件的拓展名和其他路径，比字符串split('.')更安全\n        try:\n            os.mkdir(filepath+'/')\n        except:\n            print('Subdirectory Existed! -- %s'%(filepath))\n    print('Job Finished!')\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201221\" \n    makeSubdirectory(wks)\n```\n\n需要注意的是：\n\n1. `os.mkdir`命令如果是路径已经存在就会报错\n2. glob中可以使用`*`作为通配符匹配所有后缀为`lif`的原始数据文件，返回一个list\n3. `os.path.splitext`可以把文件路径的后缀跟名字分开\n4. `try ... except ...` 可以有效处理程序运行中的异常情况\n\n### 从txt文件中收集图像参数\n\nlif文件是leica显微镜产生的数据，可以使用ImageJ打开，然后保存为tif，方便后面的处理。此时，图像的一些相关信息可以在ImageJ中查看，比如激光强度、每帧间隔时间等。然后Info还可以保存为txt文件。为了方便，我写了一个自动化脚本，收集Info文件中的重要参数并汇总到一个excel表格中。代码如下：\n\n```python\nimport os\nfrom glob import glob\nimport pandas as pd \nimport re \n\ndef findTxt(wks):\n    infos = []\n    for d in os.walk(wks):\n        path, dirs, files = d\n        for f in files:\n            if f.startswith('Info') and f.endswith('txt'):\n                # 根据文件命名特征进行筛选\n                txt = os.path.join(path,f)\n                infos.append(txt)\n    return infos\n\ndef extractInfo(txt):\n    '''\n    txt: Info for **.txt的文件路径\n    ==============================\n    从txt中提取以下关键信息：\n    1. **激光强度**：WFLaserChannelInfo_CurrentValue = 100\n    2. 穿透深度：TIRF_PenetrationDepth = 171.008186233857\n    3. 激光波长：WFLaserChannelInfo_Wavelength = 638\n    4. 相机温度：ATLCameraSettingDefinition|TargetTemperature = -75\n    5. **每帧间隔时间**：Frame interval: 0.02046 sec\n    '''\n    recs = {'WFLaserChannelInfo_CurrentValue = ':0, \n           'TIRF_PenetrationDepth = ':0, \n           'WFLaserChannelInfo_Wavelength = ':0,\n           'TargetTemperature = ':0,\n           'Frame interval:':0}\n    # 避免构造复杂的pattern正则表达式，把复杂的写入key\n    with open(txt) as f:\n        data = f.read()\n    \n    for key in recs:\n        pattern = '{}(.+)'.format(key)\n#         print(pattern)\n        a = re.search(pattern, data)\n        r = a.group(1)\n        if r.endswith('sec'):\n            r = r.split(' ')[1]\n        recs[key] = float(r)\n    return recs\n\ndef main(wks):\n    infos = findTxt(wks)\n    newkeys = ['ExIntensity', 'PenetrationDepth','ExWavelength','ccdTemperature','frameInterval']\n    data = {x:[] for x in newkeys}\n    for txt in infos:\n        recs = extractInfo(txt)\n        # 字典的键名修改，使用pop方法\n        keys = list(recs.keys())\n    #     print(keys)\n        for newkey, oldkey in zip(newkeys, keys):\n            data[newkey].append(recs[oldkey])\n    data = pd.DataFrame(data)\n    data['filepath'] = infos\n    data.to_excel(os.path.join(wks, 'infos.xlsx'), index=None)\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201223\" \n    main(wks)\n```\n\n代码稍微长了一点，还弄出来两个函数。寻找Info文件还好说，因为文件命名都带有Info前缀，然后文件又是txt类型，直接利用`os.walk`获取总的工作目录下所有的文件，然后遍历，根据文件名进行筛选即可。注意`os.walk`返回的是一个迭代器，每次吐出来一个根目录路径，根目录下文件夹列表以及文件列表组成的三元元组。\n\n从`Info.txt`中提取信息还颇为费事，因为Info中信息量非常大，我不可能用`f.readlines()`的方法去读取指定的行，所以选择了用正则匹配的方式。但是正则我用得很少，而且为了减轻正则pattern的设计难度，我把pattern中变化的部分尽可能地定义到字典的key中。我觉得这个思路（拿通用性换效率）还挺不错的——能把事情解决就好。re模块中我常用search和findall两个功能。需要注意的是：\n\n1. re.search只返回找到的第一个匹配的字符串\n2. re.findall返回所有匹配的字符串所组成的list\n3. 如果pattern中放括号，括号可以作为group提取信息，但是注意group的索引id\n4. 修改字典的键名的方法\n5. 使用pandas模块，构建dataframe，然后方便保存为excel可以打开的xlsx文件\n\n实现效果如下：\n\n![image-20201227120627018](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227120627018.png)\n\n\n\n","tags":["filepath"],"categories":["python"]},{"title":"批量缩小博客中的图片","url":"/2020/12/24/resizeImages/","content":"\n目前我最熟悉[python](https://www.liaoxuefeng.com/wiki/1016959663602400)，另外也会写一点[ImageJ的脚本处理](https://zhuanlan.zhihu.com/p/60999196)和分析图像。可是编程技能“用进废退”，特别是人年纪越大，对于各种繁复的代码细节就越难记忆。所以很多码农都会写博客，并且把博客当作“第二大脑”。我当然也不例外，以后也时不时会把自己写过的代码整理到博客上。\n\n### 需求\n\n我喜欢在文章中插入截图，实现图文并茂的效果。但是github的仓库容量有限，而且图片大小尺寸不一造成不美观。虽然可以定义`css`文件来控制显示，但它只能解决美观问题。而使用图床能够避免图片太多超出仓库容量限制，但把各种图片放到公共图床上既不能让我放心，也跟我目前的workflow不兼容。所以我决定写一个小的工具代码，实现对插图文件的resize。具体有以下需求：\n\n1. 支持对png, jpg, jpeg等常见图像的resize\n2. 已经resize的不要进行重复的操作\n3. 原始大图要保留而且方便查找\n4. 不破坏markdown中对图片的引用\n\n### 代码\n\n代码要点如下：\n\n1. 使用glob批量读取多种格式的图片地址到列表\n2. 过滤已经被resize处理的图片\n3. pillow库中的resize方法的调用\n\n废话不多说，直接上代码：\n\n```python\nfrom PIL import Image \nimport os \nfrom glob import glob\n\ndef resizeImages(wks):\n    '''\n    输入makrdown附件目录地址，\n    对其中所有png\\jpg\\jpeg等格式的图像进行缩小，\n    固定宽度到600，\n    原图被重命名带`_large.*`后缀\n    如果有带此后缀的，则两张图像都不进行处理\n    '''\n    files = []\n    formats = ['png','jpg','jpeg']\n    # 如果想压缩gif，请参考\n    # https://blog.csdn.net/huanyue6660/article/details/79423326\n    for ext in formats:\n        files.extend(glob(os.path.join(wks, '*.{}'.format(ext))))  # 使用extend的方式合并新增的list\n    \n    jobs = []\n    for f in files:\n        ext = os.path.splitext(f)[-1]\n        if (f+'_large%s'%(ext) in files) or (f.endswith('large%s'%(ext))):\n            continue\n        else:\n            jobs.append(f)\n\n    for job in jobs:\n        ext = os.path.splitext(job)[-1]  # 获取文件后缀名，注意这个带点\n        a = Image.open(job)\n        a.save(job+'_large{}'.format(ext))\n        w, h = a.size\n        if w>600:\n            # 只缩小大图\n            b = a.resize(size=(600, int(h*600/w))) # 注意size得是整数\n        else:\n            b = a\n        b.save(job)\n        print('{} resized to width = 600 px!'.format(job))\n\nif __name__ == \"__main__\":\n    wks = r\"D:\\hexo\\blog\\source\\_posts\"  # windows系统文件路径，注意不要去掉r\n    subdirs = glob(os.path.join(wks,\"*/\"))\n    # print(subdirs)\n    # 对_posts目录下所有图片进行处理\n    for subdir in subdirs:\n        resizeImages(subdir)\n```\n\n### 使用\n\n我喜欢把这种高复用的小工具代码做好接口，写到python脚本中，然后用配置好的vscode打开，根据需求修改参数（这里是`_posts`目录的地址），然后按`Ctrl+F5`运行。\n\n### 结果\n\n![image-20201224114927774](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201224114927774.png)\n\n原始图像被重名为带`_large`后缀的文件。","tags":["pillow"],"categories":["python"]},{"title":"在gitub-pages搭建Hexo博客","url":"/2020/12/20/setupHexoWebsite/","content":"\n\n我打算用**hexo**结合**github pages**构建一个轻博客网站，每周发表至少一篇博文。\n\n<!-- more -->\n\n### Hexo建站参考教程\n\n使用hexo建站到**github pages**的完整教程推荐如下：\n\nhttps://zhuanlan.zhihu.com/p/78467553\n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://theme-stun.github.io/docs/zh-CN/\n\nhttps://hexo.io/zh-cn/index.html\n\nhttps://blog.csdn.net/hhgggggg/article/details/77853665\n\n### 使用**Stun**主题\n\n[stun主题](https://theme-stun.github.io/docs/zh-CN/guide/quick-start.html#%E5%AE%89%E8%A3%85)比较好看，我还稍微配置了一下可以[跟typora连用](https://www.cnblogs.com/caoayu/articles/13855081.html)（主要是为了能够正常显示插图和附件）。效果如下图：\n\n![image-20201222100030247](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201222100030247.png)\n\n### 需要安装的插件列表\n\n1. git相关插件，用于deploy到github pages上\n2. search相关插件，方便对站内内容进行搜索\n3. asset-imager相关插件，方便插图附件之类的\n\n### 开启评论系统\n\ngitalk插件是利用github仓库的issue功能，为了节约空间，我另外开了一个**sea-comments**的仓库作为存储。然后照着以下教程进行操作：\n\nhttps://www.jianshu.com/p/4242bb065550\n\nhttps://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#gitalk\n\nhttps://github.com/gitalk/gitalk\n\n需要注意的是，stun主题已经嵌入了gitalk，只要到stun主题下的`_config.yaml`下添加自己评论仓库的一些信息即可。\n\n> gitalk会出现Error Network报错，弃用~，之后使用`valine`，评论数据存储在`LeanCloud`中，如有必要，及时备份。\n>\n> https://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#valine\n\n### 常用命令\n\n需要进入到hexo网站的本地根目录下执行命令，以windows系统为例：\n\n1. `hexo clean && hexo g && hexo s`: 若修改了**_config.yaml**配置文件，需要清除 public目录下所有内容，然后根据新的配置文件重新生成静态网页，并且启动本地服务器\n2. `hexo clean && hexo d`：修改配置后，重新发布到gitee pages上。\n\n### 注意事项\n\n1. 尽量使用`hexo new`命令生成 pages或者新的markdown文件，然后到`_posts`目录下对markdown文件进行编辑修改（使用typora）。\n2. markdown文件命名只能为英文或者数字，而标题可以在markdown文件里边的**Front Matter YAML**中进行设定（**Front Matter YAML设定非常重要**，参考[链接](https://hexo.io/zh-cn/docs/front-matter)）。\n3. github仓库容量有限（< 1 GB），长期使用个人博客，注意不要插入太多太大的图片，建议是一篇博文图片不超过10张。\n4. 博客内容完全公开，注意不要发布违法或者引发舆论的内容，并注意保护个人隐私。","tags":["hexo"],"categories":["blog"]}]