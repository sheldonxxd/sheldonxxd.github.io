[{"title":"搭建阿里云OSS图床","url":"/2020/12/27/mergeAllNotes/","content":"\n我的笔记很多东西都比较零碎，这样不适合构建一个比较完整有序的笔记系统（第二大脑）。自从上手了github-pages之后，决定全面转向markdown+github，这样可以到处访问查看，为了解决插图文件大小问题，还特地购买了阿里云的对象存储服务。一系列的操作主要参考《[【Typora】typora+picgo+阿里云oss搭建图床](https://www.cnblogs.com/myworld7/p/13132549.html)》。\n\n### 购买阿里云OSS并配置图床\n\n可以直接搜索[包月的套餐](https://common-buy.aliyun.com/?spm=5176.7933691.1309819..68b22a66FQKm7f&commodityCode=ossbag&request=%7B%22region%22%3A%22china-common%22%7D#/buy)：\n\n![image-20201227162121971](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227162121971.png)\n\n我选择的是40GB，大陆通用，两年。购买之后进入控制台新建一个bucket（水桶）。\n\n![image-20201227161929457](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161929457.png)\n\n注意权限设置为公共可读，不然上传发布到github-pages上就看不到了。\n\n### 下载和配置picgo\n\ntypora支持picgo图床上传工具，直接到github上下载最新版本\n\nhttps://github.com/Molunerfinn/PicGo/releases\n\n把阿里云的Access Key以及其他信息复制填入配置选项即可：\n\n![image-20201227161730671](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161730671.png)\n\n### 配置typora\n\n![image-20201227161535243](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161535243.png)\n\n到这一步，就可以开心的在本地使用typora写markdown，截图使用`snipaste`，直接复制粘贴过来就开始上传到自己的图床。不过第一张图片会先打开picgo程序，所以稍微慢一点。\n\n> 想想我之前还写了一个python脚本缩小图片大小，现在完全没有这个必要了，啊哈哈哈哈！","categories":["diary"]},{"title":"使用python提取txt文本中关键信息并汇总到表格","url":"/2020/12/26/collectFilesUsingPython/","content":"\n使用python处理文件、文件夹和路径之类的事情，主要是用到`os`、`glob`、`re`和`shutil`等模块。\n\n### 批量创建子目录\n\n在处理数据的时候，不可避免地会涉及到各种文件的批量处理。比如我在处理和分析数据的时候，需要根据单个原始数据创建子目录用于存放后续分析产生的各种中间数据。所以我写了下面这段代码：\n\n```python\nimport os\nfrom glob import glob\n\ndef makeSubdirectory(wks):\n    '''\n    wks: 某次DNA-PAINT实验存放lif文件的目录\n    此程序会对lif文件自动新建同名子目录\n    '''\n    lifs = glob(os.path.join(wks, \"*.lif\"))\n    for lif in lifs:\n        filepath, ext = os.path.splitext(lif)  \n        # 分离文件的拓展名和其他路径，比字符串split('.')更安全\n        try:\n            os.mkdir(filepath+'/')\n        except:\n            print('Subdirectory Existed! -- %s'%(filepath))\n    print('Job Finished!')\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201221\" \n    makeSubdirectory(wks)\n```\n\n需要注意的是：\n\n1. `os.mkdir`命令如果是路径已经存在就会报错\n2. glob中可以使用`*`作为通配符匹配所有后缀为`lif`的原始数据文件，返回一个list\n3. `os.path.splitext`可以把文件路径的后缀跟名字分开\n4. `try ... except ...` 可以有效处理程序运行中的异常情况\n\n### 从txt文件中收集图像参数\n\nlif文件是leica显微镜产生的数据，可以使用ImageJ打开，然后保存为tif，方便后面的处理。此时，图像的一些相关信息可以在ImageJ中查看，比如激光强度、每帧间隔时间等。然后Info还可以保存为txt文件。为了方便，我写了一个自动化脚本，收集Info文件中的重要参数并汇总到一个excel表格中。代码如下：\n\n```python\nimport os\nfrom glob import glob\nimport pandas as pd \nimport re \n\ndef findTxt(wks):\n    infos = []\n    for d in os.walk(wks):\n        path, dirs, files = d\n        for f in files:\n            if f.startswith('Info') and f.endswith('txt'):\n                # 根据文件命名特征进行筛选\n                txt = os.path.join(path,f)\n                infos.append(txt)\n    return infos\n\ndef extractInfo(txt):\n    '''\n    txt: Info for **.txt的文件路径\n    ==============================\n    从txt中提取以下关键信息：\n    1. **激光强度**：WFLaserChannelInfo_CurrentValue = 100\n    2. 穿透深度：TIRF_PenetrationDepth = 171.008186233857\n    3. 激光波长：WFLaserChannelInfo_Wavelength = 638\n    4. 相机温度：ATLCameraSettingDefinition|TargetTemperature = -75\n    5. **每帧间隔时间**：Frame interval: 0.02046 sec\n    '''\n    recs = {'WFLaserChannelInfo_CurrentValue = ':0, \n           'TIRF_PenetrationDepth = ':0, \n           'WFLaserChannelInfo_Wavelength = ':0,\n           'TargetTemperature = ':0,\n           'Frame interval:':0}\n    # 避免构造复杂的pattern正则表达式，把复杂的写入key\n    with open(txt) as f:\n        data = f.read()\n    \n    for key in recs:\n        pattern = '{}(.+)'.format(key)\n#         print(pattern)\n        a = re.search(pattern, data)\n        r = a.group(1)\n        if r.endswith('sec'):\n            r = r.split(' ')[1]\n        recs[key] = float(r)\n    return recs\n\ndef main(wks):\n    infos = findTxt(wks)\n    newkeys = ['ExIntensity', 'PenetrationDepth','ExWavelength','ccdTemperature','frameInterval']\n    data = {x:[] for x in newkeys}\n    for txt in infos:\n        recs = extractInfo(txt)\n        # 字典的键名修改，使用pop方法\n        keys = list(recs.keys())\n    #     print(keys)\n        for newkey, oldkey in zip(newkeys, keys):\n            data[newkey].append(recs[oldkey])\n    data = pd.DataFrame(data)\n    data['filepath'] = infos\n    data.to_excel(os.path.join(wks, 'infos.xlsx'), index=None)\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201223\" \n    main(wks)\n```\n\n代码稍微长了一点，还弄出来两个函数。寻找Info文件还好说，因为文件命名都带有Info前缀，然后文件又是txt类型，直接利用`os.walk`获取总的工作目录下所有的文件，然后遍历，根据文件名进行筛选即可。注意`os.walk`返回的是一个迭代器，每次吐出来一个根目录路径，根目录下文件夹列表以及文件列表组成的三元元组。\n\n从`Info.txt`中提取信息还颇为费事，因为Info中信息量非常大，我不可能用`f.readlines()`的方法去读取指定的行，所以选择了用正则匹配的方式。但是正则我用得很少，而且为了减轻正则pattern的设计难度，我把pattern中变化的部分尽可能地定义到字典的key中。我觉得这个思路（拿通用性换效率）还挺不错的——能把事情解决就好。re模块中我常用search和findall两个功能。需要注意的是：\n\n1. re.search只返回找到的第一个匹配的字符串\n2. re.findall返回所有匹配的字符串所组成的list\n3. 如果pattern中放括号，括号可以作为group提取信息，但是注意group的索引id\n4. 修改字典的键名的方法\n5. 使用pandas模块，构建dataframe，然后方便保存为excel可以打开的xlsx文件\n\n实现效果如下：\n\n![image-20201227120627018](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227120627018.png)\n\n\n\n","tags":["python"],"categories":["tools"]},{"title":"使用hexo-tag-aPlayer插件","url":"/2020/12/25/useMusicPlayer/","content":"\n插件github仓库地址如下：\n\n[https://github.com/MoePlayer/hexo-tag-aplayer](https://github.com/MoePlayer/hexo-tag-aplayer)\n\n右键点击hexo站点根目录文件夹，git bash here（在windows系统中一定要养成这个习惯，避免在cmd里面搞），然后运行以下命令安装：\n\n```\nnpm install --save hexo-tag-aplayer\n```\n\n插入音乐方法，在markdown中写入以下信息即可：\n\n```\n{% aplayer title author url [picture_url, narrow, autoplay, width:xxx, lrc:xxx] %}\n```\n\n如我选择把mp3文件放在附录文件夹内，就变成下边这样：\n\n```\n{% aplayer \"Helmet to Helmet\" \"Brand X Music\" \"Helmet to Helmet.mp3\" \"picture.jpg\" %}\n```\n\n测试音乐如下：\n\n{% aplayer \"Helmet to Helmet\" \"Brand X Music\" \"Helmet to Helmet.mp3\" \"picture.jpg\" %}\n\n","tags":["hexo"],"categories":["tools"]},{"title":"批量缩小博客中的图片","url":"/2020/12/24/resizeImages/","content":"\n目前我最熟悉[python](https://www.liaoxuefeng.com/wiki/1016959663602400)，另外也会写一点[ImageJ的脚本处理](https://zhuanlan.zhihu.com/p/60999196)和分析图像。可是编程技能“用进废退”，特别是人年纪越大，对于各种繁复的代码细节就越难记忆。所以很多码农都会写博客，并且把博客当作“第二大脑”。我当然也不例外，以后也时不时会把自己写过的代码整理到博客上。\n\n### 需求\n\n我喜欢在文章中插入截图，实现图文并茂的效果。但是github的仓库容量有限，而且图片大小尺寸不一造成不美观。虽然可以定义`css`文件来控制显示，但它只能解决美观问题。而使用图床能够避免图片太多超出仓库容量限制，但把各种图片放到公共图床上既不能让我放心，也跟我目前的workflow不兼容。所以我决定写一个小的工具代码，实现对插图文件的resize。具体有以下需求：\n\n1. 支持对png, jpg, jpeg等常见图像的resize\n2. 已经resize的不要进行重复的操作\n3. 原始大图要保留而且方便查找\n4. 不破坏markdown中对图片的引用\n\n### 代码\n\n代码要点如下：\n\n1. 使用glob批量读取多种格式的图片地址到列表\n2. 过滤已经被resize处理的图片\n3. pillow库中的resize方法的调用\n\n废话不多说，直接上代码：\n\n```python\nfrom PIL import Image \nimport os \nfrom glob import glob\n\ndef resizeImages(wks):\n    '''\n    输入makrdown附件目录地址，\n    对其中所有png\\jpg\\jpeg等格式的图像进行缩小，\n    固定宽度到600，\n    原图被重命名带`_large.*`后缀\n    如果有带此后缀的，则两张图像都不进行处理\n    '''\n    files = []\n    formats = ['png','jpg','jpeg']\n    # 如果想压缩gif，请参考\n    # https://blog.csdn.net/huanyue6660/article/details/79423326\n    for ext in formats:\n        files.extend(glob(os.path.join(wks, '*.{}'.format(ext))))  # 使用extend的方式合并新增的list\n    \n    jobs = []\n    for f in files:\n        ext = os.path.splitext(f)[-1]\n        if (f+'_large%s'%(ext) in files) or (f.endswith('large%s'%(ext))):\n            continue\n        else:\n            jobs.append(f)\n\n    for job in jobs:\n        ext = os.path.splitext(job)[-1]  # 获取文件后缀名，注意这个带点\n        a = Image.open(job)\n        a.save(job+'_large{}'.format(ext))\n        w, h = a.size\n        if w>600:\n            # 只缩小大图\n            b = a.resize(size=(600, int(h*600/w))) # 注意size得是整数\n        else:\n            b = a\n        b.save(job)\n        print('{} resized to width = 600 px!'.format(job))\n\nif __name__ == \"__main__\":\n    wks = r\"D:\\hexo\\blog\\source\\_posts\"  # windows系统文件路径，注意不要去掉r\n    subdirs = glob(os.path.join(wks,\"*/\"))\n    # print(subdirs)\n    # 对_posts目录下所有图片进行处理\n    for subdir in subdirs:\n        resizeImages(subdir)\n```\n\n### 使用\n\n我喜欢把这种高复用的小工具代码做好接口，写到python脚本中，然后用配置好的vscode打开，根据需求修改参数（这里是`_posts`目录的地址），然后按`Ctrl+F5`运行。\n\n### 结果\n\n![image-20201224114927774](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201224114927774.png)\n\n原始图像被重名为带`_large`后缀的文件。","tags":["python","图像处理"],"categories":["tools"]},{"title":"单分子定位超分辨成像数据分析","url":"/2020/12/23/SMLMpostProcessing/","content":"\n最近在尝试DNA-PAINT相关的课题，遇到了不少问题。DNA-PAINT属于单分子定位超分辨成像（SMLM）的一种，而要想拿到质量好的超分辨结果，在**样品制备**、**仪器状态**、**拍摄参数**和**数据后处理**等四个方面都有很高的要求。\n\n目前我主要参考DNA-PAINT的泰斗[Ralf Jungmann的相关工作](http://www.nature.com/articles/nprot.2017.024)开展课题，但是不得不承认上述四个方面，我们跟人家还有很大的差距，以至于蹉跎数月，还不能稳定地产出好的结果。所以，即便超分辨领域拿诺贝尔奖已经多年，但这仍然是一门手艺活儿。\n\n在单分子定位数据后续分析方面，我也主要沿用[picasso软件](https://github.com/jungmannlab/picasso)，虽然latest version在我的电脑上有诸多版本兼容性问题，但我还是通过自己写小工具代码的方式，基本上能够实现完整的PAINT相关的统计指标量的获取。\n\nDNA-PAINT从属于SMLM，可能前期样本制备和数据采集上跟其他方法有所差异，但最后得到的raw数据基本上是一致的——它本质上是一个movie。所以SMLM的数据分析方法，具有较高的通用性，因此在这个方面，也有不少文章出来。\n\n### 利用贝叶斯方法分析单分子结合事件\n\n单分子可逆结合到界面上，它的结合和解离动力学，以及结合了多少个分子，这些东西都需要从`Intensity-Time Curves`中统计分析出来。我之前尝试过使用隐马尔可夫模型来分析过荧光淬灭的台阶曲线，但感觉几个概率矩阵的参数难调。而贝叶斯估计，虽然我目前还不大懂，但有一篇工作《[An automated Bayesian pipeline for rapid analysis of single-molecule binding data](http://www.nature.com/articles/s41467-018-08045-5)》做得还不错。可以看到它通过这种分析方法，能够把有很多毛刺的曲线的状态分类表示出来：\n\n![image-20201223115958200](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201223115958200.png)\n\n除了贝叶斯，最大似然估计和隐马尔可夫的方法都有看到过报道，后面再进行补充把！\n\n### 超分辨成像结果中的模式识别\n\n一般来说，细胞内有功能的蛋白复合体具有一致的微观结构，表现出来某种pattern。当这些结构分布密集，而且还多色混合在一起，人眼是很难分辨的。所以这种模式识别也具有重要的意义。\n\n![image-20201223120508029](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201223120508029.png)\n\n《[Ultrafast data mining of molecular assemblies in multiplexed high-density super-resolution images](http://www.nature.com/articles/s41467-018-08048-2)》这篇文章就利用算法实现密集图像中比较简单的模式识别，我觉得这个方法在我的研究领域经常能用到，而且不仅仅局限于SMLM的超分辨数据，比如一些透射电子显微镜或者原子力显微镜的结果都可用。\n\n另外前一阵子《Nano Letters》杂志上也有一篇文章《[Nanoscale Pattern Extraction from Relative Positions of Sparse 3D Localizations](https://pubs.acs.org/doi/10.1021/acs.nanolett.0c03332)》涉及到模式的识别与提取，不过相对来说实现的方法可能比较low：\n\n![image-20201223120931550](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201223120931550.png)\n\n这种方法基于圆环结构pattern，自己提出了一个Rotational symmetry models，然后（我估计是用模拟数据，文章还没细看😂）进行拟合训练，参数调好之后，就拿去对raw data进行处理。感觉有点类似炼丹术。","tags":["SMLM"],"categories":["reading"]},{"title":"文献阅读-20201222","url":"/2020/12/22/read-20201222/","content":"\n掌握行业内最新动态非常重要，这样一来可以开阔视野获得灵感，二来能够避免自己做很多无用功，所以**文献阅读是做学术研究的第一要事**。我主要用 mendeley 软件进行文献管理，好处虽然很多，但毕竟还是工具。要想真正消化海量文献，除了在 paper 上零碎的 mark，更多是需要思考归纳总结。\n\n<!-- more -->\n\n### protein nanoparticle\n\n基于蛋白质或者多肽链组装的纳米颗粒，自然界典型的就是病毒。如果拿DNA、RNA和多肽相比较，感觉DNA纳米结构是很容易搞的，RNA很容易被降解，而多肽可能组装不是很好控制。《Cell》上有文开发了一种基于protein nanoparticle的新冠病毒疫苗[^1]。这篇文章的作图配色不错。\n\n![image-20201222094915030](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201222094915030.png)\n\n之前还有《JACS》上介绍protein nanocage以及peptide自组装之类的工作[^2]，我觉得还挺有意思的，如果什么时候能够像DNA一样更加容易控制设计于合成，就非常好了。甚至更早的用protein nanocage作为DNA protocell的外壳的工作[^3]，说明这种基于蛋白的纳米颗粒已经越来越多的被使用了。\n\n[^1]: https://linkinghub.elsevier.com/retrieve/pii/S0092867420314501\n[^2]: https://pubs.acs.org/doi/10.1021/jacs.0c07285\n[^3]: http://www.nature.com/articles/s41565-019-0399-9\n\n### DNA polymer/protocell/droplet\n\n想不到很好的名字概括，就是DNA作为类似高分子做了一个毛线球一样，主要是`Andreas Walther`作为通讯作者的一系列工作（[实验室网站](https://www.walther-group.com/research)）。上一次文献大组会的时候我讲过一篇他们发表在《Chem》上的基于DNA的相分离（Liquid-Liquid Phase Seperation）的工作[^4]，后来YGB师兄跟我分享了这个人其它一些非常类似的东西，特别是同期一篇在《Nature Communications》上的也是用ATP驱动做self-sorting的组装体的工作[^5]。\n\n![image-20201222095009157](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201222095009157.png)\n\n[^4]: https://linkinghub.elsevier.com/retrieve/pii/S2451929420304861\n[^5]: http://www.nature.com/articles/s41467-020-17479-9","tags":["paper"],"categories":["reading"]},{"title":"在gitub-pages搭建Hexo博客","url":"/2020/12/20/setupHexoWebsite/","content":"\n\n我打算用**hexo**结合**github pages**构建一个轻博客网站，每周发表至少一篇博文。\n\n<!-- more -->\n\n### Hexo建站参考教程\n\n使用hexo建站到**github pages**的完整教程推荐如下：\n\nhttps://zhuanlan.zhihu.com/p/78467553\n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://theme-stun.github.io/docs/zh-CN/\n\nhttps://hexo.io/zh-cn/index.html\n\nhttps://blog.csdn.net/hhgggggg/article/details/77853665\n\n### 使用**Stun**主题\n\n[stun主题](https://theme-stun.github.io/docs/zh-CN/guide/quick-start.html#%E5%AE%89%E8%A3%85)比较好看，我还稍微配置了一下可以[跟typora连用](https://www.cnblogs.com/caoayu/articles/13855081.html)（主要是为了能够正常显示插图和附件）。效果如下图：\n\n![image-20201222100030247](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201222100030247.png)\n\n### 需要安装的插件列表\n\n1. git相关插件，用于deploy到github pages上\n2. search相关插件，方便对站内内容进行搜索\n3. asset-imager相关插件，方便插图附件之类的\n\n### 开启评论系统\n\ngitalk插件是利用github仓库的issue功能，为了节约空间，我另外开了一个**sea-comments**的仓库作为存储。然后照着以下教程进行操作：\n\nhttps://www.jianshu.com/p/4242bb065550\n\nhttps://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#gitalk\n\nhttps://github.com/gitalk/gitalk\n\n需要注意的是，stun主题已经嵌入了gitalk，只要到stun主题下的`_config.yaml`下添加自己评论仓库的一些信息即可。\n\n> gitalk会出现Error Network报错，弃用~，之后使用`valine`，评论数据存储在`LeanCloud`中，如有必要，及时备份。\n>\n> https://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#valine\n\n### 常用命令\n\n需要进入到hexo网站的本地根目录下执行命令，以windows系统为例：\n\n1. `hexo clean && hexo g && hexo s`: 若修改了**_config.yaml**配置文件，需要清除 public目录下所有内容，然后根据新的配置文件重新生成静态网页，并且启动本地服务器\n2. `hexo clean && hexo d`：修改配置后，重新发布到gitee pages上。\n\n### 注意事项\n\n1. 尽量使用`hexo new`命令生成 pages或者新的markdown文件，然后到`_posts`目录下对markdown文件进行编辑修改（使用typora）。\n2. markdown文件命名只能为英文或者数字，而标题可以在markdown文件里边的**Front Matter YAML**中进行设定（**Front Matter YAML设定非常重要**，参考[链接](https://hexo.io/zh-cn/docs/front-matter)）。\n3. github仓库容量有限（< 1 GB），长期使用个人博客，注意不要插入太多太大的图片，建议是一篇博文图片不超过10张。\n4. 博客内容完全公开，注意不要发布违法或者引发舆论的内容，并注意保护个人隐私。","tags":["hexo"],"categories":["tools"]}]