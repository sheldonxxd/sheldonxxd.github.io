[{"title":"累积分布函数和S型曲线拟合","url":"/2021/01/04/cdf/","content":"\n查看数据分布情况，以及使用特定的函数模型对已观测的数据进行拟合是非常常见的操作。这里从正态分布入手，到应用**Logistic**模型完成累积分布曲线的拟合，并根据拟合后模型参数重新得到样本的均值。\n\n### 导入依赖包\n\n本case在jupyter notebook中运行，可先导入以下包：\n\n```python\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfrom collections import Counter\nfrom scipy.optimize import curve_fit\n```\n\n### 制作正态分布数据集\n\n先制作一个数据集，均值设置为10，变异度为5。\n\n```python\nmu = 10\nsigma = 5\nX = mu + sigma*np.random.randn(1000)\n```\n\n简单作图可以看到如下的数值分布：\n\n![temp](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp.svg)\n\n如果就X数据集直接计算均值，发现与10还是有一些差距：\n\n![image-20210104191011726](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210104191011726.png)\n\n### 获取分布计数\n\n主要使用`collections.Counter`函数：\n\n```python\na = Counter(X)\nb = dict(a)\nX2 = []\nfor key in b:\n    X2.append([key, b[key]])\nX2 = pd.DataFrame(X2, columns=['value', 'count'])\nX3 = X2.sort_values(by='value')\nX3['cum_count'] = X3['count'].cumsum()  # 计算累积计数值\nsns.lineplot(x='value', y='cum_count', data=X3)\nplt.show()\n```\n\n![temp2](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp2.svg)\n\n### 选取模型进行曲线拟合\n\n这里主要利用`scipy.optimize.curve_fit`来实现曲线拟合，需要选择合适的函数模型。对于S型曲线，比较常用的是**Logistic**模型，关于S型曲线拟合，可以参考《[轻松构建多种模型拟合S型曲线](https://zhuanlan.zhihu.com/p/147467845)》。\n\n```python\ndef func(x, a, b, c):\n    '''\n    使用Logistic模型来拟合\n    该模型有3个参数\n    当x=b/c时，曲线处于拐点，可用于估计正态分布的峰值\n    https://zhuanlan.zhihu.com/p/147467845\n    '''\n    return a/(1+np.exp(b-c*x))\n\nparams = curve_fit(func, X3['value'], X3['cum_count'])\npopt, pcov = params\nprint(popt[1]/popt[2])\n```\n\n![image-20210104192236878](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20210104192236878.png)\n\n可以看到拐点对应的value，跟我们最初设置的`mu=10`非常接近了。\n\n### 查看拟合效果\n\n```python\nX4 = [func(i, popt[0],popt[1],popt[2]) for i in X3['value']]\nX3['fit'] = X4\n\nsns.lineplot(x='value', y='cum_count', data=X3)\nsns.lineplot(x='value', y='fit', color='r', data=X3)\nplt.savefig(os.path.join(os.path.expanduser('~/Desktop/'), 'temp.svg'))\n```\n\n![temp3](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/temp3.svg)\n\n> 注意一下，后面如果命名临时文件，简易带上时间戳，不然图床会覆盖保存。\n\n### 参考链接\n\n1. [轻松构建多种模型拟合S型曲线](https://zhuanlan.zhihu.com/p/147467845)\n2. [Exponential Fit with Python](https://swharden.com/blog/2020-09-24-python-exponential-fit/)\n3. [累积分布函数](https://baike.baidu.com/item/%E7%B4%AF%E7%A7%AF%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0)\n\n","tags":["curveFit","data","cumulative","distribution"],"categories":["python"]},{"title":"使用ImageJ提取某点的强度时间曲线","url":"/2020/12/30/getIntensityCurve/","content":"\n有时候拿到原始视频数据(xyt)之后，需要在ImageJ中稍微预览一下，了解大概情况。其中一个重要的view是得看一下某个位置的信号的闪烁情况，所以需要提取该位置(x,y)的强度随时间变化的曲线。为了方便，我写了一个简单的ImageJ Macro脚本来实现此功能。脚本运行后，用户在图像上点击某个位置，就会把该位置保存到ROI管理器，然后提取该位置的Intensity(t)，并绘制图像。\n\n### 代码\n\n代码的几个要注意的地方：\n\n1. 需要自定义选取框的大小，对于单分子荧光光斑，4就足够了\n2. 可以设置是否多次选取，如果是多次，就默认选取3个点\n3. 提取的intensity是选取框的均值，这样能有效抑制背景噪声波动\n\n在实现上，这个代码还要解决以下问题：\n\n1. 监听并返回用户点击鼠标事件\n2. 自动切换Slice，即逐帧同xy坐标取值\n\n\n```javascript\n/*\n * 该脚本适用于手动分析TIRF视频数据中某点的强度随时间变化情况\n * Author: Xiao-Dong Xie\n * 2020年12月31日\n */\n\ntime_interval = Stack.getFrameInterval();  \n// 内置函数，获取stack的帧时间间隔，适用于原始TIRF数据\nboxWidth = 4;\n// 点选取值框大小\nmultiCheck = true; \n// 如需多次检查，修改此处为true，可将main置入for循环中\nmultiCheckNum = 3;\n\n// ================================================\nif (multiCheck) {\n\tfor (i = 0; i < multiCheckNum; i++) {\n\t\tmain(boxWidth, i); \n\t}\t\n}\nelse {\n\tmain(boxWidth, 0);\n}\n\n// =================================================\n\nfunction main(boxWidth, cid){\n\t/** 点击stack上一个点，返回time profile\n\t *  boxWidth: 全选框大小\n\t *  cid：选点的序号\n\t */\n\tloc = chooseParticle();\n\tx = loc[0];\n\ty = loc[1];\n\tmakePoint(x, y, \"small yellow hybrid\");\n\troiManager(\"add\"); // 把该点加入roi管理器\n\twait(300);\n\tn = getSliceNumber();\n\tlong = nSlices;\n\tv = newArray(long);\n\tt = newArray(long);\n\tfor (i = 0; i < long; i++) {\n\t\tsetSlice(i+1);\n\t\t// va = getPixel(x, y);\n\t\t// 用取小区域均值的方式更好\n\t\tva = getRectRegionMeanValue(x, y, boxWidth);\n\t\tv[i] = va;\n\t\tt[i] = i*time_interval;\n\t}\n\tsetSlice(n);\n\tPlot.create(\"Time Profile \"+cid, \"Time (sec)\", \"Intensity\", t, v);\n\tPlot.show();\n}\n\nfunction chooseParticle(){\n    // 鼠标单击图像上某个点，即返回其坐标\n    leftButton = 16;\n    flags = 0;\n    while(flags&leftButton == 0) getCursorLoc(x,y,z,flags);\n    return newArray(x,y);\n}\n\nfunction getRectRegionMeanValue(x, y, width){\n\t//以(x,y)坐标为中心，获取边长为width的正方形区域的平均强度\n\t// 不返回最大的强度，是因为波动比较大\n\tmakeRectangle(x-width/2, y-width/2, width, width);\n\tgetStatistics(area, mean, min, max, std, histogram);\n\treturn mean;\n}\n```\n\n### 结果\n\n使用的时候，把视频数据（tif格式）打开后，再把`getIntensityCurves.ijm`脚本文件拖入Fiji打开，然后在console中点击`run`运行，然后在视频图像上选取你想提取数值的点。结果如下：\n\n![image-20201231082554717](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201231082554717.png)","tags":["TIRF"],"categories":["ImageJ"]},{"title":"免费下载网易云音乐mp3的小工具","url":"/2020/12/30/downloadCloudMusic/","content":"\n直接上代码吧，逻辑非常简单，就是从网易云分享链接中提取music的ID，然后构造外链：`http://music.163.com/song/media/outer/url?id={musicID}.mp3`，再直接用requests模块请求这个外链，下载内容保存为mp3格式的文件即可。为了方便实用，我还利用`pySimpleGUI`做了一个非常简易的用户界面，方便操作。\n\n```python\nimport requests\nimport re\nimport os\nimport PySimpleGUI as sg\n\ndef extractMusiceID(url):\n    pattern = re.compile(r'id=\\d+')\n    mID,uID = pattern.findall(url)\n    mID = mID.split(\"=\")[1]\n    uID = uID.split(\"=\")[1]\n    print(\"User:{}\\nMusic:{}\".format(uID,mID))\n    return mID\n\ndef downloadMusic(mID,name):\n    url = \"http://music.163.com/song/media/outer/url?id={}.mp3\".format(mID)\n    # 需要使用header，不然url重定向之后抓不到\n    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n    r = requests.get(url,headers=header)\n    data = r.content\n    wks = os.path.expanduser(\"~/desktop\")  # 默认下载到桌面\n    fpath = os.path.join(wks,\"{}.mp3\".format(name))\n    with open(fpath,\"wb\") as f:\n        f.write(data)\n\ndef main(url,name):\n    mID = extractMusiceID(url)\n    downloadMusic(mID,name)\n\nif __name__ == \"__main__\":\n    \n    layout = [\n            [sg.Text(\"曲名\"),sg.Input(key='-Name-')],\n            [sg.Text(\"链接\"),sg.Input(key='-Url-')],\n            [sg.Button('Download'),sg.Exit()]\n            ]\n\n    window = sg.Window('网易云音乐下载', layout)\n\n    while True:\n        event, values = window.Read() \n        if event == 'Download':\n            name = values['-Name-']\n            url = values['-Url-']\n            main(url,name)\n        if event in (None, 'Exit'):      \n            break\n    window.Close()\n```\n\n运行之后，弹出一个窗口，填写两行信息，Download就会把mp3下载到桌面：\n\n![image-20201230122701166](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122701166.png)\n\n然后输入歌曲名字，以及分享链接。链接可以从网易云音乐中获取：\n\n![image-20201230122626573](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122626573.png)\n\n效果如下：\n\n![image-20201230122805612](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201230122805612.png)\n\n### 来一首《梦伴》\n\n\n<iframe src=\"//player.bilibili.com/player.html?aid=23415950&bvid=BV1Kp411Z7ci&cid=39036220&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" > </iframe>\n","tags":["requests","gui"],"categories":["python"]},{"title":"pandas模块常用功能备忘录","url":"/2020/12/29/pandasUsageSummary/","content":"\n使用python进行数据分析，离不开`numpy`，`pandas`，`matplotlib`三个包。为了作图好看，我还常用`seaborn`。不过`numpy`使用起来还是没有pandas那么爽，所以我还是用pandas和seaborn最多。所以趁着还有一点写博客的热情，先整理一下pandas的常用功能。\n\n当然了， 介绍最详细的还是官网文档：\n\nhttps://pandas.pydata.org/docs/\n\n### 读取数据\n\npandas可以读取各类数据文件，比如`csv`、`xlsx`、`hdf5`等。而读取的命令一般输入`pd.read`代码自动补全提示就会出来很多。我这里也不多说，想查看，可以输入一下命令：\n\n```python\nimport pandas as pd\nfor func in dir(pd):\n\tif func.startswith('read'):\n\t\tprint(func)\n```\n\n使用方法基本上都是要输入filepath，然后`pd.read_hdf`还需要输入hdf5的dataset的key名。我在处理picasso产出的中间数据时，还经常用到以下命令：\n\n```python\ndef hdf2dataframe(filepath):\n    '''\n    只适用于picasso分析过程中生成的hdf5文件数据结构\n    输出一个字典，包含了不同的dataframe\n    '''\n    data = {}\n    dataset = h5py.File(filepath, 'r')\n    for key in dataset.keys():\n        df = pd.DataFrame(dataset[key][()],dtype=\"float64\")  # 注意数据精度\n        data[key] = df\n    return data\n```\n\n这个自定义函数可以把hdf5数据中不同的dataset转化为pandas的DataFrame，并保存到字典中。但事实上当你知道dataset的key名很固定，基本都是`locs`的时候，只需要下面一行命令就可以了：\n\n```python\ndata = pd.read_hdf('cluster_filtered.hdf5','locs')\n```\n\n### 保存数据\n\n对于`pd.DataFrame`对象，可以把数据表保存到各种格式的文件中。下面简单举几个例子：\n\n```python\n# 假设data是一个dataFrame\ndata.to_csv(filepath, index=None)  # 保存为csv文件\ndata.to_excel(filepath, index=None)  # 保存为xlsx文件\ndata.to_pickle(filepath)  # 保存为pkl文件\ndata.to_hdf(filepath, 'locs')  # 保存为hdf5文件\n```\n\n其中csv和xlsx文件都可以使用excel打开，我经常使用，但为了避免多余的index列，我通常会指定`index=None`。处理的中间数据若需要保存，使用pkl文件是最方便的，没有其他参数需要声明。保存为hdf5文件跟打开的时候类似，还要写key名。\n\n### pd.DataFrame对象\n\n数据帧是pandas最重要的概念，也是最常用的一种数据结构。简单来说，就跟你在excel中看到的表格一样的。从形状上讲，就是一个二维矩形。\n\n使用`pd.read_??`方法读取数据文件，返回的基本上都是数据帧，然后你就可以进行各种愉快的操作。当然你也可以把程序运行中其他类型的数据转化为一个DataFrame，下面介绍几种方式：\n\n```python\nimport numpy as np\na = np.random.randint(10, size=(5,3))  # 产生一个3x5的随机数矩阵\nb = pd.DataFrame(a, columns=list('abc'))  \n# 数据有5列，必须给column名，\n# index名也可以指定，若无则会默认从0开始\n```\n\n以上就是从numpy二维数组直接转成数据帧的操作，就是要注意columns的命名。效果如下：\n\n<img src=\"https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229201642898.png\" alt=\"image-20201229201642898\"  />\n\n另外也有一些更傻瓜的方法：\n\n```python\na = pd.DataFrame()   # 可以先创建一个空的数据帧\na['a'] = [x for x in range(3)]\na['b'] = np.random.randint(10, size=(3,))\n```\n\n先创建一个空的数据帧对象，然后赋值也是可以的，但是要注意每一个column的数据长度要一致。这个操作有点像字典。没错，你也可以直接把字典转化为数据帧：\n\n```python\na = {'a':[1,2,3], 'b':[4,5,6]}\nb = pd.DataFrame(a)\n```\n\n### 数据帧的取值\n\n```python\na = np.random.randint(10, size=(3,5))  # 产生一个5x3的随机数矩阵\nb = pd.DataFrame(a, columns=list('abcde'))  # 数据有三列，给column名\nb.loc[0,'c']  # 注意是方括号，第一个是index name， 第二个是column名，返回单个值\nb.iloc[0] # 注意方括号，这里只能输入index名，返回一行数据（pd.Series）\n```\n\n如上所示，数据帧就是一个矩阵，取值给索引就好，不过在我的实践过程中，很少直接取某个特定位置的数值。遍历倒是常用，不过需要注意的是，为了加快速度，建议使用如下的遍历方式：\n\n```python\nfor idx,row in df.iterrows():\n    # row是数据帧的某一行\n    doSomeThingWith(row)\n```\n\n### 数据帧分组\n\n```python\ngp =data.groupby(by=['column_name_1', 'column_name_2'])\n```\n\n一般分组完了，还可以遍历分组，做一些操作：\n\n```python\nfor key, df in gp:\n    # key 是分组数据对应 column_name_1', 'column_name_2' 这两列的值\n    # df 是分组的子数据帧\n    doSomeThingWith(df)\n```\n\n### 数据筛选\n\n```python\ndata2 = data.filter(items=['column_1', 'column_2'])\n# 类似的还有一个drop方法\n```\n\n上述代码可以从data原数据帧中取出两列值。\n\n```python\nlabel = [0,2,3]\nregion_new = region[region['Cluster'].isin(labels)]  \n```\n\n上述代码可以按`Cluster`这一列的值进行筛选，如果值存在于label列表中，则被提取出来。\n\n```python\ndf = localization[(localization['x']>(x-radii))&\\\n                 (localization['x']<(x+radii))&\\\n                 (localization['y']>(y-radii))&\\\n                 (localization['y']<(y+radii))]\n```\n\n上述代码根据数据帧中的数值指定范围进行筛选，注意方括号，然后多个判断条件的布尔运算要用小括号。\n\n### 数据排序\n\n`data.sort_values(by=??)`，偶尔会用。\n\n### 数据翻转\n\n`data.T`\n\n把row和column翻转一下，偶尔会用。\n\n### 数据合并\n\n`pd.concat([df1, df2])`\n\n把一些符合条件的DataFrame合并，注意前提是它们具有相同的column，然后可以放到一个列表里面。\n\n下面还提供一个例子，怎么把series叠加做成数据帧。\n\n```python\na = pd.Series([1,2,3], index=list('abc'))  # 序列\nb = pd.Series([4,5,6], index=list('abc')) \nc = pd.concat([a,b]) # 纵向叠加series\nd = pd.concat([a,b], axis=1) # 横向叠加\ne = d.T # 翻转\n```\n\n### 数据融化\n\n`pd.melt`\n\n可以把新建一个column，把其中某些column转成新的column的value，也非常实用，在seaborn作图的时候，用到过一些。\n\n### 随机采样\n\n`data.sample([n, frac, ...])`\n\n可以指定随机返回的数量，也可以指定比例。加入data的长度是1000，n<1000, frac在0-1之间取小数。\n\n### 后记\n\n写到这里，想写点什么的冲动消退，想想我又干了一件蠢事。什么模块什么功能特别是具体到哪个函数的用法，真的不要去试图记忆甚至汇总它们。如果你用的别人的，请查看他们的文档。如果是你自己写的，请自己写好文档。让这些电子记录成为你的“第二大脑”，这不就是我的初心么？所以像这种事情，只有这一次，以后的每一篇博文，都要对应我自己在数据分析或者日常生活中遇到的具体的可以用编程的方式解决的问题。","tags":["pandas"],"categories":["python"]},{"title":"基于python的DNA定量浓度计算器","url":"/2020/12/29/oligoCalculator/","content":"\n### 起因\n\n使用离线的OligoCalculator只能一个个序列拷贝进去然后输入A260的值，再计算。这样做太繁琐了，如果数量比较少还好。但如果一次性测定比较多的链，很容易复制出错。所以我在《[基于紫外可见光吸收光谱的DNA分子定量](https://sheldonxxd.github.io/2020/12/28/basicUVspectrumAnalysis/index.html)》文章的基础之上，进一步参照[Thermo的说明](https://www.sigmaaldrich.com/china-mainland/technical-documents/articles/biology/quantitation-of-oligos.html)写了一个能够批量自动根据吸光度计算样品浓度的python版本的oligo calculator。\n\n> 要插入自己之前的文章的url，记得加上index.html，不然无法跳转超链接。\n\n### 输入\n\n![image-20201229100558994](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229100558994.png)\n\n注意这个表格文件前面三列都是《基于紫外可见光吸收光谱的DNA分子定量》文章中代码处理后生成的，而Label、Sequence、Modufication以及DiluteRatio需要自己补充。拿到了这样一张表之后，可以使用下面的表进行计算。\n\n### 代码\n\n```python\nimport pandas as pd\n\ndef getNearestBaseCombination(seq):\n    n = len(seq)\n    data = []\n    for i in range(n-1):\n        data.append(seq[i:i+2])\n    return data\n\ndef getRepeatSingleBase(seq):\n    return list(seq[1:-1])\n\ndef getMolarConc(Abs, Seq, Length=1):\n    '''\n    === 输入参数 ===\n    Abs: Float，测定UV-Vis吸收光谱的峰值，一般为A260\n    Seq: String，寡核苷酸样品的序列 \n    Length: Float，光程，一般为1 cm\n    === 返回值 ===\n    E_seq: 单纯计算序列获得的消光系数，如有修饰基团，请到IDT上查询其消光系数后做浓度校正\n    Conc: Float, 测定样品的摩尔浓度，以uM（umol/L）为单位\n    如要获取原液样品，可自行乘以稀释倍数\n    '''\n    e = {'A':15400, 'C':7400, 'G':11500, 'T':8700, \n        'AA':27400, 'AC':21200, 'AG':25000, 'AT':22800,\n        'CA':21200, 'CC':14600, 'CG':18000, 'CT':15200,\n        'GA':25200, 'GC':17600, 'GG':21600, 'GT':20000,\n        'TA':23400, 'TC':16200, 'TG':19000, 'TT':16800}\n    \n    # 消光系数表格， 参考链接如下\n    # https://www.sigmaaldrich.com/china-mainland/technical-documents/articles/biology/quantitation-of-oligos.html\n    cut = getNearestBaseCombination(Seq)\n    repeat = getRepeatSingleBase(Seq)\n    E_seq = 0\n    for c in cut:\n        E_seq += e[c]\n    for r in repeat:\n        E_seq -= e[r]\n    # E的单位是L ⋅ mol-1 ⋅ cm-1\n    Conc = Abs/(E_seq*Length)*1e6\n    # Conc的单位是 uM\n    return E_seq,Conc\n\ndef main(filepath):\n    '''\n    filepath：输入汇总表格文件路径\n    表格中必须包含样品的吸光值（PeakValue）,序列（Sequence）和稀释倍数(DiluteRatio)三列信息\n    建议在`extractUVspectrum.py`生成的excel文件中补充序列和稀释倍数即可\n    计算完成后，在表格中新增消光系数和原液浓度两列信息\n    '''\n    data = pd.read_excel(filepath)\n    concs = []\n    Es = []\n    for idx, row in data.iterrows():\n        e,c = getMolarConc(row['PeakValue'], row['Sequence'])\n        c = c*row['DiluteRatio']\n        concs.append(c)\n        Es.append(e)\n    data['E(L⋅mol-1⋅cm-1)'] = Es\n    data['Conc(uM)'] = concs\n    data.to_excel(filepath, index=None)\n    print('浓度计算完成，已刷新表格数据！')\n\nif __name__ == \"__main__\":\n    filepath = \"/path/to/ssDNA定量结果.xlsx\"\n    main(filepath)\n```\n\n### 结果\n\n![image-20201229100438254](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229100438254.png)\n\n计算完成后，新出现了两列数据，分别是不同样本所对应的消光系数和稀释前原液的浓度。需要注意，**如果有修饰，自行到[IDT查询修饰基团的消光系数](https://sg.idtdna.com/site/Catalog/Modifications)，跟此表中的消光系数相加，然后根据琅勃比尔定律对浓度进行校正**。\n\n![image-20201229103945287](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201229103945287.png)\n\n如上图所示，在excel表格中进行操作即可，最后得到最终的准确的定量浓度结果。","tags":["DNA","calculator"],"categories":["python"]},{"title":"基于紫外可见光吸收光谱的DNA分子定量","url":"/2020/12/28/basicUVspectrumAnalysis/","content":"\n我们实验室采集UV-Vis吸收光谱，只能一个一个样品地来。有时候我要对几十条DNA单链做定量的时候，一个个去记录 A_260nm 的数值比较烦（而且有时候峰值会有微弱漂移），所以打算用python写个小工具来实现峰值的提取。\n\n### 数据文件分析\n\n通过仪器配套软件可以导出csv文件，命名格式是`Sample??.样品.csv`。\n\n![image-20201228134634805](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228134634805.png)\n\n但是csv文件内数据比较脏，具体如下：\n\n![image-20201228134427947](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228134427947.png)\n\n第一行和最后一行都不是正经的内容，而第二行的column name也很随意，所以后面在使用pandas模块读取csv文件的时候要做一些处理。\n\n### 完整代码\n\n要实现功能除了得正确读取数据之外，还有以下几个要点：\n\n1. 使用glob收集csv文件（列表顺序未必等同于上样顺序）\n2. 提取sample的ID（对应着你测定样品的顺序）\n3. 使用`np.argmax`获取峰值所在序列的索引，然后取峰值\n4. 根据sampleID对数据进行排序\n\n具体代码如下：\n\n```python\nimport os, glob\nimport numpy as np\nimport pandas as pd\nimport re\n\ndef exportMaxPeak(wks):\n    '''\n    输入包含UV吸收光谱导出csv文件的路径\n    输出A260nm的峰值数据表excel到目录下\n    注意不要导出blank的光谱\n    '''\n    files = glob.glob(os.path.join(wks,'*.csv'))\n    data = []\n    pattern = 'Sample(\\d+).样品.csv'\n    for f in files:\n        # 处理文件名提取ID\n        _, fname = os.path.split(f)\n        # 读取光谱数据\n        label = re.search(pattern, fname)\n        sid = label.group(1) # 直接提取数字\n        sid = float(sid)\n        df = pd.read_csv(f, header=1)\n        mid = np.argmax(df[\"A\"])\n        max_wav = df[\"nm\"][mid]\n        max_OD = df[\"A\"][mid]\n        # 收集数据到列表\n        data.append([int(sid), max_wav, max_OD])\n    # 处理数据表格\n    data = pd.DataFrame(data, columns=[\"Sample\",\"Wavelength\", \"PeakValue\"])\n    data.sort_values(by='Sample', inplace=True)  # 排序\n    data_path = os.path.join(wks,\"A260-Max.xlsx\")\n    data.to_excel(data_path, index=False)\n    print(\"数据已保存到{}!\".format(data_path))\n\nif __name__=='__main__':\n    # 修改变量wks的值为你存放数据的目录\n    wks = r'D:\\Lab\\Data\\UV-Vis\\20201228'\n    exportMaxPeak(wks)\n```\n\n### 处理效果\n\n![image-20201228135130971](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228135130971.png)\n\n然后再基于生成的这个汇总表，把实际sample的lable以及序列填写上去，再使用`Oligo Calculator`即可计算出来样品的浓度了。\n\n![image-20201228135309897](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228135309897.png)\n\n### 使用OligoCalculator工具\n\n![image-20201228131109199](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201228131109199.png)\n\nOligoCalculator工具[点此链接](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/Oligo%20caculator.exe)下载。\n\n把序列输入进去（注意去除一些修饰），然后输入紫外测得的A260峰值，点击`Caculate`即可计算。","tags":["spectrum","DNA"],"categories":["python"]},{"title":"搭建阿里云OSS图床","url":"/2020/12/27/mergeAllNotes/","content":"\n我的笔记很多东西都比较零碎，这样不适合构建一个比较完整有序的笔记系统（第二大脑）。自从上手了github-pages之后，决定全面转向markdown+github，这样可以到处访问查看，为了解决插图文件大小问题，还特地购买了阿里云的对象存储服务。一系列的操作主要参考《[【Typora】typora+picgo+阿里云oss搭建图床](https://www.cnblogs.com/myworld7/p/13132549.html)》。\n\n### 购买阿里云OSS并配置图床\n\n可以直接搜索[包月的套餐](https://common-buy.aliyun.com/?spm=5176.7933691.1309819..68b22a66FQKm7f&commodityCode=ossbag&request=%7B%22region%22%3A%22china-common%22%7D#/buy)：\n\n![image-20201227162121971](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227162121971.png)\n\n我选择的是40GB，大陆通用，两年。购买之后进入控制台新建一个bucket（水桶）。\n\n![image-20201227161929457](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161929457.png)\n\n注意权限设置为公共可读，不然上传发布到github-pages上就看不到了。\n\n### 下载和配置picgo\n\ntypora支持picgo图床上传工具，直接到github上下载最新版本\n\nhttps://github.com/Molunerfinn/PicGo/releases\n\n把阿里云的Access Key以及其他信息复制填入配置选项即可：\n\n![image-20201227161730671](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161730671.png)\n\n### 配置typora\n\n![image-20201227161535243](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227161535243.png)\n\n到这一步，就可以开心的在本地使用typora写markdown，截图使用`snipaste`，直接复制粘贴过来就开始上传到自己的图床。不过第一张图片会先打开picgo程序，所以稍微慢一点。\n\n> 想想我之前还写了一个python脚本缩小图片大小，现在完全没有这个必要了，啊哈哈哈哈！","tags":["图床"],"categories":["blog"]},{"title":"使用python提取txt文本中关键信息并汇总到表格","url":"/2020/12/26/collectFilesUsingPython/","content":"\n使用python处理文件、文件夹和路径之类的事情，主要是用到`os`、`glob`、`re`和`shutil`等模块。\n\n### 批量创建子目录\n\n在处理数据的时候，不可避免地会涉及到各种文件的批量处理。比如我在处理和分析数据的时候，需要根据单个原始数据创建子目录用于存放后续分析产生的各种中间数据。所以我写了下面这段代码：\n\n```python\nimport os\nfrom glob import glob\n\ndef makeSubdirectory(wks):\n    '''\n    wks: 某次DNA-PAINT实验存放lif文件的目录\n    此程序会对lif文件自动新建同名子目录\n    '''\n    lifs = glob(os.path.join(wks, \"*.lif\"))\n    for lif in lifs:\n        filepath, ext = os.path.splitext(lif)  \n        # 分离文件的拓展名和其他路径，比字符串split('.')更安全\n        try:\n            os.mkdir(filepath+'/')\n        except:\n            print('Subdirectory Existed! -- %s'%(filepath))\n    print('Job Finished!')\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201221\" \n    makeSubdirectory(wks)\n```\n\n需要注意的是：\n\n1. `os.mkdir`命令如果是路径已经存在就会报错\n2. glob中可以使用`*`作为通配符匹配所有后缀为`lif`的原始数据文件，返回一个list\n3. `os.path.splitext`可以把文件路径的后缀跟名字分开\n4. `try ... except ...` 可以有效处理程序运行中的异常情况\n\n### 从txt文件中收集图像参数\n\nlif文件是leica显微镜产生的数据，可以使用ImageJ打开，然后保存为tif，方便后面的处理。此时，图像的一些相关信息可以在ImageJ中查看，比如激光强度、每帧间隔时间等。然后Info还可以保存为txt文件。为了方便，我写了一个自动化脚本，收集Info文件中的重要参数并汇总到一个excel表格中。代码如下：\n\n```python\nimport os\nfrom glob import glob\nimport pandas as pd \nimport re \n\ndef findTxt(wks):\n    infos = []\n    for d in os.walk(wks):\n        path, dirs, files = d\n        for f in files:\n            if f.startswith('Info') and f.endswith('txt'):\n                # 根据文件命名特征进行筛选\n                txt = os.path.join(path,f)\n                infos.append(txt)\n    return infos\n\ndef extractInfo(txt):\n    '''\n    txt: Info for **.txt的文件路径\n    ==============================\n    从txt中提取以下关键信息：\n    1. **激光强度**：WFLaserChannelInfo_CurrentValue = 100\n    2. 穿透深度：TIRF_PenetrationDepth = 171.008186233857\n    3. 激光波长：WFLaserChannelInfo_Wavelength = 638\n    4. 相机温度：ATLCameraSettingDefinition|TargetTemperature = -75\n    5. **每帧间隔时间**：Frame interval: 0.02046 sec\n    '''\n    recs = {'WFLaserChannelInfo_CurrentValue = ':0, \n           'TIRF_PenetrationDepth = ':0, \n           'WFLaserChannelInfo_Wavelength = ':0,\n           'TargetTemperature = ':0,\n           'Frame interval:':0}\n    # 避免构造复杂的pattern正则表达式，把复杂的写入key\n    with open(txt) as f:\n        data = f.read()\n    \n    for key in recs:\n        pattern = '{}(.+)'.format(key)\n#         print(pattern)\n        a = re.search(pattern, data)\n        r = a.group(1)\n        if r.endswith('sec'):\n            r = r.split(' ')[1]\n        recs[key] = float(r)\n    return recs\n\ndef main(wks):\n    infos = findTxt(wks)\n    newkeys = ['ExIntensity', 'PenetrationDepth','ExWavelength','ccdTemperature','frameInterval']\n    data = {x:[] for x in newkeys}\n    for txt in infos:\n        recs = extractInfo(txt)\n        # 字典的键名修改，使用pop方法\n        keys = list(recs.keys())\n    #     print(keys)\n        for newkey, oldkey in zip(newkeys, keys):\n            data[newkey].append(recs[oldkey])\n    data = pd.DataFrame(data)\n    data['filepath'] = infos\n    data.to_excel(os.path.join(wks, 'infos.xlsx'), index=None)\n\nif __name__ == \"__main__\":\n    wks = r\"E:\\xxd\\TIRF-LeicaDMi8\\20201223\" \n    main(wks)\n```\n\n代码稍微长了一点，还弄出来两个函数。寻找Info文件还好说，因为文件命名都带有Info前缀，然后文件又是txt类型，直接利用`os.walk`获取总的工作目录下所有的文件，然后遍历，根据文件名进行筛选即可。注意`os.walk`返回的是一个迭代器，每次吐出来一个根目录路径，根目录下文件夹列表以及文件列表组成的三元元组。\n\n从`Info.txt`中提取信息还颇为费事，因为Info中信息量非常大，我不可能用`f.readlines()`的方法去读取指定的行，所以选择了用正则匹配的方式。但是正则我用得很少，而且为了减轻正则pattern的设计难度，我把pattern中变化的部分尽可能地定义到字典的key中。我觉得这个思路（拿通用性换效率）还挺不错的——能把事情解决就好。re模块中我常用search和findall两个功能。需要注意的是：\n\n1. re.search只返回找到的第一个匹配的字符串\n2. re.findall返回所有匹配的字符串所组成的list\n3. 如果pattern中放括号，括号可以作为group提取信息，但是注意group的索引id\n4. 修改字典的键名的方法\n5. 使用pandas模块，构建dataframe，然后方便保存为excel可以打开的xlsx文件\n\n实现效果如下：\n\n![image-20201227120627018](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201227120627018.png)\n\n\n\n","tags":["filepath"],"categories":["python"]},{"title":"批量缩小博客中的图片","url":"/2020/12/24/resizeImages/","content":"\n目前我最熟悉[python](https://www.liaoxuefeng.com/wiki/1016959663602400)，另外也会写一点[ImageJ的脚本处理](https://zhuanlan.zhihu.com/p/60999196)和分析图像。可是编程技能“用进废退”，特别是人年纪越大，对于各种繁复的代码细节就越难记忆。所以很多码农都会写博客，并且把博客当作“第二大脑”。我当然也不例外，以后也时不时会把自己写过的代码整理到博客上。\n\n### 需求\n\n我喜欢在文章中插入截图，实现图文并茂的效果。但是github的仓库容量有限，而且图片大小尺寸不一造成不美观。虽然可以定义`css`文件来控制显示，但它只能解决美观问题。而使用图床能够避免图片太多超出仓库容量限制，但把各种图片放到公共图床上既不能让我放心，也跟我目前的workflow不兼容。所以我决定写一个小的工具代码，实现对插图文件的resize。具体有以下需求：\n\n1. 支持对png, jpg, jpeg等常见图像的resize\n2. 已经resize的不要进行重复的操作\n3. 原始大图要保留而且方便查找\n4. 不破坏markdown中对图片的引用\n\n### 代码\n\n代码要点如下：\n\n1. 使用glob批量读取多种格式的图片地址到列表\n2. 过滤已经被resize处理的图片\n3. pillow库中的resize方法的调用\n\n废话不多说，直接上代码：\n\n```python\nfrom PIL import Image \nimport os \nfrom glob import glob\n\ndef resizeImages(wks):\n    '''\n    输入makrdown附件目录地址，\n    对其中所有png\\jpg\\jpeg等格式的图像进行缩小，\n    固定宽度到600，\n    原图被重命名带`_large.*`后缀\n    如果有带此后缀的，则两张图像都不进行处理\n    '''\n    files = []\n    formats = ['png','jpg','jpeg']\n    # 如果想压缩gif，请参考\n    # https://blog.csdn.net/huanyue6660/article/details/79423326\n    for ext in formats:\n        files.extend(glob(os.path.join(wks, '*.{}'.format(ext))))  # 使用extend的方式合并新增的list\n    \n    jobs = []\n    for f in files:\n        ext = os.path.splitext(f)[-1]\n        if (f+'_large%s'%(ext) in files) or (f.endswith('large%s'%(ext))):\n            continue\n        else:\n            jobs.append(f)\n\n    for job in jobs:\n        ext = os.path.splitext(job)[-1]  # 获取文件后缀名，注意这个带点\n        a = Image.open(job)\n        a.save(job+'_large{}'.format(ext))\n        w, h = a.size\n        if w>600:\n            # 只缩小大图\n            b = a.resize(size=(600, int(h*600/w))) # 注意size得是整数\n        else:\n            b = a\n        b.save(job)\n        print('{} resized to width = 600 px!'.format(job))\n\nif __name__ == \"__main__\":\n    wks = r\"D:\\hexo\\blog\\source\\_posts\"  # windows系统文件路径，注意不要去掉r\n    subdirs = glob(os.path.join(wks,\"*/\"))\n    # print(subdirs)\n    # 对_posts目录下所有图片进行处理\n    for subdir in subdirs:\n        resizeImages(subdir)\n```\n\n### 使用\n\n我喜欢把这种高复用的小工具代码做好接口，写到python脚本中，然后用配置好的vscode打开，根据需求修改参数（这里是`_posts`目录的地址），然后按`Ctrl+F5`运行。\n\n### 结果\n\n![image-20201224114927774](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201224114927774.png)\n\n原始图像被重名为带`_large`后缀的文件。","tags":["pillow"],"categories":["python"]},{"title":"在gitub-pages搭建Hexo博客","url":"/2020/12/20/setupHexoWebsite/","content":"\n\n我打算用**hexo**结合**github pages**构建一个轻博客网站，每周发表至少一篇博文。\n\n<!-- more -->\n\n### Hexo建站参考教程\n\n使用hexo建站到**github pages**的完整教程推荐如下：\n\nhttps://zhuanlan.zhihu.com/p/78467553\n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://theme-stun.github.io/docs/zh-CN/\n\nhttps://hexo.io/zh-cn/index.html\n\nhttps://blog.csdn.net/hhgggggg/article/details/77853665\n\n### 使用**Stun**主题\n\n[stun主题](https://theme-stun.github.io/docs/zh-CN/guide/quick-start.html#%E5%AE%89%E8%A3%85)比较好看，我还稍微配置了一下可以[跟typora连用](https://www.cnblogs.com/caoayu/articles/13855081.html)（主要是为了能够正常显示插图和附件）。效果如下图：\n\n![image-20201222100030247](https://sheldon-notes.oss-cn-shanghai.aliyuncs.com/img/image-20201222100030247.png)\n\n### 需要安装的插件列表\n\n1. git相关插件，用于deploy到github pages上\n2. search相关插件，方便对站内内容进行搜索\n3. asset-imager相关插件，方便插图附件之类的\n\n### 开启评论系统\n\ngitalk插件是利用github仓库的issue功能，为了节约空间，我另外开了一个**sea-comments**的仓库作为存储。然后照着以下教程进行操作：\n\nhttps://www.jianshu.com/p/4242bb065550\n\nhttps://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#gitalk\n\nhttps://github.com/gitalk/gitalk\n\n需要注意的是，stun主题已经嵌入了gitalk，只要到stun主题下的`_config.yaml`下添加自己评论仓库的一些信息即可。\n\n> gitalk会出现Error Network报错，弃用~，之后使用`valine`，评论数据存储在`LeanCloud`中，如有必要，及时备份。\n>\n> https://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#valine\n\n### 常用命令\n\n需要进入到hexo网站的本地根目录下执行命令，以windows系统为例：\n\n1. `hexo clean && hexo g && hexo s`: 若修改了**_config.yaml**配置文件，需要清除 public目录下所有内容，然后根据新的配置文件重新生成静态网页，并且启动本地服务器\n2. `hexo clean && hexo d`：修改配置后，重新发布到gitee pages上。\n\n### 注意事项\n\n1. 尽量使用`hexo new`命令生成 pages或者新的markdown文件，然后到`_posts`目录下对markdown文件进行编辑修改（使用typora）。\n2. markdown文件命名只能为英文或者数字，而标题可以在markdown文件里边的**Front Matter YAML**中进行设定（**Front Matter YAML设定非常重要**，参考[链接](https://hexo.io/zh-cn/docs/front-matter)）。\n3. github仓库容量有限（< 1 GB），长期使用个人博客，注意不要插入太多太大的图片，建议是一篇博文图片不超过10张。\n4. 博客内容完全公开，注意不要发布违法或者引发舆论的内容，并注意保护个人隐私。","tags":["hexo"],"categories":["blog"]}]